"use strict";(function(){const t={encode:!1,tokenize:function(e){return e.replace(/[\x00-\x7F]/g,"").split("")}};t.doc={id:"id",field:["title","content"],store:["title","href","section"]};const e=FlexSearch.create("balance",t);window.bookSearchIndex=e,e.add({id:0,href:"/flink/flink-cdc-docs-master/zh/docs/core-concept/data-pipeline/",title:"Data Pipeline",section:"核心概念",content:` Definition # Since events in Flink CDC flow from the upstream to the downstream in a pipeline manner, the whole ETL task is referred as a Data Pipeline.
Parameters # A pipeline corresponds to a chain of operators in Flink.
To describe a Data Pipeline, the following parts are required:
source sink pipeline the following parts are optional:
route transform Example # Only required # We could use following yaml file to define a concise Data Pipeline describing synchronize all tables under MySQL app_db database to Doris :
source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* sink: type: doris fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; pipeline: name: Sync MySQL Database to Doris parallelism: 2 With optional # We could use following yaml file to define a complicated Data Pipeline describing synchronize all tables under MySQL app_db database to Doris and give specific target database name ods_db and specific target table name prefix ods_ :
source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* sink: type: doris fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; route: - source-table: app_db.orders sink-table: ods_db.ods_orders - source-table: app_db.shipments sink-table: ods_db.ods_shipments - source-table: app_db.products sink-table: ods_db.ods_products pipeline: name: Sync MySQL Database to Doris parallelism: 2 Pipeline Configurations # The following config options of Data Pipeline level are supported:
parameter meaning optional/required name The name of the pipeline, which will be submitted to the Flink cluster as the job name. optional parallelism The global parallelism of the pipeline. required local-time-zone The local time zone defines current session time zone id. optional `}),e.add({id:1,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/mongodb-tutorial/",title:"MongoDB 教程",section:"Flink CDC Sources 教程",content:` 演示: MongoDB CDC 导入 Elasticsearch # 下载 docker-compose.yml version: &#39;2.1&#39; services: mongo: image: &#34;mongo:4.0-xenial&#34; command: --replSet rs0 --smallfiles --oplogSize 128 ports: - &#34;27017:27017&#34; environment: - MONGO_INITDB_ROOT_USERNAME=mongouser - MONGO_INITDB_ROOT_PASSWORD=mongopw elasticsearch: image: elastic/elasticsearch:7.6.0 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34; - discovery.type=single-node ports: - &#34;9200:9200&#34; - &#34;9300:9300&#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 ports: - &#34;5601:5601&#34; 进入 MongoDB 容器，初始化副本集和数据: docker-compose exec mongo /usr/bin/mongo -u mongouser -p mongopw // 1. 初始化副本集 rs.initiate(); rs.status(); // 2. 切换数据库 use mgdb; // 3. 初始化数据 db.orders.insertMany([ { order_id: 101, order_date: ISODate(&#34;2020-07-30T10:08:22.001Z&#34;), customer_id: 1001, price: NumberDecimal(&#34;50.50&#34;), product: { name: &#39;scooter&#39;, description: &#39;Small 2-wheel scooter&#39; }, order_status: false }, { order_id: 102, order_date: ISODate(&#34;2020-07-30T10:11:09.001Z&#34;), customer_id: 1002, price: NumberDecimal(&#34;15.00&#34;), product: { name: &#39;car battery&#39;, description: &#39;12V car battery&#39; }, order_status: false }, { order_id: 103, order_date: ISODate(&#34;2020-07-30T12:00:30.001Z&#34;), customer_id: 1003, price: NumberDecimal(&#34;25.25&#34;), product: { name: &#39;hammer&#39;, description: &#39;16oz carpenter hammer&#39; }, order_status: false } ]); db.customers.insertMany([ { customer_id: 1001, name: &#39;Jark&#39;, address: &#39;Hangzhou&#39; }, { customer_id: 1002, name: &#39;Sally&#39;, address: &#39;Beijing&#39; }, { customer_id: 1003, name: &#39;Edward&#39;, address: &#39;Shanghai&#39; } ]); 下载以下 jar 包到 &lt;FLINK_HOME&gt;/lib/: 下载链接只对已发布的版本有效, SNAPSHOT 版本需要本地编译
flink-sql-connector-elasticsearch7-3.0.1-1.17.jar flink-sql-connector-mongodb-cdc-2.4.0.jar 然后启动 Flink 集群，再启动 SQL CLI. -- Flink SQL -- 设置间隔时间为3秒 Flink SQL&gt; SET execution.checkpointing.interval = 3s; -- 设置本地时区为 Asia/Shanghai Flink SQL&gt; SET table.local-time-zone = Asia/Shanghai; Flink SQL&gt; CREATE TABLE orders ( _id STRING, order_id INT, order_date TIMESTAMP_LTZ(3), customer_id INT, price DECIMAL(10, 5), product ROW&lt;name STRING, description STRING&gt;, order_status BOOLEAN, PRIMARY KEY (_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mongodb-cdc&#39;, &#39;hosts&#39; = &#39;localhost:27017&#39;, &#39;username&#39; = &#39;mongouser&#39;, &#39;password&#39; = &#39;mongopw&#39;, &#39;database&#39; = &#39;mgdb&#39;, &#39;collection&#39; = &#39;orders&#39; ); Flink SQL&gt; CREATE TABLE customers ( _id STRING, customer_id INT, name STRING, address STRING, PRIMARY KEY (_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mongodb-cdc&#39;, &#39;hosts&#39; = &#39;localhost:27017&#39;, &#39;username&#39; = &#39;mongouser&#39;, &#39;password&#39; = &#39;mongopw&#39;, &#39;database&#39; = &#39;mgdb&#39;, &#39;collection&#39; = &#39;customers&#39; ); Flink SQL&gt; CREATE TABLE enriched_orders ( order_id INT, order_date TIMESTAMP_LTZ(3), customer_id INT, price DECIMAL(10, 5), product ROW&lt;name STRING, description STRING&gt;, order_status BOOLEAN, customer_name STRING, customer_address STRING, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;elasticsearch-7&#39;, &#39;hosts&#39; = &#39;http://localhost:9200&#39;, &#39;index&#39; = &#39;enriched_orders&#39; ); Flink SQL&gt; INSERT INTO enriched_orders SELECT o.order_id, o.order_date, o.customer_id, o.price, o.product, o.order_status, c.name, c. address FROM orders AS o LEFT JOIN customers AS c ON o.customer_id = c.customer_id; 修改 MongoDB 里面的数据，观察 elasticsearch 里的结果。 db.orders.insert({ order_id: 104, order_date: ISODate(&#34;2020-07-30T12:00:30.001Z&#34;), customer_id: 1004, price: NumberDecimal(&#34;25.25&#34;), product: { name: &#39;rocks&#39;, description: &#39;box of assorted rocks&#39; }, order_status: false }); db.customers.insert({ customer_id: 1004, name: &#39;Jacob&#39;, address: &#39;Shanghai&#39; }); db.orders.updateOne( { order_id: 104 }, { $set: { order_status: true } } ); db.orders.deleteOne( { order_id : 104 } ); Back to top
`}),e.add({id:2,href:"/flink/flink-cdc-docs-master/zh/docs/get-started/quickstart/mysql-to-doris/",title:"MySQL 同步到 Doris",section:"快速开始",content:" Streaming ELT 同步 MySQL 到 Doris # 这篇教程将展示如何基于 Flink CDC 快速构建 MySQL 到 Doris 的 Streaming ELT 作业，包含整库同步、表结构变更同步和分库分表同步的功能。 本教程的演示都将在 Flink CDC CLI 中进行，无需一行 Java/Scala 代码，也无需安装 IDE。\n准备阶段 # 准备一台已经安装了 Docker 的 Linux 或者 MacOS 电脑。\n准备 Flink Standalone 集群 # 下载 Flink 1.18.0，解压后得到 flink-1.18.0 目录。 使用下面的命令跳转至 Flink 目录下，并且设置 FLINK_HOME 为 flink-1.18.0 所在目录。\ncd flink-1.18.0 通过在 conf/flink-conf.yaml 配置文件追加下列参数开启 checkpoint，每隔 3 秒做一次 checkpoint。\nexecution.checkpointing.interval: 3000 使用下面的命令启动 Flink 集群。\n./bin/start-cluster.sh 启动成功的话，可以在 http://localhost:8081/访问到 Flink Web UI，如下所示：\n多次执行 start-cluster.sh 可以拉起多个 TaskManager。\n准备 Docker 环境 # 接下来的教程将以 docker-compose 的方式准备所需要的组件。\n宿主机配置 由于 Doris 的运行需要内存映射支持，需在宿主机执行如下命令:\nsysctl -w vm.max_map_count=2000000 MacOS 由于内部实现容器的方式不同，在部署时宿主机直接修改max_map_count值可能无法成功，需要先创建以下容器：\ndocker run -it --privileged --pid=host --name=change_count debian nsenter -t 1 -m -u -n -i sh 容器创建成功执行以下命令：\nsysctl -w vm.max_map_count=2000000 然后 exit 退出，创建 Doris Docker 集群。\ndocker 镜像启动 使用下面的内容创建一个 docker-compose.yml 文件：\nversion: &#39;2.1&#39; services: doris: image: yagagagaga/doris-standalone ports: - &#34;8030:8030&#34; - &#34;8040:8040&#34; - &#34;9030:9030&#34; mysql: image: debezium/example-mysql:1.1 ports: - &#34;3306:3306&#34; environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_USER=mysqluser - MYSQL_PASSWORD=mysqlpw 该 Docker Compose 中包含的容器有：\nMySQL: 包含商品信息的数据库 app_db Doris: 存储从 MySQL 中根据规则映射过来的结果表 在 docker-compose.yml 所在目录下执行下面的命令来启动本教程需要的组件：\ndocker-compose up -d 该命令将以 detached 模式自动启动 Docker Compose 配置中定义的所有容器。你可以通过 docker ps 来观察上述的容器是否正常启动了，也可以通过访问http://localhost:8030/ 来查看 Doris 是否运行正常。\n在 MySQL 数据库中准备数据 # 进入 MySQL 容器\ndocker-compose exec mysql mysql -uroot -p123456 创建数据库 app_db 和表 orders,products,shipments，并插入数据\n-- 创建数据库 CREATE DATABASE app_db; USE app_db; -- 创建 orders 表 CREATE TABLE `orders` ( `id` INT NOT NULL, `price` DECIMAL(10,2) NOT NULL, PRIMARY KEY (`id`) ); -- 插入数据 INSERT INTO `orders` (`id`, `price`) VALUES (1, 4.00); INSERT INTO `orders` (`id`, `price`) VALUES (2, 100.00); -- 创建 shipments 表 CREATE TABLE `shipments` ( `id` INT NOT NULL, `city` VARCHAR(255) NOT NULL, PRIMARY KEY (`id`) ); -- 插入数据 INSERT INTO `shipments` (`id`, `city`) VALUES (1, &#39;beijing&#39;); INSERT INTO `shipments` (`id`, `city`) VALUES (2, &#39;xian&#39;); -- 创建 products 表 CREATE TABLE `products` ( `id` INT NOT NULL, `product` VARCHAR(255) NOT NULL, PRIMARY KEY (`id`) ); -- 插入数据 INSERT INTO `products` (`id`, `product`) VALUES (1, &#39;Beer&#39;); INSERT INTO `products` (`id`, `product`) VALUES (2, &#39;Cap&#39;); INSERT INTO `products` (`id`, `product`) VALUES (3, &#39;Peanut&#39;); Create database in Doris # Doris 暂时不支持自动创建数据库，需要先创建写入表对应的数据库。\n进入 Doris Web UI。\nhttp://localhost:8030/\n默认的用户名为 root，默认密码为空。\n通过 Web UI 创建 app_db 数据库\ncreate database app_db; 通过 FlinkCDC cli 提交任务 # 下载下面列出的二进制压缩包，并解压得到目录 flink cdc-3.0.0 '： flink-cdc-3.0.0-bin.tar.gz. flink-cdc-3.0.0 下会包含 bin、lib、log、conf 四个目录。\n下载下面列出的 connector 包，并且移动到 lib 目录下 下载链接只对已发布的版本有效, SNAPSHOT 版本需要本地基于 master 或 release- 分支编译.\nMySQL pipeline connector 3.0.0 Apache Doris pipeline connector 3.0.0 MySQL Connector Java 3.编写任务配置 yaml 文件 下面给出了一个整库同步的示例文件 mysql-to-doris.yaml：\n################################################################################ # Description: Sync MySQL all tables to Doris ################################################################################ source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: doris fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; table.create.properties.light_schema_change: true table.create.properties.replication_num: 1 pipeline: name: Sync MySQL Database to Doris parallelism: 2 其中： source 中的 tables: app_db.\\.* 通过正则匹配同步 app_db 下的所有表。 sink 添加 table.create.properties.replication_num 参数是由于 Docker 镜像中只有一个 Doris BE 节点。\n最后，通过命令行提交任务到 Flink Standalone cluster bash bin/flink-cdc.sh mysql-to-doris.yaml --jar lib/mysql-connector-java-8.0.27.jar 提交成功后，返回信息如：\nPipeline has been submitted to cluster. Job ID: ae30f4580f1918bebf16752d4963dc54 Job Description: Sync MySQL Database to Doris 在 Flink Web UI，可以看到一个名为 Sync MySQL Database to Doris 的任务正在运行。\n打开 Doris 的 Web UI，可以看到数据表已经被创建出来，数据能成功写入。\n同步变更 # 进入 MySQL 容器\ndocker-compose exec mysql mysql -uroot -p123456 接下来，修改 MySQL 数据库中表的数据，Doris 中显示的订单数据也将实时更新：\n在 MySQL 的 orders 表中插入一条数据\nINSERT INTO app_db.orders (id, price) VALUES (3, 100.00); 在 MySQL 的 orders 表中增加一个字段\nALTER TABLE app_db.orders ADD amount varchar(100) NULL; 在 MySQL 的 orders 表中更新一条数据\nUPDATE app_db.orders SET price=100.00, amount=100.00 WHERE id=1; 在 MySQL 的 orders 表中删除一条数据\nDELETE FROM app_db.orders WHERE id=2; 每执行一步就刷新一次 Doris Web UI，可以看到 Doris 中显示的 orders 数据将实时更新，如下所示：\n同样的，去修改 shipments, products 表，也能在 Doris 中实时看到同步变更的结果。\nRoute the changes # Flink CDC 提供了将源表的表结构/数据路由到其他表名的配置，借助这种能力，我们能够实现表名库名替换，整库同步等功能。 下面提供一个配置文件说明：\n################################################################################ # Description: Sync MySQL all tables to Doris ################################################################################ source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: doris fenodes: 127.0.0.1:8030 benodes: 127.0.0.1:8040 username: root password: &#34;&#34; table.create.properties.light_schema_change: true table.create.properties.replication_num: 1 route: - source-table: app_db.orders sink-table: ods_db.ods_orders - source-table: app_db.shipments sink-table: ods_db.ods_shipments - source-table: app_db.products sink-table: ods_db.ods_products pipeline: name: Sync MySQL Database to Doris parallelism: 2 通过上面的 route 配置，会将 app_db.orders 表的结构和数据同步到 ods_db.ods_orders 中。从而实现数据库迁移的功能。 特别地，source-table 支持正则表达式匹配多表，从而实现分库分表同步的功能，例如下面的配置：\nroute: - source-table: app_db.order\\.* sink-table: ods_db.ods_orders 这样，就可以将诸如 app_db.order01、app_db.order02、app_db.order03 的表汇总到 ods_db.ods_orders 中。注意，目前还不支持多表中存在相同主键数据的场景，将在后续版本支持。\n环境清理 # 本教程结束后，在 docker-compose.yml 文件所在的目录下执行如下命令停止所有容器：\ndocker-compose down 在 Flink 所在目录 flink-1.18.0 下执行如下命令停止 Flink 集群：\n./bin/stop-cluster.sh Back to top\n"}),e.add({id:3,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/",title:"Pipeline 连接器",section:"连接器",content:" "}),e.add({id:4,href:"/flink/flink-cdc-docs-master/zh/docs/deployment/standalone/",title:"Standalone",section:"部署模式",content:` Introduction # Standalone mode is Flink’s simplest deployment mode. This short guide will show you how to download the latest stable version of Flink, install, and run it. You will also run an example Flink CDC job and view it in the web UI.
Preparation # Flink runs on all UNIX-like environments, i.e. Linux, Mac OS X, and Cygwin (for Windows).
You can refer overview to check supported versions and download the binary release of Flink, then extract the archive:
tar -xzf flink-*.tgz You should set FLINK_HOME environment variables like:
export FLINK_HOME=/path/flink-* Start and stop a local cluster # To start a local cluster, run the bash script that comes with Flink:
cd /path/flink-* ./bin/start-cluster.sh Flink is now running as a background process. You can check its status with the following command:
ps aux | grep flink You should be able to navigate to the web UI at localhost:8081 to view the Flink dashboard and see that the cluster is up and running.
To quickly stop the cluster and all running components, you can use the provided script:
./bin/stop-cluster.sh Set up Flink CDC # Download the tar file of Flink CDC from release page, then extract the archive:
tar -xzf flink-cdc-*.tar.gz Extracted flink-cdc contains four directories: bin,lib,log and conf.
Download the connector jars from release page, and move it to the lib directory. Download links are available only for stable releases, SNAPSHOT dependencies need to be built based on specific branch by yourself.
Submit a Flink CDC Job # Here is an example file for synchronizing the entire database mysql-to-doris.yaml：
################################################################################ # Description: Sync MySQL all tables to Doris ################################################################################ source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: doris fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; pipeline: name: Sync MySQL Database to Doris parallelism: 2 You need to modify the configuration file according to your needs, refer to connectors more information.
MySQL pipeline connector Apache Doris pipeline connector Finally, submit job to Flink Standalone cluster using Cli.
cd /path/flink-cdc-* ./bin/flink-cdc.sh mysql-to-doris.yaml After successful submission, the return information is as follows：
Pipeline has been submitted to cluster. Job ID: ae30f4580f1918bebf16752d4963dc54 Job Description: Sync MySQL Database to Doris Then you can find a job named Sync MySQL Database to Doris running through Flink Web UI.
`}),e.add({id:5,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/overview/",title:"概览",section:"Flink Source 连接器",content:` Flink Sources 连接器 # Flink CDC sources is a set of source connectors for Apache Flink®, ingesting changes from different databases using change data capture (CDC). Some CDC sources integrate Debezium as the engine to capture data changes. So it can fully leverage the ability of Debezium. See more about what is Debezium.
You can also read tutorials about how to use these sources.
Supported Connectors # Connector Database Driver mongodb-cdc MongoDB: 3.6, 4.x, 5.0 MongoDB Driver: 4.3.4 mysql-cdc MySQL: 5.6, 5.7, 8.0.x RDS MySQL: 5.6, 5.7, 8.0.x PolarDB MySQL: 5.6, 5.7, 8.0.x Aurora MySQL: 5.6, 5.7, 8.0.x MariaDB: 10.x PolarDB X: 2.0.1 JDBC Driver: 8.0.28 oceanbase-cdc OceanBase CE: 3.1.x, 4.x OceanBase EE: 2.x, 3.x, 4.x OceanBase Driver: 2.4.x oracle-cdc Oracle: 11, 12, 19, 21 Oracle Driver: 19.3.0.0 postgres-cdc PostgreSQL: 9.6, 10, 11, 12, 13, 14 JDBC Driver: 42.5.1 sqlserver-cdc Sqlserver: 2012, 2014, 2016, 2017, 2019 JDBC Driver: 9.4.1.jre8 tidb-cdc TiDB: 5.1.x, 5.2.x, 5.3.x, 5.4.x, 6.0.0 JDBC Driver: 8.0.27 db2-cdc Db2: 11.5 Db2 Driver: 11.5.0.0 vitess-cdc Vitess: 8.0.x, 9.0.x MySql JDBC Driver: 8.0.26 Supported Flink Versions # The following table shows the version mapping between Flink® CDC Connectors and Flink®:
Flink® CDC Version Flink® Version 1.0.0 1.11.* 1.1.0 1.11.* 1.2.0 1.12.* 1.3.0 1.12.* 1.4.0 1.13.* 2.0.* 1.13.* 2.1.* 1.13.* 2.2.* 1.13.*, 1.14.* 2.3.* 1.13.*, 1.14.*, 1.15.*, 1.16.* 2.4.* 1.13.*, 1.14.*, 1.15.*, 1.16.*, 1.17.* 3.0.* 1.14.*, 1.15.*, 1.16.*, 1.17.*, 1.18.* Features # Supports reading database snapshot and continues to read binlogs with exactly-once processing even failures happen. CDC connectors for DataStream API, users can consume changes on multiple databases and tables in a single job without Debezium and Kafka deployed. CDC connectors for Table/SQL API, users can use SQL DDL to create a CDC source to monitor changes on a single table. The following table shows the current features of the connector:
Connector No-lock Read Parallel Read Exactly-once Read Incremental Snapshot Read mongodb-cdc ✅ ✅ ✅ ✅ mysql-cdc ✅ ✅ ✅ ✅ oracle-cdc ✅ ✅ ✅ ✅ postgres-cdc ✅ ✅ ✅ ✅ sqlserver-cdc ✅ ✅ ✅ ✅ oceanbase-cdc ❌ ❌ ❌ ❌ tidb-cdc ✅ ❌ ✅ ❌ db2-cdc ✅ ✅ ✅ ✅ vitess-cdc ✅ ❌ ✅ ❌ Usage for Table/SQL API # We need several steps to setup a Flink cluster with the provided connector.
Setup a Flink cluster with version 1.12+ and Java 8+ installed. Download the connector SQL jars from the Downloads page (or build yourself). Put the downloaded jars under FLINK_HOME/lib/. Restart the Flink cluster. The example shows how to create a MySQL CDC source in Flink SQL Client and execute queries on it.
-- creates a mysql cdc table source CREATE TABLE mysql_binlog ( id INT NOT NULL, name STRING, description STRING, weight DECIMAL(10,3), PRIMARY KEY(id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;username&#39; = &#39;flinkuser&#39;, &#39;password&#39; = &#39;flinkpw&#39;, &#39;database-name&#39; = &#39;inventory&#39;, &#39;table-name&#39; = &#39;products&#39; ); -- read snapshot and binlog data from mysql, and do some transformation, and show on the client SELECT id, UPPER(name), description, weight FROM mysql_binlog; Usage for DataStream API # Include following Maven dependency (available through Maven Central):
&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;!-- add the dependency matching your database --&gt; &lt;artifactId&gt;flink-connector-mysql-cdc&lt;/artifactId&gt; &lt;!-- The dependency is available only for stable releases, SNAPSHOT dependencies need to be built based on master or release branches by yourself. --&gt; &lt;version&gt;3.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.cdc.connectors.mysql.source.MySqlSource; public class MySqlBinlogSourceExample { public static void main(String[] args) throws Exception { MySqlSource&lt;String&gt; mySqlSource = MySqlSource.&lt;String&gt;builder() .hostname(&#34;yourHostname&#34;) .port(yourPort) .databaseList(&#34;yourDatabaseName&#34;) // set captured database .tableList(&#34;yourDatabaseName.yourTableName&#34;) // set captured table .username(&#34;yourUsername&#34;) .password(&#34;yourPassword&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000); env .fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), &#34;MySQL Source&#34;) // set 4 parallel source tasks .setParallelism(4) .print().setParallelism(1); // use parallelism 1 for sink to keep message ordering env.execute(&#34;Print MySQL Snapshot + Binlog&#34;); } } Deserialization # The following JSON data show the change event in JSON format.
{ &#34;before&#34;: { &#34;id&#34;: 111, &#34;name&#34;: &#34;scooter&#34;, &#34;description&#34;: &#34;Big 2-wheel scooter&#34;, &#34;weight&#34;: 5.18 }, &#34;after&#34;: { &#34;id&#34;: 111, &#34;name&#34;: &#34;scooter&#34;, &#34;description&#34;: &#34;Big 2-wheel scooter&#34;, &#34;weight&#34;: 5.15 }, &#34;source&#34;: {...}, &#34;op&#34;: &#34;u&#34;, // the operation type, &#34;u&#34; means this this is an update event &#34;ts_ms&#34;: 1589362330904, // the time at which the connector processed the event &#34;transaction&#34;: null } Note: Please refer Debezium documentation to know the meaning of each field.
In some cases, users can use the JsonDebeziumDeserializationSchema(true) Constructor to enabled include schema in the message. Then the Debezium JSON message may look like this:
{ &#34;schema&#34;: { &#34;type&#34;: &#34;struct&#34;, &#34;fields&#34;: [ { &#34;type&#34;: &#34;struct&#34;, &#34;fields&#34;: [ { &#34;type&#34;: &#34;int32&#34;, &#34;optional&#34;: false, &#34;field&#34;: &#34;id&#34; }, { &#34;type&#34;: &#34;string&#34;, &#34;optional&#34;: false, &#34;default&#34;: &#34;flink&#34;, &#34;field&#34;: &#34;name&#34; }, { &#34;type&#34;: &#34;string&#34;, &#34;optional&#34;: true, &#34;field&#34;: &#34;description&#34; }, { &#34;type&#34;: &#34;double&#34;, &#34;optional&#34;: true, &#34;field&#34;: &#34;weight&#34; } ], &#34;optional&#34;: true, &#34;name&#34;: &#34;mysql_binlog_source.inventory_1pzxhca.products.Value&#34;, &#34;field&#34;: &#34;before&#34; }, { &#34;type&#34;: &#34;struct&#34;, &#34;fields&#34;: [ { &#34;type&#34;: &#34;int32&#34;, &#34;optional&#34;: false, &#34;field&#34;: &#34;id&#34; }, { &#34;type&#34;: &#34;string&#34;, &#34;optional&#34;: false, &#34;default&#34;: &#34;flink&#34;, &#34;field&#34;: &#34;name&#34; }, { &#34;type&#34;: &#34;string&#34;, &#34;optional&#34;: true, &#34;field&#34;: &#34;description&#34; }, { &#34;type&#34;: &#34;double&#34;, &#34;optional&#34;: true, &#34;field&#34;: &#34;weight&#34; } ], &#34;optional&#34;: true, &#34;name&#34;: &#34;mysql_binlog_source.inventory_1pzxhca.products.Value&#34;, &#34;field&#34;: &#34;after&#34; }, { &#34;type&#34;: &#34;struct&#34;, &#34;fields&#34;: {...}, &#34;optional&#34;: false, &#34;name&#34;: &#34;io.debezium.connector.mysql.Source&#34;, &#34;field&#34;: &#34;source&#34; }, { &#34;type&#34;: &#34;string&#34;, &#34;optional&#34;: false, &#34;field&#34;: &#34;op&#34; }, { &#34;type&#34;: &#34;int64&#34;, &#34;optional&#34;: true, &#34;field&#34;: &#34;ts_ms&#34; } ], &#34;optional&#34;: false, &#34;name&#34;: &#34;mysql_binlog_source.inventory_1pzxhca.products.Envelope&#34; }, &#34;payload&#34;: { &#34;before&#34;: { &#34;id&#34;: 111, &#34;name&#34;: &#34;scooter&#34;, &#34;description&#34;: &#34;Big 2-wheel scooter&#34;, &#34;weight&#34;: 5.18 }, &#34;after&#34;: { &#34;id&#34;: 111, &#34;name&#34;: &#34;scooter&#34;, &#34;description&#34;: &#34;Big 2-wheel scooter&#34;, &#34;weight&#34;: 5.15 }, &#34;source&#34;: {...}, &#34;op&#34;: &#34;u&#34;, // the operation type, &#34;u&#34; means this this is an update event &#34;ts_ms&#34;: 1589362330904, // the time at which the connector processed the event &#34;transaction&#34;: null } } Usually, it is recommended to exclude schema because schema fields makes the messages very verbose which reduces parsing performance.
The JsonDebeziumDeserializationSchema can also accept custom configuration of JsonConverter, for example if you want to obtain numeric output for decimal data, you can construct JsonDebeziumDeserializationSchema as following:
Map&lt;String, Object&gt; customConverterConfigs = new HashMap&lt;&gt;(); customConverterConfigs.put(JsonConverterConfig.DECIMAL_FORMAT_CONFIG, &#34;numeric&#34;); JsonDebeziumDeserializationSchema schema = new JsonDebeziumDeserializationSchema(true, customConverterConfigs); Building from source # Prerequisites:
git Maven At least Java 8 git clone https://github.com/apache/flink-cdc.git cd flink-cdc mvn clean install -DskipTests The dependencies are now available in your local .m2 repository.
Back to top
`}),e.add({id:6,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/overview/",title:"概览",section:"Pipeline 连接器",content:` Pipeline Connectors # Flink CDC 提供了可用于 YAML 作业的 Pipeline Source 和 Sink 连接器来与外部系统交互。您可以直接使用这些连接器，只需将 JAR 文件添加到您的 Flink CDC 环境中，并在您的 YAML Pipeline 定义中指定所需的连接器。
Supported Connectors # 连接器 类型 支持的外部系统 Apache Doris Sink Apache Doris: 1.2.x, 2.x.x Kafka Sink Kafka MySQL Source MySQL: 5.6, 5.7, 8.0.x RDS MySQL: 5.6, 5.7, 8.0.x PolarDB MySQL: 5.6, 5.7, 8.0.x Aurora MySQL: 5.6, 5.7, 8.0.x MariaDB: 10.x PolarDB X: 2.0.1 Paimon Sink Paimon: 0.6, 0.7, 0.8 StarRocks Sink StarRocks: 2.x, 3.x Develop Your Own Connector # 如果现有的连接器无法满足您的需求，您可以自行开发自己的连接器，以将您的外部系统集成到 Flink CDC 数据管道中。查阅 Flink CDC APIs 了解如何开发您自己的连接器。
Back to top
`}),e.add({id:7,href:"/flink/flink-cdc-docs-master/zh/docs/developer-guide/understand-flink-cdc-api/",title:"理解 Flink CDC API",section:"开发者指南",content:` Understand Flink CDC API # If you are planning to build your own Flink CDC connectors, or considering contributing to Flink CDC, you might want to hava a deeper look at the APIs of Flink CDC. This document will go through some important concepts and interfaces in order to help you with your development.
Event # An event under the context of Flink CDC is a special kind of record in Flink&rsquo;s data stream. It describes the captured changes in the external system on source side, gets processed and transformed by internal operators built by Flink CDC, and finally passed to data sink then write or applied to the external system on sink side.
Each change event contains the table ID it belongs to, and the payload that the event carries. Based on the type of payload, we categorize events into these kinds:
DataChangeEvent # DataChangeEvent describes data changes in the source. It consists of 5 fields
Table ID: table ID it belongs to Before: pre-image of the data After: post-image of the data Operation type: type of the change operation Meta: metadata of the change For the operation type field, we pre-define 4 operation types:
Insert: new data entry, with before = null and after = new data Delete: removal of data, with before = removed data and after = null Update: update of existed data, with before = data before change and after = data after change Replace: SchemaChangeEvent # SchemaChangeEvent describes schema changes in the source. Compared to DataChangeEvent, the payload of SchemaChangeEvent describes changes in the table structure in the external system, including:
AddColumnEvent: new column in the table AlterColumnTypeEvent: type change of a column CreateTableEvent: creation of a new table. Also used to describe the schema of a pre-emitted DataChangeEvent DropColumnEvent: removal of a column RenameColumnEvent: name change of a column Flow of Events # As you may have noticed, data change event doesn&rsquo;t have its schema bound with it. This reduces the size of data change event and the overhead of serialization, but makes it not self-descriptive Then how does the framework know how to interpret the data change event?
To resolve the problem, the framework adds a requirement to the flow of events: a CreateTableEvent must be emitted before any DataChangeEvent if a table is new to the framework, and SchemaChangeEvent must be emitted before any DataChangeEvent if the schema of a table is changed. This requirement makes sure that the framework has been aware of the schema before processing any data changes.
Data Source # Data source works as a factory of EventSource and MetadataAccessor, constructing runtime implementations of source that captures changes from external system and provides metadata.
EventSource is a Flink source that reads changes, converts them to events , then emits to downstream Flink operators. You can refer to Flink documentation to learn internals and how to implement a Flink source.
MetadataAccessor serves as the metadata reader of the external system, by listing namespaces, schemas and tables, and provide the table schema (table structure) of the given table ID.
Data Sink # Symmetrical with data source, data sink consists of EventSink and MetadataApplier, which writes data change events and apply schema changes (metadata changes) to external system.
EventSink is a Flink sink that receives change event from upstream operator, and apply them to the external system. Currently we only support Flink&rsquo;s Sink V2 API.
MetadataApplier will be used to handle schema changes. When the framework receives schema change event from source, after making some internal synchronizations and flushes, it will apply the schema change to external system via this applier.
`}),e.add({id:8,href:"/flink/flink-cdc-docs-master/zh/docs/get-started/",title:"入门指南",section:"Docs",content:" "}),e.add({id:9,href:"/flink/flink-cdc-docs-master/zh/docs/faq/faq/",title:"通用FAQ",section:"常见问题",content:` 通用FAQ # Q1: 为啥没法下载 flink-sql-connector-mysql-cdc-2.2-SNAPSHOT.jar ，maven 仓库为啥没有 xxx-SNAPSHOT 依赖？ # 和主流的 maven 项目版本管理相同，xxx-SNAPSHOT 版本都是对应开发分支的代码，需要用户自己下载源码并编译对应的jar， 用户应该使用已经 release 过的版本，比如 flink-sql-connector-mysql-cdc-2.1.0.jar，release 过的版本maven中心仓库才会有。
Q2: 什么时候使用 flink-sql-connector-xxx.jar，什么时候使用 flink-connector-xxx.jar，两者有啥区别? # Flink CDC 项目中各个connector的依赖管理和Flink 项目中 connector 保持一致。flink-sql-connector-xx 是胖包，除了connector的代码外，还把 connector 依赖的所有三方包 shade 后打入，提供给 SQL 作业使用，用户只需要在 lib目录下添加该胖包即可。flink-connector-xx 只有该 connector 的代码，不包含其所需的依赖，提供 datastream 作业使用，用户需要自己管理所需的三方包依赖，有冲突的依赖需要自己做 exclude, shade 处理。
Q3: 为啥把包名从 com.alibaba.ververica 改成 org.apache.flink? 为啥 maven 仓库里找不到 2.x 版本？ # Flink CDC 项目 从 2.0.0 版本将 group id 从com.alibaba.ververica 改成 com.ververica, 自 3.1 版本从将 group id 从 com.ververica 改成 org.apache.flink。 这是为了让项目更加社区中立，让各个公司的开发者共建时更方便。所以在maven仓库找 2.x 的包时，路径是 /com/ververica；找3.1及以上版本的包时，路径是/org/apache/flink
MySQL CDC FAQ # Q1: 使用CDC 2.x版本，只能读取全量数据，无法读取增量（binlog） 数据，怎么回事？ # CDC 2.0 支持了无锁算法，支持并发读取，为了保证全量数据 + 增量数据的顺序性，依赖Flink 的 checkpoint机制，所以作业需要配置 checkpoint。 SQL 作业中配置方式：
Flink SQL&gt; SET &#39;execution.checkpointing.interval&#39; = &#39;3s&#39;; DataStream 作业配置方式：
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(3000); 此外，如果某些数据库的只读实例是简化过binlog的，比如阿里云RDS MySQL 5.6 只读实例，其binlog不含有变更数据，自然无法获得所需增量数据
Q2: 使用 MySQL CDC，增量阶段读取出来的 timestamp 字段时区相差8小时，怎么回事呢？ # 在解析binlog数据中的timestamp字段时，cdc 会使用到作业里配置的server-time-zone信息，也就是MySQL服务器的时区，如果这个时区没有和你的MySQL服务器时区一致，就会出现这个问题。
此外，如果是在DataStream作业中自定义列化器如 MyDeserializer implements DebeziumDeserializationSchema, 自定义的序列化器在解析 timestamp 类型数据时，需要参考下 RowDataDebeziumDeserializeSchema 中对 timestamp 类型的解析，用时给定的时区信息。
private TimestampData convertToTimestamp(Object dbzObj, Schema schema) { if (dbzObj instanceof Long) { switch (schema.name()) { case Timestamp.SCHEMA_NAME: return TimestampData.fromEpochMillis((Long) dbzObj); case MicroTimestamp.SCHEMA_NAME: long micro = (long) dbzObj; return TimestampData.fromEpochMillis(micro / 1000, (int) (micro % 1000 * 1000)); case NanoTimestamp.SCHEMA_NAME: long nano = (long) dbzObj; return TimestampData.fromEpochMillis(nano / 1000_000, (int) (nano % 1000_000)); } } LocalDateTime localDateTime = TemporalConversions.toLocalDateTime(dbzObj, serverTimeZone); return TimestampData.fromLocalDateTime(localDateTime); } Q3: mysql cdc支持监听从库吗？从库需要如何配置？ # 支持的，从库需要配置 log-slave-updates = 1 使从实例也能将从主实例同步的数据写入从库的 binlog 文件中，如果主库开启了gtid mode，从库也需要开启。
log-slave-updates = 1 gtid_mode = on enforce_gtid_consistency = on Q4: 我想同步分库分表，应该如何配置？ # 通过 mysql cdc 表的with参数中，表名和库名均支持正则配置，比如 &rsquo;table-name&rsquo; =&lsquo;user_.&rsquo; 可以匹配表名 user_1, user_2,user_a表，注意正则匹配任意字符是&rsquo;.&rsquo; 而不是 &lsquo;*&rsquo;, 其中点号表示任意字符，星号表示0个或多个，database-name也如此。
Q5: 我想跳过存量读取阶段，只读取 binlog 数据，怎么配置？ # 在 mysql cdc 表的 with 参数中指定 &lsquo;scan.startup.mode&rsquo; = &rsquo;latest-offset&rsquo; 即可。
&#39;scan.startup.mode&#39; = &#39;latest-offset&#39;. Q6: 我想获取数据库中的 DDL事件，怎么办，有demo吗？ # CDC 2.1 版本提供了 DataStream API： MysqlSource， 用户可以配置 includeSchemaChanges 表示是否需要DDL 事件，获取到 DDL 事件后自己写代码处理。
public void consumingAllEvents() throws Exception { inventoryDatabase.createAndInitialize(); MySqlSource&lt;String&gt; mySqlSource = MySqlSource.&lt;String&gt;builder() .hostname(MYSQL_CONTAINER.getHost()) .port(MYSQL_CONTAINER.getDatabasePort()) .databaseList(inventoryDatabase.getDatabaseName()) .tableList(inventoryDatabase.getDatabaseName() + &#34;.products&#34;) .username(inventoryDatabase.getUsername()) .password(inventoryDatabase.getPassword()) .serverId(&#34;5401-5404&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) .includeSchemaChanges(true) // Configure here and output DDL events .build(); ... // Other processing logic } Q7: MySQL 整库同步怎么做, Flink CDC 支持吗？ # Flink CDC 支持的.
Q6 中 提供的 DataStream API 已经可以让用户获取 DDL 变更事件和数据变更事件，用户需要在此基础上，根据自己的业务逻辑和下游存储进行 DataStream 作业开发。 Flink CDC 3.0以上版本支持以Pipeline的形式对Mysql整库同步。 Q8: 同一个实例下，某个库的表无法同步增量数据，其他库都可以，这是为啥？ # 这个问题是因为 mysql 服务器 可以配置 binlog 过滤器，忽略了某些库的 binlog。用户可以通过 show master status 命令查看 Binlog_Ignore_DB 和 Binlog_Do_DB。
mysql&gt; show master status; +------------------+----------+--------------+------------------+----------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+----------------------+ | mysql-bin.000006 | 4594 | | | xxx:1-15 | +------------------+----------+--------------+------------------+----------------------+ Q9: 作业报错 The connector is trying to read binlog starting at GTIDs xxx and binlog file &lsquo;binlog.000064&rsquo;, pos=89887992, skipping 4 events plus 1 rows, but this is no longer available on the server. Reconfigure the connector to use a snapshot when needed，怎么办呢？ # 出现这种错误是 作业正在读取的binlog文件在 MySQL 服务器已经被清理掉，这种情况一般是 MySQL 服务器上保留的 binlog 文件过期时间太短，可以将该值设置大一点，比如7天。
mysql&gt; show variables like &#39;expire_logs_days&#39;; mysql&gt; set global expire_logs_days=7; 还有种情况是 flink cdc 作业消费binlog 太慢，这种一般分配足够的资源即可。
Q10: 作业报错 ConnectException: A slave with the same server_uuid/server_id as this slave has connected to the master，怎么办呢？ # 出现这种错误是 作业里使用的 server id 和其他作业或其他同步工具使用的server id 冲突了，server id 需要全局唯一，server id 是一个int类型整数。 在 CDC 2.x 版本中，source 的每个并发都需要一个server id，建议合理规划好server id，比如作业的 source 设置成了四个并发，可以配置 &lsquo;server-id&rsquo; = &lsquo;5001-5004&rsquo;, 这样每个 source task 就不会冲突了。
Q11: 作业报错 ConnectException: Received DML ‘…’ for processing, binlog probably contains events generated with statement or mixed based replication format，怎么办呢？ # 出现这种错误是 MySQL 服务器配置不对，需要检查下 binlog_format 是不是 ROW? 可以通过下面的命令查看
mysql&gt; show variables like &#39;%binlog_format%&#39;; Q12: 作业报错 Mysql8.0 Public Key Retrieval is not allowed， 怎么办呢? # 这是因为用户配置的 MySQL 用户 使用的是 sha256 密码认证，需要 TLS 等协议传输密码。一种简单的方法是使允许 MySQL用户 支持原始密码方式访问。
mysql&gt; ALTER USER &#39;username&#39;@&#39;localhost&#39; IDENTIFIED WITH mysql_native_password BY &#39;password&#39;; mysql&gt; FLUSH PRIVILEGES; Q13: 作业报错 EventDataDeserializationException: Failed to deserialize data of EventHeaderV4 &hellip;. Caused by: java.net.SocketException: Connection reset， 怎么办呢 ? # 这个问题一般是网络原因或者数据库繁忙引起，首先排查flink 集群 到 数据库之间的网络情况，其次可以调大 MySQL 服务器的网络参数。
mysql&gt; set global slave_net_timeout = 120; mysql&gt; set global thread_pool_idle_timeout = 120; 或者采用下面的Flink配置：
execution.checkpointing.interval=10min execution.checkpointing.tolerable-failed-checkpoints=100 restart-strategy=fixed-delay restart-strategy.fixed-delay.attempts=2147483647 restart-strategy.fixed-delay.delay= 30s 如果作业存在反压，也可能出现这个问题。你需要先处理作业的反压。
Q14: 作业报错 The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires. 怎么办呢 ? # 出现这个问题的原因是的作业全量阶段读取太慢，在全量阶段读完后，之前记录的全量阶段开始时的 gtid 位点已经被 mysql 清理掉了。这种可以增大 mysql 服务器上 binlog 文件的保存时间，也可以调大 source 的并发，让全量阶段读取更快。
Q15: 在 DataStream API中构建MySQL CDC源时如何配置tableList选项？ # tableList选项要求表名使用数据库名，而不是DataStream API中的表名。对于MySQL CDC源代码，tableList选项值应该类似于‘my_db.my_table’。
Postgres CDC FAQ # Q1: 发现 PG 服务器磁盘使用率高，WAL 不释放 是什么原因？ # Flink Postgres CDC 只会在 checkpoint 完成的时候更新 Postgres slot 中的 LSN。因此如果发现磁盘使用率高的情况下，请先确认 checkpoint 是否开启。
Q2: Flink Postgres CDC 同步 Postgres 中将 超过最大精度（38，18）的 DECIMAL 类型返回 NULL # Flink 中如果收到数据的 precision 大于在 Flink 中声明的类型的 precision 时，会将数据处理成 NULL。此时可以配置相应&rsquo;debezium.decimal.handling.mode&rsquo; = &lsquo;string&rsquo; 将读取的数据用 STRING 类型 来处理。
Q3: Flink Postgres CDC 提示未传输 TOAST 数据，是什么原因？ # 请先确保 REPLICA IDENTITY 是 FULL。 TOAST 的数据比较大，为了节省 wal 的大小，如果 TOAST 数据没有变更，那么 wal2json plugin 就不会在更新后的数据中带上 toast 数据。为了避免这个问题，可以通过 &lsquo;debezium.schema.refresh.mode&rsquo;=&lsquo;columns_diff_exclude_unchanged_toast&rsquo;来解决。
Q4: 作业报错 Replication slot &ldquo;xxxx&rdquo; is active， 怎么办？ # 当前 Flink Postgres CDC 在作业退出后并不会手动释放 slot。前往 Postgres 中手动执行以下命令：
select pg_drop_replication_slot(&#39;rep_slot&#39;); ERROR: replication slot &#34;rep_slot&#34; is active for PID 162564 select pg_terminate_backend(162564); select pg_drop_replication_slot(&#39;rep_slot&#39;); Q5: 作业有脏数据，比如非法的日期，有参数可以配置可以过滤吗？ # 可以的，可以在 Flink CDC 表的with 参数里 加下 &lsquo;debezium.event.deserialization.failure.handling.mode&rsquo;=&lsquo;warn&rsquo; 参数，跳过脏数据，将脏数据打印到WARN日志里。 也可以配置 &lsquo;debezium.event.deserialization.failure.handling.mode&rsquo;=&lsquo;ignore&rsquo;， 直接跳过脏数据，不打印脏数据到日志。
Q6: 在DataStream API中构建Postgres CDC源时如何配置tableList选项？ # tableList选项要求表名使用架构名，而不是DataStream API中的表名。对于Postgres CDC source，tableList选项值应为‘my_schema.my_table’。
MongoDB CDC FAQ # Q1: MongoDB CDC 支持 全量+增量读 和 只读增量吗？ # 支持，默认为 全量+增量 读取；使用 &lsquo;scan.startup.mode&rsquo; = &rsquo;latest-offset&rsquo; 参数设置为只读增量。
Q2: MongoDB CDC 支持从 checkpoint 恢复吗? 原理是怎么样的呢？ # 支持，checkpoint 会记录 ChangeStream 的 resumeToken，恢复的时候可以通过resumeToken重新恢复ChangeStream。其中 resumeToken 对应 oplog.rs (MongoDB 变更日志collection) 的位置，oplog.rs 是一个固定容量的 collection。当 resumeToken 对应的记录在 oplog.rs 中不存在的时候，可能会出现 Invalid resumeToken 的异常。这种情况，在使用时可以设置合适oplog.rs的集合大小，避免oplog.rs保留时间过短，可以参考 https://docs.mongodb.com/manual/tutorial/change-oplog-size/ 。另外，resumeToken 可以通过新到的变更记录和 heartbeat 记录来刷新。
Q3: MongoDB CDC 支持输出 -U（update_before，更新前镜像值）消息吗？ # 在 MongoDB 6.0 及以上版本，若数据库开启了前像或后像功能，可以在SQL作业中配置参数 &lsquo;scan.full-changelog&rsquo; = &rsquo;true&rsquo;，使得数据源能够输出-U 消息，从而省去ChangelogNormalize。
在 MongoDB 6.0 版本前，MongoDB 原始的 oplog.rs 只有 INSERT, UPDATE, REPLACE, DELETE 这几种操作类型，没有保留更新前的信息，不能输出-U 消息，在 Flink 中只能实现 UPSERT 语义。在使用MongoDBTableSource 时，Flink planner 会自动进行 ChangelogNormalize 优化，补齐缺失的 -U 消息，输出完整的 +I, -U， +U， -D 四种消息， 代价是 ChangelogNormalize 优化的代价是该节点会保存之前所有 key 的状态。所以，如果是 DataStream 作业直接使用 MongoDBSource，如果没有 Flink planner 的优化，将不会自动进行 ChangelogNormalize，所以不能直接获取 —U 消息。想要获取更新前镜像值，需要自己管理状态，如果不希望自己管理状态，可以将 MongoDBTableSource 转换为 ChangelogStream 或者 RetractStream，借助 Flink planner 的优化能力补齐更新前镜像值，示例如下：
tEnv.executeSql(&#34;CREATE TABLE orders ( ... ) WITH ( &#39;connector&#39;=&#39;mongodb-cdc&#39;,... )&#34;); Table table = tEnv.from(&#34;orders&#34;) .select($(&#34;*&#34;)); tEnv.toChangelogStream(table) .print() .setParallelism(1); env.execute(); Q4: MongoDB CDC 支持订阅多个集合吗？ # 支持订阅整库的 collection，例如配置 database 为 &lsquo;mgdb&rsquo;，并且配置 collection 为空字符串，则会订阅 &lsquo;mgdb&rsquo; 库下所有 collection。
也支持通过正则表达式匹配 collection，如果要监控的集合名称中包含正则表达式特殊字符，则 collection 参数必须配置为完全限定的名字空间（数据库名称.集合名称），否则无法捕获对应 collection 的变更。
Q5: MongoDB CDC 支持 MongoDB 的版本是哪些？ # MongoDB CDC 基于 ChangeStream 特性实现，ChangeStream 是 MongoDB 3.6 推出的新特性。MongoDB CDC 理论上支持 3.6 及以上版本，建议运行版本 &gt;= 4.0, 在低于3.6版本执行时，会出现错误: Unrecognized pipeline stage name: &lsquo;$changeStream&rsquo; 。
Q6: MongoDB CDC 支持 MongoDB 的运行模式是什么？ # ChangeStream 需要 MongoDB 以副本集或者分片模式运行，本地测试可以使用单机版副本集 rs.initiate() 。在 standalone 模式下会出现错误：The $changestage is only supported on replica sets.
Q7: MongoDB CDC 报错用户名密码错误, 但其他组件使用该用户名密码都能正常连接，这是什么原因？ # 如果用户不是在默认的admin数据库下创建的，需要在with参数里加下 &lsquo;connection.options&rsquo; = &lsquo;authSource=用户所在的db&rsquo;。
Q8: MongoDB CDC 是否支持 debezium 相关的参数？ # 不支持的，因为 MongoDB CDC 连接器是在 Flink CDC 项目中独立开发，并不依赖Debezium项目，所以不支持。
Oracle CDC FAQ # Q1: Oracle CDC 的归档日志增长很快，且读取 log 慢？ # 可以使用在线挖掘的模式，不写入数据字典到 redo log 中，但是这样无法处理 DDL 语句。 生产环境默认策略读取 log 较慢，且默认策略会写入数据字典信息到 redo log 中导致日志量增加较多，可以添加如下 debezium 的配置项。 &rsquo;log.mining.strategy&rsquo; = &lsquo;online_catalog&rsquo;,&rsquo;log.mining.continuous.mine&rsquo; = &rsquo;true&rsquo;。如果使用 SQL 的方式，则需要在配置项中加上前缀 &lsquo;debezium.&rsquo;,即：
&#39;debezium.log.mining.strategy&#39; = &#39;online_catalog&#39;, &#39;debezium.log.mining.continuous.mine&#39; = &#39;true&#39; Q2: 作业报错 Caused by: io.debezium.DebeziumException: Supplemental logging not configured for table xxx. Use command: ALTER TABLE xxx ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS， 怎么办呢？ # 对于 oracle11 版本，debezium 会默认把 tableIdCaseInsensitive 设置为true, 导致表名被更新为小写，因此在oracle中查询不到 这个表补全日志设置，导致误报这个Supplemental logging not configured for table 错误”。 添加 debezium 的配置项 &lsquo;database.tablename.case.insensitive&rsquo; = &lsquo;false&rsquo;， 如果使用 SQL 的方式，则在表的 option 中添加配置项 &lsquo;debezium.database.tablename.case.insensitive&rsquo; = &lsquo;false&rsquo;
Q3: Oracle CDC 如何切换成 XStream 的方式？ # 添加 debezium 的配置项 &lsquo;database.connection.adpter&rsquo; = &lsquo;xstream&rsquo;， 如果使用 SQL 的方式，则在表的 option 中添加配置项 &lsquo;debezium.database.connection.adpter&rsquo; = &lsquo;xstream&rsquo;
Q4: Oracle CDC 的 database-name 和 schema-name 分别是什么? # database-name 是数据库示例的名字，也就是 Oracle 的 SID schema-name 是表对应的 schema，一般而言，一个用户就对应一个 schema, 该用户的 schema 名等于用户名，并作为该用户缺省 schema。所以 schema-name 一般都是创建这个表的用户名，但是如果创建表的时候指定了 schema，则指定的 schema 则为 schema-name。 比如用 CREATE TABLE aaaa.testtable(xxxx) 的方式成功创建了表 testtable， 则 aaaa 为 schema-name。
`}),e.add({id:10,href:"/flink/flink-cdc-docs-master/zh/docs/get-started/introduction/",title:"项目介绍",section:"入门指南",content:` 欢迎使用 Flink CDC 🎉 # Flink CDC 是一个基于流的数据集成工具，旨在为用户提供一套功能更加全面的编程接口（API）。 该工具使得用户能够以 YAML 配置文件的形式，优雅地定义其 ETL（Extract, Transform, Load）流程，并协助用户自动化生成定制化的 Flink 算子并且提交 Flink 作业。 Flink CDC 在任务提交过程中进行了优化，并且增加了一些高级特性，如表结构变更自动同步（Schema Evolution）、数据转换（Data Transformation）、整库同步（Full Database Synchronization）以及 精确一次（Exactly-once）语义。
Flink CDC 深度集成并由 Apache Flink 驱动，提供以下核心功能：
✅ 端到端的数据集成框架 ✅ 为数据集成的用户提供了易于构建作业的 API ✅ 支持在 Source 和 Sink 中处理多个表 ✅ 整库同步 ✅具备表结构变更自动同步的能力（Schema Evolution）， 如何使用 Flink CDC # Flink CDC 提供了基于 YAML 格式的用户 API，更适合于数据集成场景。以下是一个 YAML 文件的示例，它定义了一个数据管道(Pipeline)，该Pipeline从 MySQL 捕获实时变更，并将它们同步到 Apache Doris：
source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: doris fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; table.create.properties.light_schema_change: true table.create.properties.replication_num: 1 pipeline: name: Sync MySQL Database to Doris parallelism: 2 通过使用 flink-cdc.sh 提交 YAML 文件，一个 Flink 作业将会被编译并部署到指定的 Flink 集群。 请参考 核心概念 以获取 Pipeline 支持的所有功能的完整文档说明。
编写你的第一个 Flink CDC Pipeline # 浏览 Flink CDC 文档，开始创建您的第一个实时数据集成管道(Pipeline)。
快速开始 # 查看快速入门指南，了解如何建立一个 Flink CDC Pipeline：
MySQL to Apache Doris MySQL to StarRocks 理解核心概念 # 熟悉我们在 Flink CDC 中引入的核心概念，并尝试构建更复杂的数据Pipeline：
Data Pipeline Data Source Data Sink Table ID Transform Route 提交 Pipeline 到 Flink 集群 # 了解如何将 Pipeline 提交到运行在不同部署模式下的 Flink 集群：
standalone Kubernetes YARN 开发与贡献 # 如果您想要将 Flink CDC 连接到您定制化的外部系统，或者想要为框架本身做出贡献，以下这些部分可能会有所帮助：
理解 Flink CDC API，开发您自己的Flink CDC 连接器。 了解如何向 Flink CDC 提交贡献 查看 Flink CDC 使用的许可证 `}),e.add({id:11,href:"/flink/flink-cdc-docs-master/zh/docs/core-concept/data-source/",title:"Data Source",section:"核心概念",content:` Definition # Data Source is used to access metadata and read the changed data from external systems.
A Data Source can read data from multiple tables simultaneously.
Parameters # To describe a data source, the follows are required:
parameter meaning optional/required type The type of the source, such as mysql. required name The name of the source, which is user-defined (a default value provided). optional configurations of Data Source Configurations to build the Data Source e.g. connection configurations and source table properties. optional Example # We could use yaml files to define a mysql source:
source: type: mysql name: mysql-source #optional，description information host: localhost port: 3306 username: admin password: pass tables: adb.*, bdb.user_table_[0-9]+, [app|web]_order_\\.* `}),e.add({id:12,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/db2-tutorial/",title:"Db2 教程",section:"Flink CDC Sources 教程",content:` Demo: Db2 CDC to Elasticsearch # 1. Create docker-compose.yml file using following contents:
version: &#39;2.1&#39; services: db2: image: ruanhang/db2-cdc-demo:v1 privileged: true ports: - 50000:50000 environment: - LICENSE=accept - DB2INSTANCE=db2inst1 - DB2INST1_PASSWORD=admin - DBNAME=testdb - ARCHIVE_LOGS=true elasticsearch: image: elastic/elasticsearch:7.6.0 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34; - discovery.type=single-node ports: - &#34;9200:9200&#34; - &#34;9300:9300&#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 ports: - &#34;5601:5601&#34; volumes: - /var/run/docker.sock:/var/run/docker.sock The Docker Compose environment consists of the following containers:
Db2: db2 server and a pre-populated products table in the database testdb. Elasticsearch: store the result of the products table. Kibana: mainly used to visualize the data in Elasticsearch To start all containers, run the following command in the directory that contains the docker-compose.yml file.
docker-compose up -d This command automatically starts all the containers defined in the Docker Compose configuration in a detached mode. Run docker ps to check whether these containers are running properly. You can also visit http://localhost:5601/ to see if Kibana is running normally.
Don’t forget to run the following command to stop all containers after you finished the tutorial:
docker-compose down 2. Download following JAR package to &lt;FLINK_HOME&gt;/lib
Download links are available only for stable releases, SNAPSHOT dependencies need to be built based on master or release branches by yourself.
flink-sql-connector-elasticsearch7-3.0.1-1.17.jar flink-sql-connector-db2-cdc-3.0-SNAPSHOT.jar 3. Launch a Flink cluster and start a Flink SQL CLI
Execute following SQL statements in the Flink SQL CLI:
-- Flink SQL -- checkpoint every 3000 milliseconds Flink SQL&gt; SET execution.checkpointing.interval = 3s; Flink SQL&gt; CREATE TABLE products ( ID INT NOT NULL, NAME STRING, DESCRIPTION STRING, WEIGHT DECIMAL(10,3), PRIMARY KEY (ID) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;db2-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;50000&#39;, &#39;username&#39; = &#39;db2inst1&#39;, &#39;password&#39; = &#39;admin&#39;, &#39;database-name&#39; = &#39;TESTDB&#39;, &#39;table-name&#39; = &#39;DB2INST1.PRODUCTS&#39; ); Flink SQL&gt; CREATE TABLE es_products ( ID INT NOT NULL, NAME STRING, DESCRIPTION STRING, WEIGHT DECIMAL(10,3), PRIMARY KEY (ID) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;elasticsearch-7&#39;, &#39;hosts&#39; = &#39;http://localhost:9200&#39;, &#39;index&#39; = &#39;enriched_products_1&#39; ); Flink SQL&gt; INSERT INTO es_products SELECT * FROM products; 4. Check result in Elasticsearch
Check the data has been written to Elasticsearch successfully, you can visit Kibana to see the data.
5. Make changes in Db2 and watch result in Elasticsearch
Enter Db2&rsquo;s container to make some changes in Db2, then you can see the result in Elasticsearch will change after executing every SQL statement:
docker exec -it \${containerId} /bin/bash su db2inst1 db2 connect to testdb # enter db2 and execute sqls db2 UPDATE DB2INST1.PRODUCTS SET DESCRIPTION=&#39;18oz carpenter hammer&#39; WHERE ID=106; INSERT INTO DB2INST1.PRODUCTS VALUES (default,&#39;jacket&#39;,&#39;water resistent white wind breaker&#39;,0.2); INSERT INTO DB2INST1.PRODUCTS VALUES (default,&#39;scooter&#39;,&#39;Big 2-wheel scooter &#39;,5.18); DELETE FROM DB2INST1.PRODUCTS WHERE ID=111; Back to top
`}),e.add({id:13,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/",title:"Flink Source 连接器",section:"连接器",content:" "}),e.add({id:14,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/mysql-cdc/",title:"MySQL",section:"Flink Source 连接器",content:` MySQL CDC 连接器 # MySQL CDC 连接器允许从 MySQL 数据库读取快照数据和增量数据。本文描述了如何设置 MySQL CDC 连接器来对 MySQL 数据库运行 SQL 查询。
支持的数据库 # Connector Database Driver mysql-cdc MySQL: 5.6, 5.7, 8.0.x RDS MySQL: 5.6, 5.7, 8.0.x PolarDB MySQL: 5.6, 5.7, 8.0.x Aurora MySQL: 5.6, 5.7, 8.0.x MariaDB: 10.x PolarDB X: 2.0.1 JDBC Driver: 8.0.27 依赖 # 为了设置 MySQL CDC 连接器，下表提供了使用构建自动化工具（如 Maven 或 SBT ）和带有 SQL JAR 包的 SQL 客户端的两个项目的依赖关系信息。
Maven dependency # &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-mysql-cdc&lt;/artifactId&gt; &lt;!-- 请使用已发布的版本依赖，snapshot 版本的依赖需要本地自行编译。 --&gt; &lt;version&gt;3.2-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; SQL Client JAR # 下载链接仅在已发布版本可用，请在文档网站左下角选择浏览已发布的版本。
下载 flink-sql-connector-mysql-cdc-3.0.1.jar 到 &lt;FLINK_HOME&gt;/lib/ 目录下。
注意: 参考 flink-sql-connector-mysql-cdc 当前已发布的所有版本都可以在 Maven 中央仓库获取。
由于 MySQL Connector 采用的 GPLv2 协议与 Flink CDC 项目不兼容，我们无法在 jar 包中提供 MySQL 连接器。 您可能需要手动配置以下依赖：
依赖名称 说明 mysql:mysql-connector-java:8.0.27 用于连接到 MySQL 数据库。 配置 MySQL 服务器 # 你必须定义一个 MySQL 用户，该用户对 MySQL CDC 连接器监视的所有数据库都应该具有所需的权限。
创建 MySQL 用户： mysql&gt; CREATE USER &#39;user&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;password&#39;; 向用户授予所需的权限： mysql&gt; GRANT SELECT, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO &#39;user&#39; IDENTIFIED BY &#39;password&#39;; 注意: 在 scan.incremental.snapshot.enabled 参数已启用时（默认情况下已启用）时，不再需要授予 reload 权限。
刷新用户权限： mysql&gt; FLUSH PRIVILEGES; 查看更多用户权限问题请参考 权限说明.
注意事项 # 为每个 Reader 设置不同的 Server id # 每个用于读取 binlog 的 MySQL 数据库客户端都应该有一个唯一的 id，称为 Server id。 MySQL 服务器将使用此 id 来维护网络连接和 binlog 位置。 因此，如果不同的作业共享相同的 Server id， 则可能导致从错误的 binlog 位置读取数据。 因此，建议通过为每个 Reader 设置不同的 Server id SQL Hints, 假设 Source 并行度为 4, 我们可以使用 SELECT * FROM source_table /*+ OPTIONS('server-id'='5401-5404') */ ; 来为 4 个 Source readers 中的每一个分配唯一的 Server id。
设置 MySQL 会话超时时间 # 当为大型数据库创建初始一致快照时，你建立的连接可能会在读取表时碰到超时问题。你可以通过在 MySQL 侧配置 interactive_timeout 和 wait_timeout 来缓解此类问题。
interactive_timeout: 服务器在关闭交互连接之前等待活动的秒数。 更多信息请参考 MySQL documentations. wait_timeout: 服务器在关闭非交互连接之前等待活动的秒数。 更多信息请参考 MySQL documentations. 如何创建 MySQL CDC 表 # MySQL CDC 表可以定义如下：
-- 每 3 秒做一次 checkpoint，用于测试，生产配置建议5到10分钟 Flink SQL&gt; SET &#39;execution.checkpointing.interval&#39; = &#39;3s&#39;; -- 在 Flink SQL中注册 MySQL 表 &#39;orders&#39; Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY(order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;username&#39; = &#39;root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;orders&#39;); -- 从订单表读取全量数据(快照)和增量数据(binlog) Flink SQL&gt; SELECT * FROM orders; 连接器选项 # Option Required Default Type Description connector required (none) String 指定要使用的连接器, 这里应该是 'mysql-cdc'. hostname required (none) String MySQL 数据库服务器的 IP 地址或主机名。 username required (none) String 连接到 MySQL 数据库服务器时要使用的 MySQL 用户的名称。 password required (none) String 连接 MySQL 数据库服务器时使用的密码。 database-name required (none) String 要监视的 MySQL 服务器的数据库名称。数据库名称还支持正则表达式，以监视多个与正则表达式匹配的表。 table-name required (none) String 需要监视的 MySQL 数据库的表名。表名支持正则表达式，以监视满足正则表达式的多个表。注意：MySQL CDC 连接器在正则匹配表名时，会把用户填写的 database-name， table-name 通过字符串 \`\\\\.\` 连接成一个全路径的正则表达式，然后使用该正则表达式和 MySQL 数据库中表的全限定名进行正则匹配。 port optional 3306 Integer MySQL 数据库服务器的整数端口号。 server-id optional (none) String 读取数据使用的 server id，server id 可以是个整数或者一个整数范围，比如 '5400' 或 '5400-5408', 建议在 'scan.incremental.snapshot.enabled' 参数为启用时，配置成整数范围。因为在当前 MySQL 集群中运行的所有 slave 节点，标记每个 salve 节点的 id 都必须是唯一的。 所以当连接器加入 MySQL 集群作为另一个 slave 节点（并且具有唯一 id 的情况下），它就可以读取 binlog。 默认情况下，连接器会在 5400 和 6400 之间生成一个随机数，但是我们建议用户明确指定 Server id。 scan.incremental.snapshot.enabled optional true Boolean 增量快照是一种读取表快照的新机制，与旧的快照机制相比， 增量快照有许多优点，包括： （1）在快照读取期间，Source 支持并发读取， （2）在快照读取期间，Source 支持进行 chunk 粒度的 checkpoint， （3）在快照读取之前，Source 不需要数据库锁权限。 如果希望 Source 并行运行，则每个并行 Readers 都应该具有唯一的 Server id，所以 Server id 必须是类似 \`5400-6400\` 的范围，并且该范围必须大于并行度。 请查阅 增量快照读取 章节了解更多详细信息。 scan.incremental.snapshot.chunk.size optional 8096 Integer 表快照的块大小（行数），读取表的快照时，捕获的表被拆分为多个块。 scan.snapshot.fetch.size optional 1024 Integer 读取表快照时每次读取数据的最大条数。 scan.startup.mode optional initial String MySQL CDC 消费者可选的启动模式， 合法的模式为 "initial"，"earliest-offset"，"latest-offset"，"specific-offset" 和 "timestamp"。 请查阅 启动模式 章节了解更多详细信息。 scan.startup.specific-offset.file optional (none) String 在 "specific-offset" 启动模式下，启动位点的 binlog 文件名。 scan.startup.specific-offset.pos optional (none) Long 在 "specific-offset" 启动模式下，启动位点的 binlog 文件位置。 scan.startup.specific-offset.gtid-set optional (none) String 在 "specific-offset" 启动模式下，启动位点的 GTID 集合。 scan.startup.specific-offset.skip-events optional (none) Long 在指定的启动位点后需要跳过的事件数量。 scan.startup.specific-offset.skip-rows optional (none) Long 在指定的启动位点后需要跳过的数据行数量。 server-time-zone optional (none) String 数据库服务器中的会话时区， 例如： "Asia/Shanghai". 它控制 MYSQL 中的时间戳类型如何转换为字符串。 更多请参考 这里. 如果没有设置，则使用ZoneId.systemDefault()来确定服务器时区。 debezium.min.row. count.to.stream.result optional 1000 Integer 在快照操作期间，连接器将查询每个包含的表，以生成该表中所有行的读取事件。 此参数确定 MySQL 连接是否将表的所有结果拉入内存（速度很快，但需要大量内存）， 或者结果是否需要流式传输（传输速度可能较慢，但适用于非常大的表）。 该值指定了在连接器对结果进行流式处理之前，表必须包含的最小行数，默认值为1000。将此参数设置为\`0\`以跳过所有表大小检查，并始终在快照期间对所有结果进行流式处理。 connect.timeout optional 30s Duration 连接器在尝试连接到 MySQL 数据库服务器后超时前应等待的最长时间。 connect.max-retries optional 3 Integer 连接器应重试以建立 MySQL 数据库服务器连接的最大重试次数。 connection.pool.size optional 20 Integer 连接池大小。 jdbc.properties.* optional 20 String 传递自定义 JDBC URL 属性的选项。用户可以传递自定义属性，如 'jdbc.properties.useSSL' = 'false'. heartbeat.interval optional 30s Duration 用于跟踪最新可用 binlog 偏移的发送心跳事件的间隔。 debezium.* optional (none) String 将 Debezium 的属性传递给 Debezium 嵌入式引擎，该引擎用于从 MySQL 服务器捕获数据更改。 For example: 'debezium.snapshot.mode' = 'never'. 查看更多关于 Debezium 的 MySQL 连接器属性 scan.incremental.close-idle-reader.enabled optional false Boolean 是否在快照结束后关闭空闲的 Reader。 此特性需要 flink 版本大于等于 1.14 并且 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' 需要设置为 true。 支持的元数据 # 下表中的元数据可以在 DDL 中作为只读（虚拟）meta 列声明。
Key DataType Description table_name STRING NOT NULL 当前记录所属的表名称。 database_name STRING NOT NULL 当前记录所属的库名称。 op_ts TIMESTAMP_LTZ(3) NOT NULL 当前记录表在数据库中更新的时间。 如果从表的快照而不是 binlog 读取记录，该值将始终为0。 下述创建表示例展示元数据列的用法：
CREATE TABLE products ( db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, table_name STRING METADATA FROM &#39;table_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY(order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;username&#39; = &#39;root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;orders&#39; ); 下述创建表示例展示使用正则表达式匹配多张库表的用法：
CREATE TABLE products ( db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, table_name STRING METADATA FROM &#39;table_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY(order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;username&#39; = &#39;root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;(^(test).*|^(tpc).*|txc|.*[p$]|t{2})&#39;, &#39;table-name&#39; = &#39;(t[5-8]|tt)&#39; ); 匹配示例 表达式 描述 前缀匹配 ^(test).* 匹配前缀为test的数据库名或表名，例如test1、test2等。 后缀匹配 .*[p$] 匹配后缀为p的数据库名或表名，例如cdcp、edcp等。 特定匹配 txc 匹配具体的数据库名或表名。 进行库表匹配时，会使用正则表达式 database-name\\\\.table-name 来与MySQL表的全限定名做匹配，所以该例子使用 (^(test).*|^(tpc).*|txc|.*[p$]|t{2})\\\\.(t[5-8]|tt)，可以匹配到表 txc.tt、test2.test5。
支持的特性 # 增量快照读取 # 增量快照读取是一种读取表快照的新机制。与旧的快照机制相比，增量快照具有许多优点，包括：
（1）在快照读取期间，Source 支持并发读取， （2）在快照读取期间，Source 支持进行 chunk 粒度的 checkpoint， （3）在快照读取之前，Source 不需要数据库锁权限。 如果希望 source 并行运行，则每个并行 reader 都应该具有唯一的 server id，因此server id的范围必须类似于 5400-6400， 且范围必须大于并行度。在增量快照读取过程中，MySQL CDC Source 首先通过表的主键将表划分成多个块（chunk）， 然后 MySQL CDC Source 将多个块分配给多个 reader 以并行读取表的数据。
并发读取 # 增量快照读取提供了并行读取快照数据的能力。 你可以通过设置作业并行度的方式来控制 Source 的并行度 parallelism.default. For example, in SQL CLI:
Flink SQL&gt; SET &#39;parallelism.default&#39; = 8; 全量阶段支持 checkpoint # 增量快照读取提供了在区块级别执行检查点的能力。它使用新的快照读取机制解决了以前版本中的检查点超时问题。
无锁算法 # MySQL CDC source 使用 增量快照算法, 避免了数据库锁的使用，因此不需要 “RELOAD” 权限。
MySQL高可用性支持 # mysql cdc 连接器通过使用 GTID 提供 MySQL 高可用集群的高可用性信息。为了获得高可用性， MySQL集群需要启用 GTID 模式，MySQL 配置文件中的 GTID 模式应该包含以下设置：
gtid_mode = on enforce_gtid_consistency = on 如果监控的MySQL服务器地址包含从实例，则需要对MySQL配置文件设置以下设置。设置 log slave updates=1 允许从实例也将从主实例同步的数据写入其binlog， 这确保了mysql cdc连接器可以使用从实例中的全部数据。
gtid_mode = on enforce_gtid_consistency = on log-slave-updates = 1 MySQL 集群中你监控的服务器出现故障后, 你只需将受监视的服务器地址更改为其他可用服务器，然后从最新的检查点/保存点重新启动作业, 作业将从 checkpoint/savepoint 恢复，不会丢失任何记录。
建议为 MySQL 集群配置 DNS（域名服务）或 VIP（虚拟 IP 地址）， 使用mysql cdc连接器的 DNS 或 VIP 地址， DNS或VIP将自动将网络请求路由到活动MySQL服务器。 这样，你就不再需要修改地址和重新启动管道。
MySQL心跳事件支持 # 如果表不经常更新，则 binlog 文件或 GTID 集可能已在其最后提交的 binlog 位置被清理。 在这种情况下，CDC 作业可能会重新启动失败。因此心跳事件将帮助更新 binlog 位置。 默认情况下，MySQL CDC Source 启用心跳事件，间隔设置为30秒。 可以使用表选项heartbeat指定间隔。或将选项设置为0s以禁用心跳事件。
增量快照读取的工作原理 # 当 MySQL CDC Source 启动时，它并行读取表的快照，然后以单并行度的方式读取表的 binlog。
在快照阶段，根据表的主键和表行的大小将快照切割成多个快照块。 快照块被分配给多个快照读取器。每个快照读取器使用 区块读取算法 并将读取的数据发送到下游。 Source 会管理块的进程状态（完成或未完成），因此快照阶段的 Source 可以支持块级别的 checkpoint。 如果发生故障，可以恢复 Source 并继续从最后完成的块中读取块。
所有快照块完成后，Source 将继续在单个任务中读取 binlog。 为了保证快照记录和 binlog 记录的全局数据顺序，binlog reader 将开始读取数据直到快照块完成后并有一个完整的 checkpoint，以确保所有快照数据已被下游消费。 binlog reader 在状态中跟踪所使用的 binlog 位置，因此 binlog 阶段的 Source 可以支持行级别的 checkpoint。
Flink 定期为 Source 执行 checkpoint，在故障转移的情况下，作业将重新启动并从最后一个成功的 checkpoint 状态恢复，并保证只执行一次语义。
全量阶段分片算法 # 在执行增量快照读取时，MySQL CDC source 需要一个用于分片的的算法。 MySQL CDC Source 使用主键列将表划分为多个分片（chunk）。 默认情况下，MySQL CDC source 会识别表的主键列，并使用主键中的第一列作为用作分片列。 如果表中没有主键， 增量快照读取将失败，你可以禁用 scan.incremental.snapshot.enabled 来回退到旧的快照读取机制。
对于数值和自动增量拆分列，MySQL CDC Source 按固定步长高效地拆分块。 例如，如果你有一个主键列为id的表，它是自动增量 BIGINT 类型，最小值为0，最大值为100， 和表选项 scan.incremental.snapshot.chunk.size 大小 value为25，表将被拆分为以下块：
(-∞, 25), [25, 50), [50, 75), [75, 100), [100, +∞) 对于其他主键列类型， MySQL CDC Source 将以下形式执行语句： SELECT MAX(STR_ID) AS chunk_high FROM (SELECT * FROM TestTable WHERE STR_ID &gt; 'uuid-001' limit 25) 来获得每个区块的低值和高值， 分割块集如下所示：
(-∞, &#39;uuid-001&#39;), [&#39;uuid-001&#39;, &#39;uuid-009&#39;), [&#39;uuid-009&#39;, &#39;uuid-abc&#39;), [&#39;uuid-abc&#39;, &#39;uuid-def&#39;), [uuid-def, +∞). Chunk 读取算法 # 对于上面的示例MyTable，如果 MySQL CDC Source 并行度设置为 4，MySQL CDC Source 将在每一个 executes 运行 4 个 Readers 通过偏移信号算法 获取快照区块的最终一致输出。 偏移信号算法简单描述如下：
(1) 将当前 binlog 位置记录为LOW偏移量 (2) 通过执行语句读取并缓冲快照区块记录 SELECT * FROM MyTable WHERE id &gt; chunk_low AND id &lt;= chunk_high (3) 将当前 binlog 位置记录为HIGH偏移量 (4) 从LOW偏移量到HIGH偏移量读取属于快照区块的 binlog 记录 (5) 将读取的 binlog 记录向上插入缓冲区块记录，并发出缓冲区中的所有记录作为快照区块的最终输出（全部作为插入记录） (6) 继续读取并发出属于 单个 binlog reader 中HIGH偏移量之后的区块的 binlog 记录。 该算法的是基于 DBLog Paper 并结合 Flink 的一个变种, 请参考它了解更多详细信息。
注意: 如果主键的实际值在其范围内分布不均匀，则在增量快照读取时可能会导致任务不平衡。
Exactly-Once 处理 # MySQL CDC 连接器是一个 Flink Source 连接器，它将首先读取表快照块，然后继续读取 binlog， 无论是在快照阶段还是读取 binlog 阶段，MySQL CDC 连接器都会在处理时准确读取数据，即使任务出现了故障。
启动模式 # 配置选项scan.startup.mode指定 MySQL CDC 使用者的启动模式。有效枚举包括：
initial （默认）：在第一次启动时对受监视的数据库表执行初始快照，并继续读取最新的 binlog。 earliest-offset：跳过快照阶段，从可读取的最早 binlog 位点开始读取 latest-offset：首次启动时，从不对受监视的数据库表执行快照， 连接器仅从 binlog 的结尾处开始读取，这意味着连接器只能读取在连接器启动之后的数据更改。 specific-offset：跳过快照阶段，从指定的 binlog 位点开始读取。位点可通过 binlog 文件名和位置指定，或者在 GTID 在集群上启用时通过 GTID 集合指定。 timestamp：跳过快照阶段，从指定的时间戳开始读取 binlog 事件。 例如使用 DataStream API:
MySQLSource.builder() .startupOptions(StartupOptions.earliest()) // 从最早位点启动 .startupOptions(StartupOptions.latest()) // 从最晚位点启动 .startupOptions(StartupOptions.specificOffset(&#34;mysql-bin.000003&#34;, 4L) // 从指定 binlog 文件名和位置启动 .startupOptions(StartupOptions.specificOffset(&#34;24DA167-0C0C-11E8-8442-00059A3C7B00:1-19&#34;)) // 从 GTID 集合启动 .startupOptions(StartupOptions.timestamp(1667232000000L) // 从时间戳启动 ... .build() 使用 SQL:
CREATE TABLE mysql_source (...) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;scan.startup.mode&#39; = &#39;earliest-offset&#39;, -- 从最早位点启动 &#39;scan.startup.mode&#39; = &#39;latest-offset&#39;, -- 从最晚位点启动 &#39;scan.startup.mode&#39; = &#39;specific-offset&#39;, -- 从特定位点启动 &#39;scan.startup.mode&#39; = &#39;timestamp&#39;, -- 从特定位点启动 &#39;scan.startup.specific-offset.file&#39; = &#39;mysql-bin.000003&#39;, -- 在特定位点启动模式下指定 binlog 文件名 &#39;scan.startup.specific-offset.pos&#39; = &#39;4&#39;, -- 在特定位点启动模式下指定 binlog 位置 &#39;scan.startup.specific-offset.gtid-set&#39; = &#39;24DA167-0C0C-11E8-8442-00059A3C7B00:1-19&#39;, -- 在特定位点启动模式下指定 GTID 集合 &#39;scan.startup.timestamp-millis&#39; = &#39;1667232000000&#39; -- 在时间戳启动模式下指定启动时间戳 ... ) 注意：
MySQL source 会在 checkpoint 时将当前位点以 INFO 级别打印到日志中，日志前缀为 &ldquo;Binlog offset on checkpoint {checkpoint-id}&quot;。 该日志可以帮助将作业从某个 checkpoint 的位点开始启动的场景。 如果捕获变更的表曾经发生过表结构变化，从最早位点、特定位点或时间戳启动可能会发生错误，因为 Debezium 读取器会在内部保存当前的最新表结构，结构不匹配的早期数据无法被正确解析。 DataStream Source # import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.cdc.connectors.mysql.source.MySqlSource; public class MySqlSourceExample { public static void main(String[] args) throws Exception { MySqlSource&lt;String&gt; mySqlSource = MySqlSource.&lt;String&gt;builder() .hostname(&#34;yourHostname&#34;) .port(yourPort) .databaseList(&#34;yourDatabaseName&#34;) // 设置捕获的数据库， 如果需要同步整个数据库，请将 tableList 设置为 &#34;.*&#34;. .tableList(&#34;yourDatabaseName.yourTableName&#34;) // 设置捕获的表 .username(&#34;yourUsername&#34;) .password(&#34;yourPassword&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // 将 SourceRecord 转换为 JSON 字符串 .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置 3s 的 checkpoint 间隔 env.enableCheckpointing(3000); env .fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), &#34;MySQL Source&#34;) // 设置 source 节点的并行度为 4 .setParallelism(4) .print().setParallelism(1); // 设置 sink 节点并行度为 1 env.execute(&#34;Print MySQL Snapshot + Binlog&#34;); } } 动态加表 # 扫描新添加的表功能使你可以添加新表到正在运行的作业中，新添加的表将首先读取其快照数据，然后自动读取其变更日志。
想象一下这个场景：一开始， Flink 作业监控表 [product, user, address], 但几天后，我们希望这个作业还可以监控表 [order, custom]，这些表包含历史数据，我们需要作业仍然可以复用作业的已有状态，动态加表功能可以优雅地解决此问题。
以下操作显示了如何启用此功能来解决上述场景。 使用现有的 Flink CDC Source 作业，如下：
MySqlSource&lt;String&gt; mySqlSource = MySqlSource.&lt;String&gt;builder() .hostname(&#34;yourHostname&#34;) .port(yourPort) .scanNewlyAddedTableEnabled(true) // 启用扫描新添加的表功能 .databaseList(&#34;db&#34;) // 设置捕获的数据库 .tableList(&#34;db.product, db.user, db.address&#34;) // 设置捕获的表 [product, user, address] .username(&#34;yourUsername&#34;) .password(&#34;yourPassword&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // 将 SourceRecord 转换为 JSON 字符串 .build(); // 你的业务代码 如果我们想添加新表 [order, custom] 对于现有的 Flink 作业，只需更新 tableList() 将新增表 [order, custom] 加入并从已有的 savepoint 恢复作业。
Step 1: 使用 savepoint 停止现有的 Flink 作业。
$ ./bin/flink stop $Existing_Flink_JOB_ID Suspending job &#34;cca7bc1061d61cf15238e92312c2fc20&#34; with a savepoint. Savepoint completed. Path: file:/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab Step 2: 更新现有 Flink 作业的表列表选项。
更新 tableList() 参数. 编译更新后的作业，示例如下： MySqlSource&lt;String&gt; mySqlSource = MySqlSource.&lt;String&gt;builder() .hostname(&#34;yourHostname&#34;) .port(yourPort) .scanNewlyAddedTableEnabled(true) .databaseList(&#34;db&#34;) .tableList(&#34;db.product, db.user, db.address, db.order, db.custom&#34;) // 设置捕获的表 [product, user, address ,order, custom] .username(&#34;yourUsername&#34;) .password(&#34;yourPassword&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // 将 SourceRecord 转换为 JSON 字符串 .build(); // 你的业务代码 Step 3: 从 savepoint 还原更新后的 Flink 作业。
$ ./bin/flink run \\ --detached \\ --from-savepoint /tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab \\ ./FlinkCDCExample.jar 注意: 请参考文档 Restore the job from previous savepoint 了解更多详细信息。
关于无主键表 # 从2.4.0 版本开始支持无主键表，使用无主键表必须设置 scan.incremental.snapshot.chunk.key-column，且只能选择非空类型的一个字段。
在使用无主键表时，需要注意以下两种情况。
配置 scan.incremental.snapshot.chunk.key-column 时，如果表中存在索引，请尽量使用索引中的列来加快 select 速度。 无主键表的处理语义由 scan.incremental.snapshot.chunk.key-column 指定的列的行为决定： 如果指定的列不存在更新操作，此时可以保证 Exactly once 语义。 如果指定的列存在更新操作，此时只能保证 At least once 语义。但可以结合下游，通过指定下游主键，结合幂等性操作来保证数据的正确性。 数据类型映射 # MySQL type Flink SQL type NOTE TINYINT TINYINT SMALLINT
TINYINT UNSIGNED
TINYINT UNSIGNED ZEROFILL SMALLINT INT
MEDIUMINT
SMALLINT UNSIGNED
SMALLINT UNSIGNED ZEROFILL INT BIGINT
INT UNSIGNED
INT UNSIGNED ZEROFILL
MEDIUMINT UNSIGNED
MEDIUMINT UNSIGNED ZEROFILL BIGINT BIGINT UNSIGNED
BIGINT UNSIGNED ZEROFILL
SERIAL DECIMAL(20, 0) FLOAT
FLOAT UNSIGNED
FLOAT UNSIGNED ZEROFILL FLOAT REAL
REAL UNSIGNED
REAL UNSIGNED ZEROFILL
DOUBLE
DOUBLE UNSIGNED
DOUBLE UNSIGNED ZEROFILL
DOUBLE PRECISION
DOUBLE PRECISION UNSIGNED
DOUBLE PRECISION UNSIGNED ZEROFILL DOUBLE NUMERIC(p, s)
NUMERIC(p, s) UNSIGNED
NUMERIC(p, s) UNSIGNED ZEROFILL
DECIMAL(p, s)
DECIMAL(p, s) UNSIGNED
DECIMAL(p, s) UNSIGNED ZEROFILL
FIXED(p, s)
FIXED(p, s) UNSIGNED
FIXED(p, s) UNSIGNED ZEROFILL
where p <= 38
DECIMAL(p, s) NUMERIC(p, s)
NUMERIC(p, s) UNSIGNED
NUMERIC(p, s) UNSIGNED ZEROFILL
DECIMAL(p, s)
DECIMAL(p, s) UNSIGNED
DECIMAL(p, s) UNSIGNED ZEROFILL
FIXED(p, s)
FIXED(p, s) UNSIGNED
FIXED(p, s) UNSIGNED ZEROFILL
where 38 < p <= 65
STRING 在 MySQL 中，十进制数据类型的精度高达 65，但在 Flink 中，十进制数据类型的精度仅限于 38。所以，如果定义精度大于 38 的十进制列，则应将其映射到字符串以避免精度损失。在 MySQL 中，十进制数据类型的精度高达65，但在Flink中，十进制数据类型的精度仅限于38。所以，如果定义精度大于 38 的十进制列，则应将其映射到字符串以避免精度损失。 BOOLEAN
TINYINT(1)
BIT(1) BOOLEAN DATE DATE TIME [(p)] TIME [(p)] TIMESTAMP [(p)]
DATETIME [(p)] TIMESTAMP [(p)] CHAR(n) CHAR(n) VARCHAR(n) VARCHAR(n) BIT(n) BINARY(⌈n/8⌉) BINARY(n) BINARY(n) VARBINARY(N) VARBINARY(N) TINYTEXT
TEXT
MEDIUMTEXT
LONGTEXT
STRING TINYBLOB
BLOB
MEDIUMBLOB
LONGBLOB
BYTES 目前，对于 MySQL 中的 BLOB 数据类型，仅支持长度不大于 2147483647（2**31-1）的 blob。 YEAR INT ENUM STRING JSON STRING JSON 数据类型将在 Flink 中转换为 JSON 格式的字符串。 SET ARRAY&lt;STRING&gt; 因为 MySQL 中的 SET 数据类型是一个字符串对象，可以有零个或多个值 它应该始终映射到字符串数组。 GEOMETRY
POINT
LINESTRING
POLYGON
MULTIPOINT
MULTILINESTRING
MULTIPOLYGON
GEOMETRYCOLLECTION
STRING MySQL 中的空间数据类型将转换为具有固定 Json 格式的字符串。 请参考 MySQL 空间数据类型映射 章节了解更多详细信息。 空间数据类型映射 # MySQL中除GEOMETRYCOLLECTION之外的空间数据类型都会转换为 Json 字符串，格式固定，如：
{&#34;srid&#34;: 0 , &#34;type&#34;: &#34;xxx&#34;, &#34;coordinates&#34;: [0, 0]} 字段srid标识定义几何体的 SRS，如果未指定 SRID，则 SRID 0 是新几何体值的默认值。 由于 MySQL 8+ 在定义空间数据类型时只支持特定的 SRID，因此在版本较低的MySQL中，字段srid将始终为 0。
字段type标识空间数据类型，例如POINT/LINESTRING/POLYGON。
字段coordinates表示空间数据的坐标。
对于GEOMETRYCOLLECTION，它将转换为 Json 字符串，格式固定，如：
{&#34;srid&#34;: 0 , &#34;type&#34;: &#34;GeometryCollection&#34;, &#34;geometries&#34;: [{&#34;type&#34;:&#34;Point&#34;,&#34;coordinates&#34;:[10,10]}]} Geometrics字段是一个包含所有空间数据的数组。
不同空间数据类型映射的示例如下：
Spatial data in MySQL Json String converted in Flink POINT(1 1) {"coordinates":[1,1],"type":"Point","srid":0} LINESTRING(3 0, 3 3, 3 5) {"coordinates":[[3,0],[3,3],[3,5]],"type":"LineString","srid":0} POLYGON((1 1, 2 1, 2 2, 1 2, 1 1)) {"coordinates":[[[1,1],[2,1],[2,2],[1,2],[1,1]]],"type":"Polygon","srid":0} MULTIPOINT((1 1),(2 2)) {"coordinates":[[1,1],[2,2]],"type":"MultiPoint","srid":0} MultiLineString((1 1,2 2,3 3),(4 4,5 5)) {"coordinates":[[[1,1],[2,2],[3,3]],[[4,4],[5,5]]],"type":"MultiLineString","srid":0} MULTIPOLYGON(((0 0, 10 0, 10 10, 0 10, 0 0)), ((5 5, 7 5, 7 7, 5 7, 5 5))) {"coordinates":[[[[0,0],[10,0],[10,10],[0,10],[0,0]]],[[[5,5],[7,5],[7,7],[5,7],[5,5]]]],"type":"MultiPolygon","srid":0} GEOMETRYCOLLECTION(POINT(10 10), POINT(30 30), LINESTRING(15 15, 20 20)) {"geometries":[{"type":"Point","coordinates":[10,10]},{"type":"Point","coordinates":[30,30]},{"type":"LineString","coordinates":[[15,15],[20,20]]}],"type":"GeometryCollection","srid":0} Back to top
`}),e.add({id:15,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/mysql/",title:"MySQL",section:"Pipeline 连接器",content:` MySQL Connector # MySQL CDC Pipeline 连接器允许从 MySQL 数据库读取快照数据和增量数据，并提供端到端的整库数据同步能力。 本文描述了如何设置 MySQL CDC Pipeline 连接器。
依赖配置 # 由于 MySQL Connector 采用的 GPLv2 协议与 Flink CDC 项目不兼容，我们无法在 jar 包中提供 MySQL 连接器。 您可能需要手动配置以下依赖，并在提交 YAML 作业时使用 Flink CDC CLI 的 --jar 参数将其传入：
依赖名称 说明 mysql:mysql-connector-java:8.0.27 用于连接到 MySQL 数据库。 示例 # 从 MySQL 读取数据同步到 Doris 的 Pipeline 可以定义如下：
source: type: mysql name: MySQL Source hostname: 127.0.0.1 port: 3306 username: admin password: pass tables: adb.\\.*, bdb.user_table_[0-9]+, [app|web].order_\\.* server-id: 5401-5404 sink: type: doris name: Doris Sink fenodes: 127.0.0.1:8030 username: root password: pass pipeline: name: MySQL to Doris Pipeline parallelism: 4 连接器配置项 # Option Required Default Type Description hostname required (none) String MySQL 数据库服务器的 IP 地址或主机名。 port optional 3306 Integer MySQL 数据库服务器的整数端口号。 username required (none) String 连接到 MySQL 数据库服务器时要使用的 MySQL 用户的名称。 password required (none) String 连接 MySQL 数据库服务器时使用的密码。 tables required (none) String 需要监视的 MySQL 数据库的表名。表名支持正则表达式，以监视满足正则表达式的多个表。
需要注意的是，点号（.）被视为数据库和表名的分隔符。 如果需要在正则表达式中使用点（.）来匹配任何字符，必须使用反斜杠对点进行转义。
例如，db0.\\.*, db1.user_table_[0-9]+, db[1-2].[app|web]order_\\.* tables.exclude optional (none) String 需要排除的 MySQL 数据库的表名，参数会在tables参数后发生排除作用。表名支持正则表达式，以排除满足正则表达式的多个表。
用法和tables参数相同 schema-change.enabled optional true Boolean 是否发送模式更改事件，下游 sink 可以响应模式变更事件实现表结构同步，默认为true。 server-id optional (none) String 读取数据使用的 server id，server id 可以是个整数或者一个整数范围，比如 '5400' 或 '5400-5408', 建议在 'scan.incremental.snapshot.enabled' 参数为启用时，配置成整数范围。因为在当前 MySQL 集群中运行的所有 slave 节点，标记每个 salve 节点的 id 都必须是唯一的。 所以当连接器加入 MySQL 集群作为另一个 slave 节点（并且具有唯一 id 的情况下），它就可以读取 binlog。 默认情况下，连接器会在 5400 和 6400 之间生成一个随机数，但是我们建议用户明确指定 Server id。 scan.incremental.snapshot.chunk.size optional 8096 Integer 表快照的块大小（行数），读取表的快照时，捕获的表被拆分为多个块。 scan.snapshot.fetch.size optional 1024 Integer 读取表快照时每次读取数据的最大条数。 scan.startup.mode optional initial String MySQL CDC 消费者可选的启动模式， 合法的模式为 "initial"，"earliest-offset"，"latest-offset"，"specific-offset" 和 "timestamp"。 请查阅 启动模式 章节了解更多详细信息。 scan.startup.specific-offset.file optional (none) String 在 "specific-offset" 启动模式下，启动位点的 binlog 文件名。 scan.startup.specific-offset.pos optional (none) Long 在 "specific-offset" 启动模式下，启动位点的 binlog 文件位置。 scan.startup.specific-offset.gtid-set optional (none) String 在 "specific-offset" 启动模式下，启动位点的 GTID 集合。 scan.startup.specific-offset.skip-events optional (none) Long 在指定的启动位点后需要跳过的事件数量。 scan.startup.specific-offset.skip-rows optional (none) Long 在指定的启动位点后需要跳过的数据行数量。 connect.timeout optional 30s Duration 连接器在尝试连接到 MySQL 数据库服务器后超时前应等待的最长时间。 connect.max-retries optional 3 Integer 连接器应重试以建立 MySQL 数据库服务器连接的最大重试次数。 connection.pool.size optional 20 Integer 连接池大小。 jdbc.properties.* optional 20 String 传递自定义 JDBC URL 属性的选项。用户可以传递自定义属性，如 'jdbc.properties.useSSL' = 'false'. heartbeat.interval optional 30s Duration 用于跟踪最新可用 binlog 偏移的发送心跳事件的间隔。 debezium.* optional (none) String 将 Debezium 的属性传递给 Debezium 嵌入式引擎，该引擎用于从 MySQL 服务器捕获数据更改。 例如: 'debezium.snapshot.mode' = 'never'. 查看更多关于 Debezium 的 MySQL 连接器属性 scan.incremental.close-idle-reader.enabled optional false Boolean 是否在快照结束后关闭空闲的 Reader。 此特性需要 flink 版本大于等于 1.14 并且 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' 需要设置为 true。
若 flink 版本大于等于 1.15，'execution.checkpointing.checkpoints-after-tasks-finish.enabled' 默认值变更为 true，可以不用显式配置 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' = true。 启动模式 # 配置选项scan.startup.mode指定 MySQL CDC 使用者的启动模式。有效枚举包括：
initial （默认）：在第一次启动时对受监视的数据库表执行初始快照，并继续读取最新的 binlog。 earliest-offset：跳过快照阶段，从可读取的最早 binlog 位点开始读取 latest-offset：首次启动时，从不对受监视的数据库表执行快照， 连接器仅从 binlog 的结尾处开始读取，这意味着连接器只能读取在连接器启动之后的数据更改。 specific-offset：跳过快照阶段，从指定的 binlog 位点开始读取。位点可通过 binlog 文件名和位置指定，或者在 GTID 在集群上启用时通过 GTID 集合指定。 timestamp：跳过快照阶段，从指定的时间戳开始读取 binlog 事件。 数据类型映射 # MySQL type CDC type NOTE TINYINT(n) TINYINT SMALLINT
TINYINT UNSIGNED
TINYINT UNSIGNED ZEROFILL SMALLINT INT
YEAR
MEDIUMINT
MEDIUMINT UNSIGNED
MEDIUMINT UNSIGNED ZEROFILL
SMALLINT UNSIGNED
SMALLINT UNSIGNED ZEROFILL INT BIGINT
INT UNSIGNED
INT UNSIGNED ZEROFILL BIGINT BIGINT UNSIGNED
BIGINT UNSIGNED ZEROFILL
SERIAL DECIMAL(20, 0) FLOAT
FLOAT UNSIGNED
FLOAT UNSIGNED ZEROFILL FLOAT REAL
REAL UNSIGNED
REAL UNSIGNED ZEROFILL
DOUBLE
DOUBLE UNSIGNED
DOUBLE UNSIGNED ZEROFILL
DOUBLE PRECISION
DOUBLE PRECISION UNSIGNED
DOUBLE PRECISION UNSIGNED ZEROFILL DOUBLE NUMERIC(p, s)
NUMERIC(p, s) UNSIGNED
NUMERIC(p, s) UNSIGNED ZEROFILL
DECIMAL(p, s)
DECIMAL(p, s) UNSIGNED
DECIMAL(p, s) UNSIGNED ZEROFILL
FIXED(p, s)
FIXED(p, s) UNSIGNED
FIXED(p, s) UNSIGNED ZEROFILL
where p <= 38
DECIMAL(p, s) NUMERIC(p, s)
NUMERIC(p, s) UNSIGNED
NUMERIC(p, s) UNSIGNED ZEROFILL
DECIMAL(p, s)
DECIMAL(p, s) UNSIGNED
DECIMAL(p, s) UNSIGNED ZEROFILL
FIXED(p, s)
FIXED(p, s) UNSIGNED
FIXED(p, s) UNSIGNED ZEROFILL
where 38 < p <= 65
STRING 在 MySQL 中，十进制数据类型的精度高达 65，但在 Flink 中，十进制数据类型的精度仅限于 38。所以，如果定义精度大于 38 的十进制列，则应将其映射到字符串以避免精度损失。 BOOLEAN
TINYINT(1)
BIT(1) BOOLEAN DATE DATE TIME [(p)] TIME [(p)] TIMESTAMP [(p)] TIMESTAMP_LTZ [(p)] DATETIME [(p)] TIMESTAMP [(p)] CHAR(n) CHAR(n) VARCHAR(n) VARCHAR(n) BIT(n) BINARY(⌈(n + 7) / 8⌉) BINARY(n) BINARY(n) VARBINARY(N) VARBINARY(N) TINYTEXT
TEXT
MEDIUMTEXT
LONGTEXT
STRING TINYBLOB
BLOB
MEDIUMBLOB
LONGBLOB
BYTES 目前，对于 MySQL 中的 BLOB 数据类型，仅支持长度不大于 2147483647（2**31-1）的 blob。 ENUM STRING JSON STRING JSON 数据类型将在 Flink 中转换为 JSON 格式的字符串。 SET - 暂不支持 GEOMETRY
POINT
LINESTRING
POLYGON
MULTIPOINT
MULTILINESTRING
MULTIPOLYGON
GEOMETRYCOLLECTION
STRING MySQL 中的空间数据类型将转换为具有固定 Json 格式的字符串。 请参考 MySQL 空间数据类型映射 章节了解更多详细信息。 空间数据类型映射 # MySQL中除GEOMETRYCOLLECTION之外的空间数据类型都会转换为 Json 字符串，格式固定，如：
{&#34;srid&#34;: 0 , &#34;type&#34;: &#34;xxx&#34;, &#34;coordinates&#34;: [0, 0]} 字段srid标识定义几何体的 SRS，如果未指定 SRID，则 SRID 0 是新几何体值的默认值。 由于 MySQL 8+ 在定义空间数据类型时只支持特定的 SRID，因此在版本较低的MySQL中，字段srid将始终为 0。
字段type标识空间数据类型，例如POINT/LINESTRING/POLYGON。
字段coordinates表示空间数据的坐标。
对于GEOMETRYCOLLECTION，它将转换为 Json 字符串，格式固定，如：
{&#34;srid&#34;: 0 , &#34;type&#34;: &#34;GeometryCollection&#34;, &#34;geometries&#34;: [{&#34;type&#34;:&#34;Point&#34;,&#34;coordinates&#34;:[10,10]}]} Geometrics字段是一个包含所有空间数据的数组。
不同空间数据类型映射的示例如下：
Spatial data in MySQL Json String converted in Flink POINT(1 1) {"coordinates":[1,1],"type":"Point","srid":0} LINESTRING(3 0, 3 3, 3 5) {"coordinates":[[3,0],[3,3],[3,5]],"type":"LineString","srid":0} POLYGON((1 1, 2 1, 2 2, 1 2, 1 1)) {"coordinates":[[[1,1],[2,1],[2,2],[1,2],[1,1]]],"type":"Polygon","srid":0} MULTIPOINT((1 1),(2 2)) {"coordinates":[[1,1],[2,2]],"type":"MultiPoint","srid":0} MultiLineString((1 1,2 2,3 3),(4 4,5 5)) {"coordinates":[[[1,1],[2,2],[3,3]],[[4,4],[5,5]]],"type":"MultiLineString","srid":0} MULTIPOLYGON(((0 0, 10 0, 10 10, 0 10, 0 0)), ((5 5, 7 5, 7 7, 5 7, 5 5))) {"coordinates":[[[[0,0],[10,0],[10,10],[0,10],[0,0]]],[[[5,5],[7,5],[7,7],[5,7],[5,5]]]],"type":"MultiPolygon","srid":0} GEOMETRYCOLLECTION(POINT(10 10), POINT(30 30), LINESTRING(15 15, 20 20)) {"geometries":[{"type":"Point","coordinates":[10,10]},{"type":"Point","coordinates":[30,30]},{"type":"LineString","coordinates":[[15,15],[20,20]]}],"type":"GeometryCollection","srid":0} Back to top
`}),e.add({id:16,href:"/flink/flink-cdc-docs-master/zh/docs/get-started/quickstart/mysql-to-starrocks/",title:"MySQL 同步到 StarRocks",section:"快速开始",content:" Streaming ELT 同步 MySQL 到 StarRocks # 这篇教程将展示如何基于 Flink CDC 快速构建 MySQL 到 StarRocks 的 Streaming ELT 作业，包含整库同步、表结构变更同步和分库分表同步的功能。\n本教程的演示都将在 Flink CDC CLI 中进行，无需一行 Java/Scala 代码，也无需安装 IDE。\n准备阶段 # 准备一台已经安装了 Docker 的 Linux 或者 MacOS 电脑。\n准备 Flink Standalone 集群 # 下载 Flink 1.18.0 ，解压后得到 flink-1.18.0 目录。\n使用下面的命令跳转至 Flink 目录下，并且设置 FLINK_HOME 为 flink-1.18.0 所在目录。\ncd flink-1.18.0 通过在 conf/flink-conf.yaml 配置文件追加下列参数开启 checkpoint，每隔 3 秒做一次 checkpoint。\nexecution.checkpointing.interval: 3000 使用下面的命令启动 Flink 集群。\n./bin/start-cluster.sh 启动成功的话，可以在 http://localhost:8081/ 访问到 Flink Web UI，如下所示：\n多次执行 start-cluster.sh 可以拉起多个 TaskManager。\n准备 Docker 环境 # 使用下面的内容创建一个 docker-compose.yml 文件：\nversion: &#39;2.1&#39; services: StarRocks: image: starrocks/allin1-ubuntu:3.2.6 ports: - &#34;8080:8080&#34; - &#34;9030:9030&#34; MySQL: image: debezium/example-mysql:1.1 ports: - &#34;3306:3306&#34; environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_USER=mysqluser - MYSQL_PASSWORD=mysqlpw 该 Docker Compose 中包含的容器有：\nMySQL: 包含商品信息的数据库 app_db StarRocks: 存储从 MySQL 中根据规则映射过来的结果表 在 docker-compose.yml 所在目录下执行下面的命令来启动本教程需要的组件：\ndocker-compose up -d 该命令将以 detached 模式自动启动 Docker Compose 配置中定义的所有容器。你可以通过 docker ps 来观察上述的容器是否正常启动了，也可以通过访问 http://localhost:8030/ 来查看 StarRocks 是否运行正常。\n在 MySQL 数据库中准备数据 # 进入 MySQL 容器\ndocker-compose exec MySQL mysql -uroot -p123456 创建数据库 app_db 和表 orders,products,shipments，并插入数据\n-- 创建数据库 CREATE DATABASE app_db; USE app_db; -- 创建 orders 表 CREATE TABLE `orders` ( `id` INT NOT NULL, `price` DECIMAL(10,2) NOT NULL, PRIMARY KEY (`id`) ); -- 插入数据 INSERT INTO `orders` (`id`, `price`) VALUES (1, 4.00); INSERT INTO `orders` (`id`, `price`) VALUES (2, 100.00); -- 创建 shipments 表 CREATE TABLE `shipments` ( `id` INT NOT NULL, `city` VARCHAR(255) NOT NULL, PRIMARY KEY (`id`) ); -- 插入数据 INSERT INTO `shipments` (`id`, `city`) VALUES (1, &#39;beijing&#39;); INSERT INTO `shipments` (`id`, `city`) VALUES (2, &#39;xian&#39;); -- 创建 products 表 CREATE TABLE `products` ( `id` INT NOT NULL, `product` VARCHAR(255) NOT NULL, PRIMARY KEY (`id`) ); -- 插入数据 INSERT INTO `products` (`id`, `product`) VALUES (1, &#39;Beer&#39;); INSERT INTO `products` (`id`, `product`) VALUES (2, &#39;Cap&#39;); INSERT INTO `products` (`id`, `product`) VALUES (3, &#39;Peanut&#39;); 通过 FlinkCDC cli 提交任务 # 下载下面列出的二进制压缩包，并解压得到目录 flink-cdc-3.0.0： flink-cdc-3.0.0-bin.tar.gz flink-cdc-3.0.0 下会包含 bin、lib、log、conf 四个目录。\n下载下面列出的 connector 包，并且移动到 lib 目录下\n下载链接只对已发布的版本有效, SNAPSHOT 版本需要本地基于 master 或 release- 分支编译\nMySQL pipeline connector 3.0.0 StarRocks pipeline connector 3.0.0 MySQL Connector Java 编写任务配置 yaml 文件\n下面给出了一个整库同步的示例文件 mysql-to-starrocks.yaml：\n################################################################################ # Description: Sync MySQL all tables to StarRocks ################################################################################ source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: starrocks name: StarRocks Sink jdbc-url: jdbc:mysql://127.0.0.1:9030 load-url: 127.0.0.1:8080 username: root password: &#34;&#34; table.create.properties.replication_num: 1 pipeline: name: Sync MySQL Database to StarRocks parallelism: 2 其中：\nsource 中的 tables: app_db.\\.* 通过正则匹配同步 app_db 下的所有表。 sink 添加 table.create.properties.replication_num 参数是由于 Docker 镜像中只有一个 StarRocks BE 节点。 最后，通过命令行提交任务到 Flink Standalone cluster\nbash bin/flink-cdc.sh mysql-to-starrocks.yaml --jar lib/mysql-connector-java-8.0.27.jar 提交成功后，返回信息如：\nPipeline has been submitted to cluster. Job ID: 02a31c92f0e7bc9a1f4c0051980088a0 Job Description: Sync MySQL Database to StarRocks 在 Flink Web UI，可以看到一个名为 Sync MySQL Database to StarRocks 的任务正在运行。\n通过数据库连接工具例如 Dbeaver 等连接到 jdbc:mysql://127.0.0.1:9030， 可以查看 StarRocks 中写入了三张表的数据。\n同步变更 # 进入 MySQL 容器:\ndocker-compose exec mysql mysql -uroot -p123456 接下来，修改 MySQL 数据库中表的数据，StarRocks 中显示的订单数据也将实时更新：\n在 MySQL 的 orders 表中插入一条数据\nINSERT INTO app_db.orders (id, price) VALUES (3, 100.00); 在 MySQL 的 orders 表中增加一个字段\nALTER TABLE app_db.orders ADD amount varchar(100) NULL; 在 MySQL 的 orders 表中更新一条数据\nUPDATE app_db.orders SET price=100.00, amount=100.00 WHERE id=1; 在 MySQL 的 orders 表中删除一条数据\nDELETE FROM app_db.orders WHERE id=2; 通过连接工具，我们可以看到 StarRocks 上也在实时发生着这些变更： 同样的，去修改 shipments, products 表，也能在 StarRocks 中实时看到同步变更的结果。\n路由变更 # Flink CDC 提供了将源表的表结构/数据路由到其他表名的配置，借助这种能力，我们能够实现表名库名替换，整库同步等功能。\n下面提供一个配置文件说明：\n################################################################################ # Description: Sync MySQL all tables to StarRocks ################################################################################ source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: starrocks name: StarRocks Sink jdbc-url: jdbc:mysql://127.0.0.1:9030 load-url: 127.0.0.1:8030 username: root password: &#34;&#34; table.create.properties.replication_num: 1 route: - source-table: app_db.orders sink-table: ods_db.ods_orders - source-table: app_db.shipments sink-table: ods_db.ods_shipments - source-table: app_db.products sink-table: ods_db.ods_products pipeline: name: Sync MySQL Database to StarRocks parallelism: 2 通过上面的 route 配置，会将 app_db.orders 表的结构和数据同步到 ods_db.ods_orders 中。从而实现数据库迁移的功能。\n特别地，source-table 支持正则表达式匹配多表，从而实现分库分表同步的功能，例如下面的配置：\nroute: - source-table: app_db.order\\.* sink-table: ods_db.ods_orders 这样，就可以将诸如 app_db.order01、app_db.order02、app_db.order03 的表汇总到 ods_db.ods_orders 中。注意，目前还不支持多表中存在相同主键数据的场景，将在后续版本支持。\n环境清理 # 本教程结束后，在 docker-compose.yml 文件所在的目录下执行如下命令停止所有容器：\ndocker-compose down 在 Flink 所在目录 flink-1.18.0 下执行如下命令停止 Flink 集群：\n./bin/stop-cluster.sh Back to top\n"}),e.add({id:17,href:"/flink/flink-cdc-docs-master/zh/docs/deployment/yarn/",title:"YARN",section:"部署模式",content:` Introduction # Apache Hadoop YARN is a resource provider popular with many data processing frameworks. Flink services are submitted to YARN&rsquo;s ResourceManager, which spawns containers on machines managed by YARN NodeManagers. Flink deploys its JobManager and TaskManager instances into such containers.
Flink can dynamically allocate and de-allocate TaskManager resources depending on the number of processing slots required by the job(s) running on the JobManager.
Preparation # This Getting Started section assumes a functional YARN environment, starting from version 2.10.2. YARN environments are provided most conveniently through services such as Amazon EMR, Google Cloud DataProc or products like Cloudera. Manually setting up a YARN environment locally or on a cluster is not recommended for following through this Getting Started tutorial.
Make sure your YARN cluster is ready for accepting Flink applications by running yarn top. It should show no error messages. Download a recent Flink distribution from the download page and unpack it. Important Make sure that the HADOOP_CLASSPATH environment variable is set up (it can be checked by running echo $HADOOP_CLASSPATH). If not, set it up using export HADOOP_CLASSPATH=\`hadoop classpath\` Session Mode # Flink runs on all UNIX-like environments, i.e. Linux, Mac OS X, and Cygwin (for Windows).
You can refer overview to check supported versions and download the binary release of Flink, then extract the archive:
tar -xzf flink-*.tgz You should set FLINK_HOME environment variables like:
export FLINK_HOME=/path/flink-* Starting a Flink Session on YARN # Once you&rsquo;ve made sure that the HADOOP_CLASSPATH environment variable is set, you can launch a Flink on YARN session:
# we assume to be in the root directory of # the unzipped Flink distribution # export HADOOP_CLASSPATH export HADOOP_CLASSPATH=\`hadoop classpath\` # Start YARN session ./bin/yarn-session.sh --detached # Stop YARN session (replace the application id based # on the output of the yarn-session.sh command) echo &#34;stop&#34; | ./bin/yarn-session.sh -id application_XXXXX_XXX After starting YARN session, you can now access the Flink Web UI through the URL printed in the last lines of the command output, or through the YARN ResourceManager web UI.
Then, you need to add some configs to your flink-conf.yaml:
rest.bind-port: {{REST_PORT}} rest.address: {{NODE_IP}} execution.target: yarn-session yarn.application.id: {{YARN_APPLICATION_ID}} {{REST_PORT}} and {{NODE_IP}} should be replaced by the actual values of your JobManager Web Interface, and {{YARN_APPLICATION_ID}} should be replaced by the actual YARN application ID of Flink.
Set up Flink CDC # Download the tar file of Flink CDC from release page, then extract the archive:
tar -xzf flink-cdc-*.tar.gz Extracted flink-cdc contains four directories: bin,lib,log and conf.
Download the connector jars from release page, and move it to the lib directory. Download links are available only for stable releases, SNAPSHOT dependencies need to be built based on specific branch by yourself.
Submit a Flink CDC Job # Here is an example file for synchronizing the entire database mysql-to-doris.yaml:
################################################################################ # Description: Sync MySQL all tables to Doris ################################################################################ source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: doris fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; pipeline: name: Sync MySQL Database to Doris parallelism: 2 You need to modify the configuration file according to your needs. Finally, submit job to Flink Standalone cluster using Cli.
cd /path/flink-cdc-* ./bin/flink-cdc.sh mysql-to-doris.yaml After successful submission, the return information is as follows:
Pipeline has been submitted to cluster. Job ID: ae30f4580f1918bebf16752d4963dc54 Job Description: Sync MySQL Database to Doris You can find a job named Sync MySQL Database to Doris running through Flink Web UI.
Please note that submitting to application mode cluster and per-job mode cluster are not supported for now.
`}),e.add({id:18,href:"/flink/flink-cdc-docs-master/zh/docs/core-concept/",title:"核心概念",section:"Docs",content:" "}),e.add({id:19,href:"/flink/flink-cdc-docs-master/zh/docs/get-started/quickstart/",title:"快速开始",section:"入门指南",content:" "}),e.add({id:20,href:"/flink/flink-cdc-docs-master/zh/docs/developer-guide/contribute-to-flink-cdc/",title:"向 Flink CDC 提交贡献",section:"开发者指南",content:` 社区贡献 # Flink CDC 由开放和友好的社区开发而来，欢迎任何想要提供帮助的贡献者。有以下一些方式可以让贡献者和社区交流和做出贡献，包括提问，提交发现的 Bug报告，提议新的功能，加入社区邮件列表的讨论，贡献代码或文档，改进项目网站，发版前测试和编写Blog等。
你想要贡献什么？ # Flink CDC 社区的贡献不仅限于为项目贡献代码，下面列举了一些可以在社区贡献的内容。
贡献方式 更多信息 提交BUG 为了提交问题，您需要首先在 Flink jira 建立对应的issue，并在Component/s选择Flink CDC。然后在问题描述中详细描述遇到的问题的信息，如果可能的话，最好提供一下能够复现问题的操作步骤。 贡献代码 请阅读 贡献代码指导 代码评审 请阅读 代码评审指导 用户支持 通过 Flink 用户邮件列表 来帮助回复用户问题，在 Flink jira 可以查询到最新的已知问题。 如果还有其他问题，可以通过 Flink Dev 邮件列表寻求帮助。
贡献代码指导 Flink CDC 项目通过众多贡献者的代码贡献来维护，改进和拓展，欢迎各种形式的社区贡献。
在 Flink CDC 社区可以自由的在任何时间提出自己的问题，通过社区 Dev 邮件列表进行交流或在任何感兴趣的 issue 下评论和讨论。
如果您想要为 Flink CDC 贡献代码，可以通过如下的方式。
首先在 Flink jira 的想要负责的 issue 下评论（最好在评论中解释下对于这个问题的理解，和后续的设计，如果可能的话也可以提供下 POC 的代码）。 在这个 issue 被分配给你后，开始进行开发实现（提交信息请遵循[FLINK-xxx][xxx] xxxxxxx的格式）。 开发完成后可以向 Flink CDC 项目提交 PR（请确保 Clone 的项目 committer 有操作权限）。 找到一个开发者帮忙评审代码，评审前请确保 CI 通过。 Flink committer 确认代码贡献满足全部要求后，代码会被合并到代码仓库。 代码评审指导 每一次的代码评审需要检查如下一些方面的内容。
提交的 PR 是否被正确地描述了？ 评审时，需要检查对应的 PR 是否合理的描述了本次修改的内容，能否支持评审人较快的理解和评审代码。对于比较琐碎的修改，不需要提供太过详细的信息。
提交的 PR 代码质量是否符合标准？ 代码是否遵循正确的软件开发习惯？ 代码是否正确，鲁棒性如何，是否便于维护和拓展，是否是可测试的？ 代码是否有可能影响到性能？、 代码修改是否已经被正确的测试？测试执行速度是否有问题？ 项目依赖是否发生了变化，如果是，对应的 NOTICE 文件是否需要更新？ 提交信息是否遵循[FLINK-xxx][xxx] xxxxxxx格式？ 文档是否需要更新？ 如果代码提交加入了新的功能，这个新功能需要同时更新到文档中。
`}),e.add({id:21,href:"/flink/flink-cdc-docs-master/zh/docs/core-concept/data-sink/",title:"Data Sink",section:"核心概念",content:` Definition # Data Sink is used to apply schema changes and write change data to external systems. A Data Sink can write to multiple tables simultaneously.
Parameters # To describe a data sink, the follows are required:
parameter meaning optional/required type The type of the sink, such as doris or starrocks. required name The name of the sink, which is user-defined (a default value provided). optional configurations of Data Sink Configurations to build the Data Sink e.g. connection configurations and sink table properties. optional Example # We could use this yaml file to define a doris sink:
sink: type: doris name: doris-sink # Optional parameter for description purpose fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; table.create.properties.replication_num: 1 # Optional parameter for advanced functionalities `}),e.add({id:22,href:"/flink/flink-cdc-docs-master/zh/docs/deployment/kubernetes/",title:"Kubernetes",section:"部署模式",content:` Introduction # Kubernetes is a popular container-orchestration system for automating computer application deployment, scaling, and management. Flink&rsquo;s native Kubernetes integration allows you to directly deploy Flink on a running Kubernetes cluster. Moreover, Flink is able to dynamically allocate and de-allocate TaskManagers depending on the required resources because it can directly talk to Kubernetes.
Apache Flink also provides a Kubernetes operator for managing Flink clusters on Kubernetes. It supports both standalone and native deployment mode and greatly simplifies deployment, configuration and the life cycle management of Flink resources on Kubernetes.
For more information, please refer to the Flink Kubernetes Operator documentation.
Preparation # The doc assumes a running Kubernetes cluster fulfilling the following requirements:
Kubernetes &gt;= 1.9. KubeConfig, which has access to list, create, delete pods and services, configurable via ~/.kube/config. You can verify permissions by running kubectl auth can-i &lt;list|create|edit|delete&gt; pods. Enabled Kubernetes DNS. default service account with RBAC permissions to create, delete pods. If you have problems setting up a Kubernetes cluster, please take a look at how to setup a Kubernetes cluster.
Session Mode # Flink runs on all UNIX-like environments, i.e. Linux, Mac OS X, and Cygwin (for Windows).
You can refer overview to check supported versions and download the binary release of Flink, then extract the archive:
tar -xzf flink-*.tgz You should set FLINK_HOME environment variables like:
export FLINK_HOME=/path/flink-* Start a session cluster # To start a session cluster on k8s, run the bash script that comes with Flink:
cd /path/flink-* ./bin/kubernetes-session.sh -Dkubernetes.cluster-id=my-first-flink-cluster After successful startup, the return information is as follows：
org.apache.flink.kubernetes.utils.KubernetesUtils [] - Kubernetes deployment requires a fixed port. Configuration blob.server.port will be set to 6124 org.apache.flink.kubernetes.utils.KubernetesUtils [] - Kubernetes deployment requires a fixed port. Configuration taskmanager.rpc.port will be set to 6122 org.apache.flink.kubernetes.KubernetesClusterDescriptor [] - Please note that Flink client operations(e.g. cancel, list, stop, savepoint, etc.) won&#39;t work from outside the Kubernetes cluster since &#39;kubernetes.rest-service.exposed.type&#39; has been set to ClusterIP. org.apache.flink.kubernetes.KubernetesClusterDescriptor [] - Create flink session cluster my-first-flink-cluster successfully, JobManager Web Interface: http://my-first-flink-cluster-rest.default:8081 please refer to Flink documentation to expose Flink’s Web UI and REST endpoint.
You should ensure that REST endpoint can be accessed by the node of your submission. Then, you need to add these two config to your flink-conf.yaml:
rest.bind-port: {{REST_PORT}} rest.address: {{NODE_IP}} {{REST_PORT}} and {{NODE_IP}} should be replaced by the actual values of your JobManager Web Interface.
Set up Flink CDC # Download the tar file of Flink CDC from release page, then extract the archive:
tar -xzf flink-cdc-*.tar.gz Extracted flink-cdc contains four directories: bin,lib,log and conf.
Download the connector jars from release page, and move it to the lib directory. Download links are available only for stable releases, SNAPSHOT dependencies need to be built based on specific branch by yourself.
Submit a Flink CDC Job # Here is an example file for synchronizing the entire database mysql-to-doris.yaml：
################################################################################ # Description: Sync MySQL all tables to Doris ################################################################################ source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: doris fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; pipeline: name: Sync MySQL Database to Doris parallelism: 2 You need to modify the configuration file according to your needs, refer to connectors more information.
MySQL pipeline connector Apache Doris pipeline connector Finally, submit job to Flink Standalone cluster using Cli.
cd /path/flink-cdc-* ./bin/flink-cdc.sh mysql-to-doris.yaml After successful submission, the return information is as follows：
Pipeline has been submitted to cluster. Job ID: ae30f4580f1918bebf16752d4963dc54 Job Description: Sync MySQL Database to Doris Then you can find a job named Sync MySQL Database to Doris running through Flink Web UI.
Please note that submitting with native application mode and Flink Kubernetes operator are not supported for now. `}),e.add({id:23,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/oceanbase-tutorial/",title:"OceanBase 教程",section:"Flink CDC Sources 教程",content:` 演示: OceanBase CDC 导入 Elasticsearch # 视频教程 # YouTube Bilibili 准备教程所需要的组件 # 配置并启动容器 # 配置 docker-compose.yml。
version: &#39;2.1&#39; services: observer: image: oceanbase/oceanbase-ce:4.0.0.0 container_name: observer network_mode: &#34;host&#34; oblogproxy: image: whhe/oblogproxy:1.1.0_4x container_name: oblogproxy environment: - &#39;OB_SYS_USERNAME=root&#39; - &#39;OB_SYS_PASSWORD=pswd&#39; network_mode: &#34;host&#34; elasticsearch: image: &#39;elastic/elasticsearch:7.6.0&#39; container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - ES_JAVA_OPTS=-Xms512m -Xmx512m - discovery.type=single-node ports: - &#39;9200:9200&#39; - &#39;9300:9300&#39; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: &#39;elastic/kibana:7.6.0&#39; container_name: kibana ports: - &#39;5601:5601&#39; volumes: - &#39;/var/run/docker.sock:/var/run/docker.sock&#39; 在 docker-compose.yml 所在目录下执行下面的命令来启动本教程需要的组件：
docker-compose up -d 设置密码 # OceanBase 中 root 用户默认是没有密码的，但是 oblogproxy 需要配置一个使用非空密码的系统租户用户，因此这里我们需要先为 root@sys 用户设置一个密码。
登陆 sys 租户的 root 用户：
docker-compose exec observer obclient -h127.0.0.1 -P2881 -uroot@sys 设置密码，注意这里的密码需要与上一步中 oblogproxy 服务的环境变量 &lsquo;OB_SYS_PASSWORD&rsquo; 保持一样。
ALTER USER root IDENTIFIED BY &#39;pswd&#39;; OceanBase 从社区版 4.0.0.0 开始只支持对非 sys 租户的增量数据拉取，这里我们使用 test 租户的 root 用户作为示例。
登陆 test 租户的 root 用户：
docker-compose exec observer obclient -h127.0.0.1 -P2881 -uroot@test 设置密码:
ALTER USER root IDENTIFIED BY &#39;test&#39;; 准备数据 # 使用 &lsquo;root@test&rsquo; 用户登陆。
docker-compose exec observer obclient -h127.0.0.1 -P2881 -uroot@test -ptest CREATE DATABASE ob; USE ob; CREATE TABLE products ( id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(512) ); ALTER TABLE products AUTO_INCREMENT = 101; INSERT INTO products VALUES (default,&#34;scooter&#34;,&#34;Small 2-wheel scooter&#34;), (default,&#34;car battery&#34;,&#34;12V car battery&#34;), (default,&#34;12-pack drill bits&#34;,&#34;12-pack of drill bits with sizes ranging from #40 to #3&#34;), (default,&#34;hammer&#34;,&#34;12oz carpenter&#39;s hammer&#34;), (default,&#34;hammer&#34;,&#34;14oz carpenter&#39;s hammer&#34;), (default,&#34;hammer&#34;,&#34;16oz carpenter&#39;s hammer&#34;), (default,&#34;rocks&#34;,&#34;box of assorted rocks&#34;), (default,&#34;jacket&#34;,&#34;water resistent black wind breaker&#34;), (default,&#34;spare tire&#34;,&#34;24 inch spare tire&#34;); CREATE TABLE orders ( order_id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, order_date DATETIME NOT NULL, customer_name VARCHAR(255) NOT NULL, price DECIMAL(10, 5) NOT NULL, product_id INTEGER NOT NULL, order_status BOOLEAN NOT NULL -- Whether order has been placed ) AUTO_INCREMENT = 10001; INSERT INTO orders VALUES (default, &#39;2020-07-30 10:08:22&#39;, &#39;Jark&#39;, 50.50, 102, false), (default, &#39;2020-07-30 10:11:09&#39;, &#39;Sally&#39;, 15.00, 105, false), (default, &#39;2020-07-30 12:00:30&#39;, &#39;Edward&#39;, 25.25, 106, false); 下载所需要的依赖包 # 下载链接只对已发布的版本有效, SNAPSHOT 版本需要本地编译
flink-sql-connector-elasticsearch7-3.0.1-1.17.jar flink-sql-connector-oceanbase-cdc-2.4.0.jar 在 Flink SQL CLI 中使用 Flink DDL 创建表 # -- 设置间隔时间为3秒 Flink SQL&gt; SET execution.checkpointing.interval = 3s; -- 设置本地时区为 Asia/Shanghai Flink SQL&gt; SET table.local-time-zone = Asia/Shanghai; -- 创建订单表 Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;oceanbase-cdc&#39;, &#39;scan.startup.mode&#39; = &#39;initial&#39;, &#39;username&#39; = &#39;root@test&#39;, &#39;password&#39; = &#39;test&#39;, &#39;tenant-name&#39; = &#39;test&#39;, &#39;database-name&#39; = &#39;^ob$&#39;, &#39;table-name&#39; = &#39;^orders$&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;2881&#39;, &#39;rootserver-list&#39; = &#39;127.0.0.1:2882:2881&#39;, &#39;logproxy.host&#39; = &#39;localhost&#39;, &#39;logproxy.port&#39; = &#39;2983&#39;, &#39;working-mode&#39; = &#39;memory&#39; ); -- 创建商品表 Flink SQL&gt; CREATE TABLE products ( id INT, name STRING, description STRING, PRIMARY KEY (id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;oceanbase-cdc&#39;, &#39;scan.startup.mode&#39; = &#39;initial&#39;, &#39;username&#39; = &#39;root@test&#39;, &#39;password&#39; = &#39;test&#39;, &#39;tenant-name&#39; = &#39;test&#39;, &#39;database-name&#39; = &#39;^ob$&#39;, &#39;table-name&#39; = &#39;^products$&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;2881&#39;, &#39;rootserver-list&#39; = &#39;127.0.0.1:2882:2881&#39;, &#39;logproxy.host&#39; = &#39;localhost&#39;, &#39;logproxy.port&#39; = &#39;2983&#39;, &#39;working-mode&#39; = &#39;memory&#39; ); -- 创建关联后的订单数据表 Flink SQL&gt; CREATE TABLE enriched_orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, product_name STRING, product_description STRING, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;elasticsearch-7&#39;, &#39;hosts&#39; = &#39;http://localhost:9200&#39;, &#39;index&#39; = &#39;enriched_orders&#39;); -- 执行读取和写入 Flink SQL&gt; INSERT INTO enriched_orders SELECT o.order_id, o.order_date, o.customer_name, o.price, o.product_id, o.order_status, p.name, p.description FROM orders AS o LEFT JOIN products AS p ON o.product_id = p.id; 在 Kibana 中查看数据 # 访问 http://localhost:5601/app/kibana#/management/kibana/index_pattern 创建 index pattern enriched_orders，之后可以在 http://localhost:5601/app/kibana#/discover 看到写入的数据了。
修改监听表数据，查看增量数据变动 # 在OceanBase中依次执行如下修改操作，每执行一步就刷新一次 Kibana，可以看到 Kibana 中显示的订单数据将实时更新。
INSERT INTO orders VALUES (default, &#39;2020-07-30 15:22:00&#39;, &#39;Jark&#39;, 29.71, 104, false); UPDATE orders SET order_status = true WHERE order_id = 10004; DELETE FROM orders WHERE order_id = 10004; 环境清理 # 在 docker-compose.yml 文件所在的目录下执行如下命令停止所有容器：
docker-compose down 进入Flink的部署目录，停止 Flink 集群：
./bin/stop-cluster.sh Back to top
`}),e.add({id:24,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/oracle-cdc/",title:"Oracle",section:"Flink Source 连接器",content:` Oracle CDC Connector # The Oracle CDC connector allows for reading snapshot data and incremental data from Oracle database. This document describes how to setup the Oracle CDC connector to run SQL queries against Oracle databases.
Dependencies # In order to setup the Oracle CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency # &ltdependency&gt &ltgroupId&gtorg.apache.flink&lt/groupId&gt &ltartifactId&gtflink-connector-oracle-cdc&lt/artifactId&gt &ltversion&gt3.2-SNAPSHOT&lt/version&gt &lt/dependency&gt Copied to clipboard! SQL Client JAR # Download link is available only for stable releases.
Download flink-sql-connector-oracle-cdc-3.0.1.jar and put it under &lt;FLINK_HOME&gt;/lib/.
Note: Refer to flink-sql-connector-oracle-cdc, more released versions will be available in the Maven central warehouse.
由于 Oracle Connector 采用的 FUTC 协议与 Flink CDC 项目不兼容，我们无法在 jar 包中提供 Oracle 连接器。 您可能需要手动配置以下依赖：
依赖名称 说明 com.oracle.ojdbc:ojdbc8:19.3.0.0 用于连接到 Oracle 数据库。 com.oracle.database.xml:xdb:19.3.0.0 用于存储 XML 文件。 Setup Oracle # You have to enable log archiving for Oracle database and define an Oracle user with appropriate permissions on all databases that the Debezium Oracle connector monitors.
For Non-CDB database # Enable log archiving
(1.1). Connect to the database as DBA
ORACLE_SID=SID export ORACLE_SID sqlplus /nolog CONNECT sys/password AS SYSDBA (1.2). Enable log archiving
alter system set db_recovery_file_dest_size = 10G; alter system set db_recovery_file_dest = &#39;/opt/oracle/oradata/recovery_area&#39; scope=spfile; shutdown immediate; startup mount; alter database archivelog; alter database open; Note:
Enable log archiving requires database restart, pay attention when try to do it The archived logs will occupy a large amount of disk space, so consider clean the expired logs the periodically (1.3). Check whether log archiving is enabled
-- Should now &#34;Database log mode: Archive Mode&#34; archive log list; Note:
Supplemental logging must be enabled for captured tables or the database in order for data changes to capture the before state of changed database rows. The following illustrates how to configure this on the table/database level.
-- Enable supplemental logging for a specific table: ALTER TABLE inventory.customers ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS; -- Enable supplemental logging for database ALTER DATABASE ADD SUPPLEMENTAL LOG DATA; Create an Oracle user with permissions
(2.1). Create Tablespace
sqlplus sys/password@host:port/SID AS SYSDBA; CREATE TABLESPACE logminer_tbs DATAFILE &#39;/opt/oracle/oradata/SID/logminer_tbs.dbf&#39; SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED; exit; (2.2). Create a user and grant permissions
sqlplus sys/password@host:port/SID AS SYSDBA; CREATE USER flinkuser IDENTIFIED BY flinkpw DEFAULT TABLESPACE LOGMINER_TBS QUOTA UNLIMITED ON LOGMINER_TBS; GRANT CREATE SESSION TO flinkuser; GRANT SET CONTAINER TO flinkuser; GRANT SELECT ON V_$DATABASE to flinkuser; GRANT FLASHBACK ANY TABLE TO flinkuser; GRANT SELECT ANY TABLE TO flinkuser; GRANT SELECT_CATALOG_ROLE TO flinkuser; GRANT EXECUTE_CATALOG_ROLE TO flinkuser; GRANT SELECT ANY TRANSACTION TO flinkuser; GRANT LOGMINING TO flinkuser; GRANT ANALYZE ANY TO flinkuser; GRANT CREATE TABLE TO flinkuser; -- need not to execute if set scan.incremental.snapshot.enabled=true(default) GRANT LOCK ANY TABLE TO flinkuser; GRANT ALTER ANY TABLE TO flinkuser; GRANT CREATE SEQUENCE TO flinkuser; GRANT EXECUTE ON DBMS_LOGMNR TO flinkuser; GRANT EXECUTE ON DBMS_LOGMNR_D TO flinkuser; GRANT SELECT ON V_$LOG TO flinkuser; GRANT SELECT ON V_$LOG_HISTORY TO flinkuser; GRANT SELECT ON V_$LOGMNR_LOGS TO flinkuser; GRANT SELECT ON V_$LOGMNR_CONTENTS TO flinkuser; GRANT SELECT ON V_$LOGMNR_PARAMETERS TO flinkuser; GRANT SELECT ON V_$LOGFILE TO flinkuser; GRANT SELECT ON V_$ARCHIVED_LOG TO flinkuser; GRANT SELECT ON V_$ARCHIVE_DEST_STATUS TO flinkuser; exit; For CDB database # Overall, the steps for configuring CDB database is quite similar to non-CDB database, but the commands may be different.
Enable log archiving
ORACLE_SID=ORCLCDB export ORACLE_SID sqlplus /nolog CONNECT sys/password AS SYSDBA alter system set db_recovery_file_dest_size = 10G; -- should exist alter system set db_recovery_file_dest = &#39;/opt/oracle/oradata/recovery_area&#39; scope=spfile; shutdown immediate startup mount alter database archivelog; alter database open; -- Should show &#34;Database log mode: Archive Mode&#34; archive log list exit; Note: You can also use the following commands to enable supplemental logging:
-- Enable supplemental logging for a specific table: ALTER TABLE inventory.customers ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS; -- Enable supplemental logging for database ALTER DATABASE ADD SUPPLEMENTAL LOG DATA; Create an Oracle user with permissions
sqlplus sys/password@//localhost:1521/ORCLCDB as sysdba CREATE TABLESPACE logminer_tbs DATAFILE &#39;/opt/oracle/oradata/ORCLCDB/logminer_tbs.dbf&#39; SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED; exit sqlplus sys/password@//localhost:1521/ORCLPDB1 as sysdba CREATE TABLESPACE logminer_tbs DATAFILE &#39;/opt/oracle/oradata/ORCLCDB/ORCLPDB1/logminer_tbs.dbf&#39; SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED; exit sqlplus sys/password@//localhost:1521/ORCLCDB as sysdba CREATE USER flinkuser IDENTIFIED BY flinkpw DEFAULT TABLESPACE logminer_tbs QUOTA UNLIMITED ON logminer_tbs CONTAINER=ALL; GRANT CREATE SESSION TO flinkuser CONTAINER=ALL; GRANT SET CONTAINER TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$DATABASE to flinkuser CONTAINER=ALL; GRANT FLASHBACK ANY TABLE TO flinkuser CONTAINER=ALL; GRANT SELECT ANY TABLE TO flinkuser CONTAINER=ALL; GRANT SELECT_CATALOG_ROLE TO flinkuser CONTAINER=ALL; GRANT EXECUTE_CATALOG_ROLE TO flinkuser CONTAINER=ALL; GRANT SELECT ANY TRANSACTION TO flinkuser CONTAINER=ALL; GRANT LOGMINING TO flinkuser CONTAINER=ALL; GRANT CREATE TABLE TO flinkuser CONTAINER=ALL; -- need not to execute if set scan.incremental.snapshot.enabled=true(default) GRANT LOCK ANY TABLE TO flinkuser CONTAINER=ALL; GRANT CREATE SEQUENCE TO flinkuser CONTAINER=ALL; GRANT EXECUTE ON DBMS_LOGMNR TO flinkuser CONTAINER=ALL; GRANT EXECUTE ON DBMS_LOGMNR_D TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$LOG TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$LOG_HISTORY TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$LOGMNR_LOGS TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$LOGMNR_CONTENTS TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$LOGMNR_PARAMETERS TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$LOGFILE TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$ARCHIVED_LOG TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$ARCHIVE_DEST_STATUS TO flinkuser CONTAINER=ALL; exit See more about the Setting up Oracle
How to create an Oracle CDC table # The Oracle CDC table can be defined as following:
-- register an Oracle table &#39;products&#39; in Flink SQL Flink SQL&gt; CREATE TABLE products ( ID INT NOT NULL, NAME STRING, DESCRIPTION STRING, WEIGHT DECIMAL(10, 3), PRIMARY KEY(id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;oracle-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;1521&#39;, &#39;username&#39; = &#39;flinkuser&#39;, &#39;password&#39; = &#39;flinkpw&#39;, &#39;database-name&#39; = &#39;ORCLCDB&#39;, &#39;schema-name&#39; = &#39;inventory&#39;, &#39;table-name&#39; = &#39;products&#39;); -- read snapshot and redo logs from products table Flink SQL&gt; SELECT * FROM products; Note: When working with the CDB + PDB model, you are expected to add an extra option 'debezium.database.pdb.name' = 'xxx' in Flink DDL to specific the name of the PDB to connect to.
Note: While the connector might work with a variety of Oracle versions and editions, only Oracle 9i, 10g, 11g and 12c have been tested.
Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'oracle-cdc'. hostname optional (none) String IP address or hostname of the Oracle database server. If the url is not empty, hostname may not be configured, otherwise hostname can not be empty username required (none) String Name of the Oracle database to use when connecting to the Oracle database server. password required (none) String Password to use when connecting to the Oracle database server. database-name required (none) String Database name of the Oracle server to monitor. schema-name required (none) String Schema name of the Oracle database to monitor. table-name required (none) String Table name of the Oracle database to monitor. port optional 1521 Integer Integer port number of the Oracle database server. url optional jdbc:oracle:thin:@{hostname}:{port}:{database-name} String JdbcUrl of the oracle database server . If the hostname and port parameter is configured, the URL is concatenated by hostname port database-name in SID format by default. Otherwise, you need to configure the URL parameter scan.startup.mode optional initial String Optional startup mode for Oracle CDC consumer, valid enumerations are "initial" and "latest-offset". Please see Startup Reading Position section for more detailed information. scan.incremental.snapshot.enabled optional true Boolean Incremental snapshot is a new mechanism to read snapshot of a table. Compared to the old snapshot mechanism, the incremental snapshot has many advantages, including: (1) source can be parallel during snapshot reading, (2) source can perform checkpoints in the chunk granularity during snapshot reading, (3) source doesn't need to acquire ROW SHARE MODE lock before snapshot reading. scan.incremental.snapshot.chunk.size optional 8096 Integer The chunk size (number of rows) of table snapshot, captured tables are split into multiple chunks when read the snapshot of table. scan.snapshot.fetch.size optional 1024 Integer The maximum fetch size for per poll when read table snapshot. connect.max-retries optional 3 Integer The max retry times that the connector should retry to build Oracle database server connection. connection.pool.size optional 20 Integer The connection pool size. debezium.* optional (none) String Pass-through Debezium's properties to Debezium Embedded Engine which is used to capture data changes from Oracle server. For example: 'debezium.snapshot.mode' = 'never'. See more about the Debezium's Oracle Connector properties scan.incremental.close-idle-reader.enabled optional false Boolean Whether to close idle readers at the end of the snapshot phase. The flink version is required to be greater than or equal to 1.14 when 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' is set to true.
If the flink version is greater than or equal to 1.15, the default value of 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' has been changed to true, so it does not need to be explicitly configured 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' = 'true' scan.incremental.snapshot.chunk.key-column optional (none) String The chunk key of table snapshot, captured tables are split into multiple chunks by a chunk key when read the snapshot of table. By default, the chunk key is 'ROWID'. This column must be a column of the primary key. Limitation # Can&rsquo;t perform checkpoint during scanning snapshot of tables # During scanning snapshot of database tables, since there is no recoverable position, we can&rsquo;t perform checkpoints. In order to not perform checkpoints, Oracle CDC source will keep the checkpoint waiting to timeout. The timeout checkpoint will be recognized as failed checkpoint, by default, this will trigger a failover for the Flink job. So if the database table is large, it is recommended to add following Flink configurations to avoid failover because of the timeout checkpoints:
execution.checkpointing.interval: 10min execution.checkpointing.tolerable-failed-checkpoints: 100 restart-strategy: fixed-delay restart-strategy.fixed-delay.attempts: 2147483647 Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Key DataType Description table_name STRING NOT NULL Name of the table that contain the row. schema_name STRING NOT NULL Name of the schema that contain the row. database_name STRING NOT NULL Name of the database that contain the row. op_ts TIMESTAMP_LTZ(3) NOT NULL It indicates the time that the change was made in the database. If the record is read from snapshot of the table instead of the change stream, the value is always 0. The extended CREATE TABLE example demonstrates the syntax for exposing these metadata fields:
CREATE TABLE products ( db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, schema_name STRING METADATA FROM &#39;schema_name&#39; VIRTUAL, table_name STRING METADATA FROM &#39;table_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, ID INT NOT NULL, NAME STRING, DESCRIPTION STRING, WEIGHT DECIMAL(10, 3), PRIMARY KEY(id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;oracle-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;1521&#39;, &#39;username&#39; = &#39;flinkuser&#39;, &#39;password&#39; = &#39;flinkpw&#39;, &#39;database-name&#39; = &#39;ORCLCDB&#39;, &#39;schema-name&#39; = &#39;inventory&#39;, &#39;table-name&#39; = &#39;products&#39;, &#39;debezium.log.mining.strategy&#39; = &#39;online_catalog&#39;, &#39;debezium.log.mining.continuous.mine&#39; = &#39;true&#39; ); Note : The Oracle dialect is case-sensitive, it converts field name to uppercase if the field name is not quoted, Flink SQL doesn&rsquo;t convert the field name. Thus for physical columns from oracle database, we should use its converted field name in Oracle when define an oracle-cdc table in Flink SQL.
Features # Exactly-Once Processing # The Oracle CDC connector is a Flink Source connector which will read database snapshot first and then continues to read change events with exactly-once processing even failures happen. Please read How the connector works.
Startup Reading Position # The config option scan.startup.mode specifies the startup mode for Oracle CDC consumer. The valid enumerations are:
initial (default): Performs an initial snapshot on the monitored database tables upon first startup, and continue to read the latest redo log. latest-offset: Never to perform a snapshot on the monitored database tables upon first startup, just read from the change since the connector was started. Note: the mechanism of scan.startup.mode option relying on Debezium&rsquo;s snapshot.mode configuration. So please do not use them together. If you specific both scan.startup.mode and debezium.snapshot.mode options in the table DDL, it may make scan.startup.mode doesn&rsquo;t work.
Single Thread Reading # The Oracle CDC source can&rsquo;t work in parallel reading, because there is only one task can receive change events.
DataStream Source # The Oracle CDC connector can also be a DataStream source. There are two modes for the DataStream source:
incremental snapshot based, which allows parallel reading SourceFunction based, which only supports single thread reading Incremental Snapshot based DataStream (Experimental) # import org.apache.flink.cdc.connectors.base.options.StartupOptions; import org.apache.flink.cdc.connectors.base.source.jdbc.JdbcIncrementalSource; import org.apache.flink.cdc.connectors.oracle.source.OracleSourceBuilder; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import java.util.Properties; public class OracleParallelSourceExample { public static void main(String[] args) throws Exception { Properties debeziumProperties = new Properties(); debeziumProperties.setProperty(&#34;log.mining.strategy&#34;, &#34;online_catalog&#34;); JdbcIncrementalSource&lt;String&gt; oracleChangeEventSource = new OracleSourceBuilder() .hostname(&#34;host&#34;) .port(1521) .databaseList(&#34;ORCLCDB&#34;) .schemaList(&#34;DEBEZIUM&#34;) .tableList(&#34;DEBEZIUM.PRODUCTS&#34;) .username(&#34;username&#34;) .password(&#34;password&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) .includeSchemaChanges(true) // output the schema changes as well .startupOptions(StartupOptions.initial()) .debeziumProperties(debeziumProperties) .splitSize(2) .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000L); // set the source parallelism to 4 env.fromSource( oracleChangeEventSource, WatermarkStrategy.noWatermarks(), &#34;OracleParallelSource&#34;) .setParallelism(4) .print() .setParallelism(1); env.execute(&#34;Print Oracle Snapshot + RedoLog&#34;); } } SourceFunction-based DataStream # import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.cdc.connectors.oracle.OracleSource; public class OracleSourceExample { public static void main(String[] args) throws Exception { SourceFunction&lt;String&gt; sourceFunction = OracleSource.&lt;String&gt;builder() .url(&#34;jdbc:oracle:thin:@{hostname}:{port}:{database}&#34;) .port(1521) .database(&#34;ORCLCDB&#34;) // monitor XE database .schemaList(&#34;inventory&#34;) // monitor inventory schema .tableList(&#34;inventory.products&#34;) // monitor products table .username(&#34;flinkuser&#34;) .password(&#34;flinkpw&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env .addSource(sourceFunction) .print().setParallelism(1); // use parallelism 1 for sink to keep message ordering env.execute(); } } Data Type Mapping # Oracle type Flink SQL type NUMBER(p, s <= 0), p - s < 3 TINYINT NUMBER(p, s <= 0), p - s < 5 SMALLINT NUMBER(p, s <= 0), p - s < 10 INT NUMBER(p, s <= 0), p - s < 19 BIGINT NUMBER(p, s <= 0), 19 <= p - s <= 38 DECIMAL(p - s, 0) NUMBER(p, s > 0) DECIMAL(p, s) NUMBER(p, s <= 0), p - s > 38 STRING FLOAT
BINARY_FLOAT FLOAT DOUBLE PRECISION
BINARY_DOUBLE DOUBLE NUMBER(1) BOOLEAN DATE
TIMESTAMP [(p)] TIMESTAMP [(p)] [WITHOUT TIMEZONE] TIMESTAMP [(p)] WITH TIME ZONE TIMESTAMP [(p)] WITH TIME ZONE TIMESTAMP [(p)] WITH LOCAL TIME ZONE TIMESTAMP_LTZ [(p)] CHAR(n)
NCHAR(n)
NVARCHAR2(n)
VARCHAR(n)
VARCHAR2(n)
CLOB
NCLOB
XMLType
SYS.XMLTYPE STRING BLOB
ROWID BYTES INTERVAL DAY TO SECOND
INTERVAL YEAR TO MONTH BIGINT Back to top
`}),e.add({id:25,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/paimon/",title:"Paimon",section:"Pipeline 连接器",content:` Paimon Pipeline 连接器 # Paimon Pipeline 连接器可以用作 Pipeline 的 Data Sink，将数据写入Paimon。 本文档介绍如何设置 Paimon Pipeline 连接器。
连接器的功能 # 自动建表 表结构变更同步 数据实时同步 如何创建 Pipeline # 从 MySQL 读取数据同步到 Paimon 的 Pipeline 可以定义如下：
source: type: mysql name: MySQL Source hostname: 127.0.0.1 port: 3306 username: admin password: pass tables: adb.\\.*, bdb.user_table_[0-9]+, [app|web].order_\\.* server-id: 5401-5404 sink: type: paimon name: Paimon Sink catalog.properties.metastore: filesystem catalog.properties.warehouse: /path/warehouse pipeline: name: MySQL to Paimon Pipeline parallelism: 2 Pipeline 连接器配置项 # Option Required Default Type Description type required (none) String 指定要使用的连接器, 这里需要设置成 'paimon'. name optional (none) String Sink 的名称. catalog.properties.metastore optional "filesystem" String 用于构建 Paimon Catalog 的类型。可选填值 filesystem 或者 hive。 catalog.properties.warehouse optional (none) String Paimon 仓库存储数据的根目录。 catalog.properties.uri optional (none) String Hive metastore 的 uri，在 metastore 设置为 hive 的时候需要。 commit.user optional admin String 提交数据文件时的用户名。 partition.key optional (none) String 设置每个分区表的分区字段，允许填写成多个分区表的多个分区字段。 不同的表使用 ';'分割， 而不同的字段则使用 ','分割。举个例子， 我们可以为两张表的不同分区字段作如下的设置 'testdb.table1:id1,id2;testdb.table2:name'。 catalog.properties.* optional (none) String 将 Paimon catalog 支持的参数传递给 pipeline，参考 Paimon catalog options。 table.properties.* optional (none) String 将 Paimon table 支持的参数传递给 pipeline，参考 Paimon table options。 使用说明 # 只支持主键表，因此源表必须有主键
暂不支持 exactly-once，连接器 通过 at-least-once 和主键表实现幂等写
数据类型映射 # CDC type Paimon type NOTE TINYINT TINYINT SMALLINT SMALLINT INT INT BIGINT BIGINT FLOAT FLOAT DOUBLE DOUBLE DECIMAL(p, s) DECIMAL(p, s) BOOLEAN BOOLEAN DATE DATE TIMESTAMP DATETIME TIMESTAMP_LTZ TIMESTAMP_LTZ CHAR(n) CHAR(n) VARCHAR(n) VARCHAR(n) Back to top
`}),e.add({id:26,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/",title:"连接器",section:"Docs",content:" "}),e.add({id:27,href:"/flink/flink-cdc-docs-master/zh/docs/developer-guide/licenses/",title:"许可证",section:"开发者指南",content:` 许可证 # Flink CDC 使用的是 Apache License 2.0 许可。
如果对于 Flink CDC 的许可证有问题，可以通过邮件列表联系我们。
Apache Software Foundation # 您可以通过以下一些链接了解更多关于 ASF 的信息。
Apache Software Foundation License Events Security SponSprShip Thanks Privacy `}),e.add({id:28,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/kafka/",title:"Kafka",section:"Pipeline 连接器",content:" Kafka Pipeline 连接器 # Kafka Pipeline 连接器可以用作 Pipeline 的 Data Sink，将数据写入Kafka。 本文档介绍如何设置 Kafka Pipeline 连接器。\n连接器的功能 # 自动建表 表结构变更同步 数据实时同步 如何创建 Pipeline # 从 MySQL 读取数据同步到 Kafka 的 Pipeline 可以定义如下：\nsource: type: mysql name: MySQL Source hostname: 127.0.0.1 port: 3306 username: admin password: pass tables: adb.\\.*, bdb.user_table_[0-9]+, [app|web].order_\\.* server-id: 5401-5404 sink: type: kafka name: Kafka Sink properties.bootstrap.servers: PLAINTEXT://localhost:62510 pipeline: name: MySQL to Kafka Pipeline parallelism: 2 Pipeline 连接器配置项 # Option Required Default Type Description type required (none) String 指定要使用的连接器, 这里需要设置成 'kafka'。 name optional (none) String Sink 的名称。 value.format optional (none) String 用于序列化 Kafka 消息的值部分数据的格式。可选的填写值包括 debezium-json 和 canal-json, 默认值为 `debezium-json`，并且目前不支持用户自定义输出格式。 properties.bootstrap.servers required (none) String 用于建立与 Kafka 集群初始连接的主机/端口对列表。 topic optional (none) String 如果配置了这个参数，所有的消息都会发送到这一个主题。 sink.add-tableId-to-header-enabled optional (none) Boolean 如果配置了这个参数，所有的消息都会带上键为 `namespace`, 'schemaName', 'tableName'，值为事件 TableId 里对应的 字符串的 header。 properties.* optional (none) String 将 Kafka 支持的参数传递给 pipeline，参考 Kafka consume options。 sink.custom-header optional (none) String Kafka 记录自定义的 Header。每个 Header 使用 ','分割， 键值使用 ':' 分割。举例来说，可以使用这种方式 'key1:value1,key2:value2'。 使用说明 # 写入 Kafka 的 topic 默认会是上游表 namespace.schemaName.tableName 对应的字符串，可以通过 pipeline 的 route 功能进行修改。 如果配置了 topic 参数，所有的消息都会发送到这一个主题。 写入 Kafka 的 topic 如果不存在，则会默认创建。 数据类型映射 # CDC type JSON type NOTE TINYINT TINYINT SMALLINT SMALLINT INT INT BIGINT BIGINT FLOAT FLOAT DOUBLE DOUBLE DECIMAL(p, s) DECIMAL(p, s) BOOLEAN BOOLEAN DATE DATE TIMESTAMP DATETIME TIMESTAMP_LTZ TIMESTAMP_LTZ CHAR(n) CHAR(n) VARCHAR(n) VARCHAR(n) Back to top\n"}),e.add({id:29,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/oracle-tutorial/",title:"Oracle 教程",section:"Flink CDC Sources 教程",content:` 演示: Oracle CDC 导入 Elasticsearch # 创建docker-compose.yml文件，内容如下所示:
version: &#39;2.1&#39; services: oracle: image: goodboy008/oracle-19.3.0-ee:non-cdb ports: - &#34;1521:1521&#34; elasticsearch: image: elastic/elasticsearch:7.6.0 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34; - discovery.type=single-node ports: - &#34;9200:9200&#34; - &#34;9300:9300&#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 ports: - &#34;5601:5601&#34; volumes: - /var/run/docker.sock:/var/run/docker.sock 该 Docker Compose 中包含的容器有:
Oracle: Oracle 19c 数据库 Elasticsearch: orders 表将和 products 表进行join，join的结果写入Elasticsearch中 Kibana: 可视化 Elasticsearch 中的数据 在 docker-compose.yml 所在目录下运行如下命令以启动所有容器：
docker-compose up -d 该命令会以 detached 模式自动启动 Docker Compose 配置中定义的所有容器。 你可以通过 docker ps 来观察上述的容器是否正常启动了。 也可以访问 http://localhost:5601/ 来查看 Kibana 是否运行正常。 另外可以通过如下命令停止所有的容器：
docker-compose down 下载以下 jar 包到 &lt;FLINK_HOME&gt;/lib/:
下载链接只对已发布的版本有效, SNAPSHOT 版本需要本地编译
flink-sql-connector-elasticsearch7-3.0.1-1.17.jar flink-sql-connector-oracle-cdc-2.4.0.jar 在 Oracle 数据库中准备数据
创建数据库和表 products，orders，并插入数据：
docker-compose exec oracle sqlplus debezium/dbz@localhost:1521/ORCLCDB BEGIN EXECUTE IMMEDIATE &#39;DROP TABLE DEBEZIUM.PRODUCTS&#39;; EXCEPTION WHEN OTHERS THEN IF SQLCODE != -942 THEN RAISE; END IF; END; / CREATE TABLE DEBEZIUM.PRODUCTS ( ID NUMBER(9, 0) NOT NULL, NAME VARCHAR(255) NOT NULL, DESCRIPTION VARCHAR(512), WEIGHT FLOAT, PRIMARY KEY(ID) ); BEGIN EXECUTE IMMEDIATE &#39;DROP TABLE DEBEZIUM.ORDERS&#39;; EXCEPTION WHEN OTHERS THEN IF SQLCODE != -942 THEN RAISE; END IF; END; / CREATE TABLE DEBEZIUM.ORDERS ( ID NUMBER(9, 0) NOT NULL, ORDER_DATE TIMESTAMP(3) NOT NULL, PURCHASER VARCHAR(255) NOT NULL, QUANTITY NUMBER(9, 0) NOT NULL, PRODUCT_ID NUMBER(9, 0) NOT NULL, PRIMARY KEY(ID) ); ALTER TABLE DEBEZIUM.PRODUCTS ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS; ALTER TABLE DEBEZIUM.ORDERS ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS; INSERT INTO DEBEZIUM.PRODUCTS VALUES (101, &#39;scooter&#39;, &#39;Small 2-wheel scooter&#39;, 3.14); INSERT INTO DEBEZIUM.PRODUCTS VALUES (102, &#39;car battery&#39;, &#39;12V car battery&#39;, 8.1); INSERT INTO DEBEZIUM.PRODUCTS VALUES (103, &#39;12-pack drill bits&#39;, &#39;12-pack of drill bits with sizes ranging from #40 to #3&#39;, 0.8); INSERT INTO DEBEZIUM.PRODUCTS VALUES (104, &#39;hammer&#39;, &#39;12oz carpenter&#39;&#39;s hammer&#39;, 0.75); INSERT INTO DEBEZIUM.PRODUCTS VALUES (105, &#39;hammer&#39;, &#39;14oz carpenter&#39;&#39;s hammer&#39;, 0.875); INSERT INTO DEBEZIUM.PRODUCTS VALUES (106, &#39;hammer&#39;, &#39;16oz carpenter&#39;&#39;s hammer&#39;, 1.0); INSERT INTO DEBEZIUM.PRODUCTS VALUES (107, &#39;rocks&#39;, &#39;box of assorted rocks&#39;, 5.3); INSERT INTO DEBEZIUM.PRODUCTS VALUES (108, &#39;jacket&#39;, &#39;water resistent black wind breaker&#39;, 0.1); INSERT INTO DEBEZIUM.PRODUCTS VALUES (109, &#39;spare tire&#39;, &#39;24 inch spare tire&#39;, 22.2); INSERT INTO DEBEZIUM.ORDERS VALUES (1001, TO_TIMESTAMP(&#39;2020-07-30 10:08:22.001000&#39;, &#39;YYYY-MM-DD HH24:MI:SS.FF&#39;), &#39;Jark&#39;, 1, 101); INSERT INTO DEBEZIUM.ORDERS VALUES (1002, TO_TIMESTAMP(&#39;2020-07-30 10:11:09.001000&#39;, &#39;YYYY-MM-DD HH24:MI:SS.FF&#39;), &#39;Sally&#39;, 2, 102); INSERT INTO DEBEZIUM.ORDERS VALUES (1003, TO_TIMESTAMP(&#39;2020-07-30 12:00:30.001000&#39;, &#39;YYYY-MM-DD HH24:MI:SS.FF&#39;), &#39;Edward&#39;, 2, 103); INSERT INTO DEBEZIUM.ORDERS VALUES (1004, TO_TIMESTAMP(&#39;2020-07-30 15:22:00.001000&#39;, &#39;YYYY-MM-DD HH24:MI:SS.FF&#39;), &#39;Jark&#39;, 1, 104); 然后启动 Flink 集群，再启动 SQL CLI:
-- Flink SQL -- checkpoint every 3000 milliseconds Flink SQL&gt; SET execution.checkpointing.interval = 3s; Flink SQL&gt; CREATE TABLE products ( ID INT, NAME STRING, DESCRIPTION STRING, PRIMARY KEY (ID) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;oracle-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;1521&#39;, &#39;username&#39; = &#39;dbzuser&#39;, &#39;password&#39; = &#39;dbz&#39;, &#39;database-name&#39; = &#39;ORCLCDB&#39;, &#39;schema-name&#39; = &#39;DEBEZIUM&#39;, &#39;table-name&#39; = &#39;products&#39; ); Flink SQL&gt; CREATE TABLE orders ( ID INT, ORDER_DATE TIMESTAMP(3), PURCHASER STRING, QUANTITY INT, PRODUCT_ID INT, ORDER_STATUS BOOLEAN ) WITH ( &#39;connector&#39; = &#39;oracle-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;1521&#39;, &#39;username&#39; = &#39;dbzuser&#39;, &#39;password&#39; = &#39;dbz&#39;, &#39;database-name&#39; = &#39;ORCLCDB&#39;, &#39;schema-name&#39; = &#39;DEBEZIUM&#39;, &#39;table-name&#39; = &#39;orders&#39; ); Flink SQL&gt; CREATE TABLE enriched_orders ( ORDER_ID INT, ORDER_DATE TIMESTAMP(3), PURCHASER STRING, QUANTITY INT, PRODUCT_NAME STRING, PRODUCT_DESCRIPTION STRING, PRIMARY KEY (ORDER_ID) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;elasticsearch-7&#39;, &#39;hosts&#39; = &#39;http://localhost:9200&#39;, &#39;index&#39; = &#39;enriched_orders_1&#39; ); Flink SQL&gt; INSERT INTO enriched_orders SELECT o.ID,o.ORDER_DATE,o.PURCHASER,o.QUANTITY, p.NAME, p.DESCRIPTION FROM orders AS o LEFT JOIN products AS p ON o.PRODUCT_ID = p.ID; 检查 ElasticSearch 中的结果
检查最终的结果是否写入ElasticSearch中, 可以在Kibana看到ElasticSearch中的数据
在 Oracle 制造一些变更，观察 ElasticSearch 中的结果
进入Oracle容器中并通过如下的SQL语句对Oracle数据库进行一些修改, 然后就可以看到每执行一条SQL语句，Elasticsearch中的数据都会实时更新。
docker-compose exec oracle sqlplus debezium/dbz@localhost:1521/ORCLCDB INSERT INTO DEBEZIUM.ORDERS VALUES (1005, TO_TIMESTAMP(&#39;2020-07-30 15:22:00.001000&#39;, &#39;YYYY-MM-DD HH24:MI:SS.FF&#39;), &#39;Jark&#39;, 5, 105); UPDATE DEBEZIUM.ORDERS SET QUANTITY = 10 WHERE ID = 1002; DELETE FROM DEBEZIUM.ORDERS WHERE ID = 1004; Back to top
`}),e.add({id:30,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/sqlserver-cdc/",title:"SQL Server",section:"Flink Source 连接器",content:` SQLServer CDC Connector # The SQLServer CDC connector allows for reading snapshot data and incremental data from SQLServer database. This document describes how to setup the SQLServer CDC connector to run SQL queries against SQLServer databases.
Dependencies # In order to setup the SQLServer CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency # &ltdependency&gt &ltgroupId&gtorg.apache.flink&lt/groupId&gt &ltartifactId&gtflink-connector-sqlserver-cdc&lt/artifactId&gt &ltversion&gt3.2-SNAPSHOT&lt/version&gt &lt/dependency&gt Copied to clipboard! SQL Client JAR # Download link is available only for stable releases.
Download flink-sql-connector-sqlserver-cdc-3.0.1.jar and put it under &lt;FLINK_HOME&gt;/lib/.
Note: Refer to flink-sql-connector-sqlserver-cdc, more released versions will be available in the Maven central warehouse.
Setup SQLServer Database # A SQL Server administrator must enable change data capture on the source tables that you want to capture. The database must already be enabled for CDC. To enable CDC on a table, a SQL Server administrator runs the stored procedure sys.sp_cdc_enable_table for the table.
Prerequisites:
CDC is enabled on the SQL Server database. The SQL Server Agent is running. You are a member of the db_owner fixed database role for the database. Procedure:
Connect to the SQL Server database by database management studio. Run the following SQL statement to enable CDC on the table. USE MyDB GO EXEC sys.sp_cdc_enable_table @source_schema = N&#39;dbo&#39;, -- Specifies the schema of the source table. @source_name = N&#39;MyTable&#39;, -- Specifies the name of the table that you want to capture. @role_name = N&#39;MyRole&#39;, -- Specifies a role MyRole to which you can add users to whom you want to grant SELECT permission on the captured columns of the source table. Users in the sysadmin or db_owner role also have access to the specified change tables. Set the value of @role_name to NULL, to allow only members in the sysadmin or db_owner to have full access to captured information. @filegroup_name = N&#39;MyDB_CT&#39;,-- Specifies the filegroup where SQL Server places the change table for the captured table. The named filegroup must already exist. It is best not to locate change tables in the same filegroup that you use for source tables. @supports_net_changes = 0 GO Verifying that the user has access to the CDC table --The following example runs the stored procedure sys.sp_cdc_help_change_data_capture on the database MyDB: USE MyDB; GO EXEC sys.sp_cdc_help_change_data_capture GO The query returns configuration information for each table in the database that is enabled for CDC and that contains change data that the caller is authorized to access. If the result is empty, verify that the user has privileges to access both the capture instance and the CDC tables.
How to create a SQLServer CDC table # The SqlServer CDC table can be defined as following:
-- register a SqlServer table &#39;orders&#39; in Flink SQL CREATE TABLE orders ( id INT, order_date DATE, purchaser INT, quantity INT, product_id INT, PRIMARY KEY (id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;sqlserver-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;1433&#39;, &#39;username&#39; = &#39;sa&#39;, &#39;password&#39; = &#39;Password!&#39;, &#39;database-name&#39; = &#39;inventory&#39;, &#39;table-name&#39; = &#39;dob.orders&#39; ); -- read snapshot and binlogs from orders table SELECT * FROM orders; Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'sqlserver-cdc'. hostname required (none) String IP address or hostname of the SQLServer database. username required (none) String Username to use when connecting to the SQLServer database. password required (none) String Password to use when connecting to the SQLServer database. database-name required (none) String Database name of the SQLServer database to monitor. table-name required (none) String Table name of the SQLServer database to monitor, e.g.: "db1.table1" port optional 1433 Integer Integer port number of the SQLServer database. server-time-zone optional UTC String The session time zone in database server, e.g. "Asia/Shanghai". scan.incremental.snapshot.enabled optional true Boolean Whether enable parallelism snapshot. chunk-meta.group.size optional 1000 Integer The group size of chunk meta, if the meta size exceeds the group size, the meta will be divided into multiple groups. chunk-key.even-distribution.factor.lower-bound optional 0.05d Double The lower bound of chunk key distribution factor. The distribution factor is used to determine whether the table is evenly distribution or not. The table chunks would use evenly calculation optimization when the data distribution is even, and the query for splitting would happen when it is uneven. The distribution factor could be calculated by (MAX(id) - MIN(id) + 1) / rowCount. chunk-key.even-distribution.factor.upper-bound optional 1000.0d Double The upper bound of chunk key distribution factor. The distribution factor is used to determine whether the table is evenly distribution or not. The table chunks would use evenly calculation optimization when the data distribution is even, and the query for splitting would happen when it is uneven. The distribution factor could be calculated by (MAX(id) - MIN(id) + 1) / rowCount. debezium.* optional (none) String Pass-through Debezium's properties to Debezium Embedded Engine which is used to capture data changes from SQLServer. For example: 'debezium.snapshot.mode' = 'initial_only'. See more about the Debezium's SQLServer Connector properties scan.incremental.close-idle-reader.enabled optional false Boolean Whether to close idle readers at the end of the snapshot phase. The flink version is required to be greater than or equal to 1.14 when 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' is set to true.
If the flink version is greater than or equal to 1.15, the default value of 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' has been changed to true, so it does not need to be explicitly configured 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' = 'true' scan.incremental.snapshot.chunk.key-column optional (none) String The chunk key of table snapshot, captured tables are split into multiple chunks by a chunk key when read the snapshot of table. By default, the chunk key is the first column of the primary key. This column must be a column of the primary key. Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Key DataType Description table_name STRING NOT NULL Name of the table that contain the row. schema_name STRING NOT NULL Name of the schema that contain the row. database_name STRING NOT NULL Name of the database that contain the row. op_ts TIMESTAMP_LTZ(3) NOT NULL It indicates the time that the change was made in the database. If the record is read from snapshot of the table instead of the change stream, the value is always 0. Limitation # Can&rsquo;t perform checkpoint during scanning snapshot of tables # During scanning snapshot of database tables, since there is no recoverable position, we can&rsquo;t perform checkpoints. In order to not perform checkpoints, SqlServer CDC source will keep the checkpoint waiting to timeout. The timeout checkpoint will be recognized as failed checkpoint, by default, this will trigger a failover for the Flink job. So if the database table is large, it is recommended to add following Flink configurations to avoid failover because of the timeout checkpoints:
execution.checkpointing.interval: 10min execution.checkpointing.tolerable-failed-checkpoints: 100 restart-strategy: fixed-delay restart-strategy.fixed-delay.attempts: 2147483647 The extended CREATE TABLE example demonstrates the syntax for exposing these metadata fields:
CREATE TABLE products ( table_name STRING METADATA FROM &#39;table_name&#39; VIRTUAL, schema_name STRING METADATA FROM &#39;schema_name&#39; VIRTUAL, db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, id INT NOT NULL, name STRING, description STRING, weight DECIMAL(10,3) ) WITH ( &#39;connector&#39; = &#39;sqlserver-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;1433&#39;, &#39;username&#39; = &#39;sa&#39;, &#39;password&#39; = &#39;Password!&#39;, &#39;database-name&#39; = &#39;inventory&#39;, &#39;table-name&#39; = &#39;dbo.products&#39; ); Features # Exactly-Once Processing # The SQLServer CDC connector is a Flink Source connector which will read database snapshot first and then continues to read change events with exactly-once processing even failures happen. Please read How the connector works.
Startup Reading Position # The config option scan.startup.mode specifies the startup mode for SQLServer CDC consumer. The valid enumerations are:
initial (default): Takes a snapshot of structure and data of captured tables; useful if topics should be populated with a complete representation of the data from the captured tables. initial-only: Takes a snapshot of structure and data like initial but instead does not transition into streaming changes once the snapshot has completed. latest-offset: Takes a snapshot of the structure of captured tables only; useful if only changes happening from now onwards should be propagated to topics. Note: the mechanism of scan.startup.mode option relying on Debezium&rsquo;s snapshot.mode configuration. So please do not use them together. If you specific both scan.startup.mode and debezium.snapshot.mode options in the table DDL, it may make scan.startup.mode doesn&rsquo;t work.
Single Thread Reading # The SQLServer CDC source can&rsquo;t work in parallel reading, because there is only one task can receive change events.
DataStream Source # The SQLServer CDC connector can also be a DataStream source. You can create a SourceFunction as the following shows:
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.cdc.connectors.sqlserver.SqlServerSource; public class SqlServerSourceExample { public static void main(String[] args) throws Exception { SourceFunction&lt;String&gt; sourceFunction = SqlServerSource.&lt;String&gt;builder() .hostname(&#34;localhost&#34;) .port(1433) .database(&#34;sqlserver&#34;) // monitor sqlserver database .tableList(&#34;dbo.products&#34;) // monitor products table .username(&#34;sa&#34;) .password(&#34;Password!&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env .addSource(sourceFunction) .print().setParallelism(1); // use parallelism 1 for sink to keep message ordering env.execute(); } } The SQLServer CDC incremental connector (after 2.4.0) can be used as the following shows:
import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.cdc.connectors.base.options.StartupOptions; import org.apache.flink.cdc.connectors.sqlserver.source.SqlServerSourceBuilder; import org.apache.flink.cdc.connectors.sqlserver.source.SqlServerSourceBuilder.SqlServerIncrementalSource; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; public class SqlServerIncrementalSourceExample { public static void main(String[] args) throws Exception { SqlServerIncrementalSource&lt;String&gt; sqlServerSource = new SqlServerSourceBuilder() .hostname(&#34;localhost&#34;) .port(1433) .databaseList(&#34;inventory&#34;) .tableList(&#34;dbo.products&#34;) .username(&#34;sa&#34;) .password(&#34;Password!&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) .startupOptions(StartupOptions.initial()) .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000); // set the source parallelism to 2 env.fromSource( sqlServerSource, WatermarkStrategy.noWatermarks(), &#34;SqlServerIncrementalSource&#34;) .setParallelism(2) .print() .setParallelism(1); env.execute(&#34;Print SqlServer Snapshot + Change Stream&#34;); } } Data Type Mapping # SQLServer type Flink SQL type char(n) CHAR(n) varchar(n)
nvarchar(n)
nchar(n) VARCHAR(n) text
ntext
xml STRING decimal(p, s)
money
smallmoney DECIMAL(p, s) numeric NUMERIC float
real DOUBLE bit BOOLEAN int INT tinyint SMALLINT smallint SMALLINT bigint BIGINT date DATE time(n) TIME(n) datetime2
datetime
smalldatetime TIMESTAMP(n) datetimeoffset TIMESTAMP_LTZ(3) Back to top
`}),e.add({id:31,href:"/flink/flink-cdc-docs-master/zh/docs/core-concept/table-id/",title:"Table ID",section:"核心概念",content:` Definition # When connecting to external systems, it is necessary to establish a mapping relationship with the storage objects of the external system. This is what Table Id refers to.
Example # To be compatible with most external systems, the Table Id is represented by a 3-tuple : (namespace, schemaName, tableName).
Connectors should establish the mapping between Table Id and storage objects in external systems.
The following table lists the parts in table Id of different data systems:
data system parts in tableId String example Oracle/PostgreSQL database, schema, table mydb.default.orders MySQL/Doris/StarRocks database, table mydb.orders Kafka topic orders `}),e.add({id:32,href:"/flink/flink-cdc-docs-master/zh/docs/deployment/",title:"部署模式",section:"Docs",content:" "}),e.add({id:33,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/doris/",title:"Doris",section:"Pipeline 连接器",content:` Doris Connector # 本文介绍了Pipeline Doris Connector的使用。
示例 # source: type: values name: ValuesSource sink: type: doris name: Doris Sink fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; table.create.properties.replication_num: 1 pipeline: parallelism: 1 连接器配置项 # Option Required Default Type Description type required (none) String 指定要使用的Sink, 这里是 'doris'. name optional (none) String PipeLine的名称 fenodes required (none) String Doris集群FE的Http地址, 比如 127.0.0.1:8030 benodes optional (none) String Doris集群BE的Http地址, 比如 127.0.0.1:8040 jdbc-url optional (none) String Doris集群的JDBC地址，比如：jdbc:mysql://127.0.0.1:9030/db username required (none) String Doris集群的用户名 password optional (none) String Doris集群的密码 auto-redirect optional false String 是否通过FE重定向写入，直连BE写入 sink.enable.batch-mode optional true Boolean 是否使用攒批方式写入Doris sink.flush.queue-size optional 2 Integer 攒批写入的队列大小 sink.buffer-flush.max-rows optional 50000 Integer 单个批次最大Flush的记录数 sink.buffer-flush.max-bytes optional 10485760(10MB) Integer 单个批次最大Flush的字节数 sink.buffer-flush.interval optional 10s String Flush的间隔时长，超过这个时间，将异步Flush数据 sink.properties. optional (none) String StreamLoad的参数。 For example: sink.properties.strict_mode: true. 查看更多关于 StreamLoad的Properties 属性 table.create.properties.* optional (none) String 创建表的Properties配置。 For example: table.create.properties.replication_num: 1. 查看更多关于 Doris Table 的 Properties 属性 数据类型映射 # CDC type Doris type NOTE TINYINT TINYINT SMALLINT SMALLINT INT INT BIGINT BIGINT DECIMAL DECIMAL FLOAT FLOAT DOUBLE DOUBLE BOOLEAN BOOLEAN DATE DATE TIMESTAMP [(p)] DATETIME [(p)] TIMESTAMP_LTZ [(p)] DATETIME [(p)] CHAR(n) CHAR(n*3) 在Doris中，字符串是以UTF-8编码存储的，所以英文字符占1个字节，中文字符占3个字节。这里的长度统一乘3，CHAR最大的长度是255，超过后会自动转为VARCHAR类型 VARCHAR(n) VARCHAR(n*3) 同上，这里的长度统一乘3，VARCHAR最大的长度是65533，超过后会自动转为STRING类型 BINARY(n) STRING VARBINARY(N) STRING STRING STRING Back to top
`}),e.add({id:34,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/polardbx-tutorial/",title:"PolarDB-X 教程",section:"Flink CDC Sources 教程",content:` 演示: PolarDB-X CDC 导入 Elasticsearch # 本示例我们通过演示 PolarDB-X 借助 Flink-CDC 将数据导入至 Elasticsearch 来介绍 PolarDB-X 的增量订阅能力，你可以前往:PolarDB-X 了解更多细节。
准备教程所需要的组件 # 我们假设你运行在一台 MacOS 或者 Linux 机器上，并且已经安装 docker.
配置并启动容器 # 配置 docker-compose.yml。
version: &#39;2.1&#39; services: polardbx: polardbx: image: polardbx/polardb-x:2.0.1 container_name: polardbx ports: - &#34;8527:8527&#34; elasticsearch: image: &#39;elastic/elasticsearch:7.6.0&#39; container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - ES_JAVA_OPTS=-Xms512m -Xmx512m - discovery.type=single-node ports: - &#39;9200:9200&#39; - &#39;9300:9300&#39; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: &#39;elastic/kibana:7.6.0&#39; container_name: kibana ports: - &#39;5601:5601&#39; volumes: - &#39;/var/run/docker.sock:/var/run/docker.sock&#39; 该 Docker Compose 中包含的容器有：
PolarDB-X: 商品表 products 和 订单表 orders 将存储在该数据库中， 这两张表将进行关联，得到一张包含更多信息的订单表 enriched_orders Elasticsearch: 最终的订单表 enriched_orders 将写到 Elasticsearch Kibana: 用来可视化 ElasticSearch 的数据 在 docker-compose.yml 所在目录下执行下面的命令来启动本教程需要的组件：
docker-compose up -d 该命令将以 detached 模式自动启动 Docker Compose 配置中定义的所有容器。你可以通过 docker ps 来观察上述的容器是否正常启动了，也可以通过访问 http://localhost:5601/ 来查看 Kibana 是否运行正常
准备数据： # 使用已创建的用户名和密码进行登陆PolarDB-X。
mysql -h127.0.0.1 -P8527 -upolardbx_root -p&#34;123456&#34; CREATE DATABASE mydb; USE mydb; -- 创建一张产品表，并写入一些数据 CREATE TABLE products ( id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(512) ) AUTO_INCREMENT = 101; INSERT INTO products VALUES (default,&#34;scooter&#34;,&#34;Small 2-wheel scooter&#34;), (default,&#34;car battery&#34;,&#34;12V car battery&#34;), (default,&#34;12-pack drill bits&#34;,&#34;12-pack of drill bits with sizes ranging from #40 to #3&#34;), (default,&#34;hammer&#34;,&#34;12oz carpenter&#39;s hammer&#34;), (default,&#34;hammer&#34;,&#34;14oz carpenter&#39;s hammer&#34;), (default,&#34;hammer&#34;,&#34;16oz carpenter&#39;s hammer&#34;), (default,&#34;rocks&#34;,&#34;box of assorted rocks&#34;), (default,&#34;jacket&#34;,&#34;water resistent black wind breaker&#34;), (default,&#34;spare tire&#34;,&#34;24 inch spare tire&#34;); -- 创建一张订单表，并写入一些数据 CREATE TABLE orders ( order_id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, order_date DATETIME NOT NULL, customer_name VARCHAR(255) NOT NULL, price DECIMAL(10, 5) NOT NULL, product_id INTEGER NOT NULL, order_status BOOLEAN NOT NULL -- Whether order has been placed ) AUTO_INCREMENT = 10001; INSERT INTO orders VALUES (default, &#39;2020-07-30 10:08:22&#39;, &#39;Jark&#39;, 50.50, 102, false), (default, &#39;2020-07-30 10:11:09&#39;, &#39;Sally&#39;, 15.00, 105, false), (default, &#39;2020-07-30 12:00:30&#39;, &#39;Edward&#39;, 25.25, 106, false); 下载 Flink 和所需要的依赖包 # 下载 Flink 1.17.0 并将其解压至目录 flink-1.17.0 下载下面列出的依赖包，并将它们放到目录 flink-1.17.0/lib/ 下 下载链接只对已发布的版本有效, SNAPSHOT 版本需要本地编译
用于订阅PolarDB-X Binlog: flink-sql-connector-mysql-cdc-2.4.0.jar 用于写入Elasticsearch: flink-sql-connector-elasticsearch7-3.0.1-1.17.jar 启动flink服务: ./bin/start-cluster.sh 我们可以访问 http://localhost:8081/ 看到Flink正常运行:
启动Flink SQL CLI: ./bin/sql-client.sh 在 Flink SQL CLI 中使用 Flink DDL 创建表 # -- 设置间隔时间为3秒 Flink SQL&gt; SET execution.checkpointing.interval = 3s; -- 创建source1 -订单表 Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;127.0.0.1&#39;, &#39;port&#39; = &#39;8527&#39;, &#39;username&#39; = &#39;polardbx_root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;orders&#39; ); -- 创建source2 -产品表 CREATE TABLE products ( id INT, name STRING, description STRING, PRIMARY KEY (id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;127.0.0.1&#39;, &#39;port&#39; = &#39;8527&#39;, &#39;username&#39; = &#39;polardbx_root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;products&#39; ); -- 创建sink - 关联后的结果表 Flink SQL&gt; CREATE TABLE enriched_orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, product_name STRING, product_description STRING, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;elasticsearch-7&#39;, &#39;hosts&#39; = &#39;http://localhost:9200&#39;, &#39;index&#39; = &#39;enriched_orders&#39; ); -- 执行读取和写入 Flink SQL&gt; INSERT INTO enriched_orders SELECT o.order_id, o.order_date, o.customer_name, o.price, o.product_id, o.order_status, p.name, p.description FROM orders AS o LEFT JOIN products AS p ON o.product_id = p.id; 在 Kibana 中查看数据 # 访问 http://localhost:5601/app/kibana#/management/kibana/index_pattern
创建 index pattern enriched_orders，之后可以在 http://localhost:5601/app/kibana#/discover 看到写入的数据了。
修改监听表数据，查看增量数据变动 # 在PolarDB-X中依次执行如下修改操作，每执行一步就刷新一次 Kibana，可以看到 Kibana 中显示的订单数据将实时更新。
INSERT INTO orders VALUES (default, &#39;2020-07-30 15:22:00&#39;, &#39;Jark&#39;, 29.71, 104, false); UPDATE orders SET order_status = true WHERE order_id = 10004; DELETE FROM orders WHERE order_id = 10004; 环境清理 # 在 docker-compose.yml 文件所在的目录下执行如下命令停止所有容器：
docker-compose down 进入Flink的部署目录，停止 Flink 集群：
./bin/stop-cluster.sh Back to top
`}),e.add({id:35,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/postgres-cdc/",title:"Postgres",section:"Flink Source 连接器",content:` Postgres CDC Connector # The Postgres CDC connector allows for reading snapshot data and incremental data from PostgreSQL database. This document describes how to setup the Postgres CDC connector to run SQL queries against PostgreSQL databases.
Dependencies # In order to setup the Postgres CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency # &ltdependency&gt &ltgroupId&gtorg.apache.flink&lt/groupId&gt &ltartifactId&gtflink-connector-postgres-cdc&lt/artifactId&gt &ltversion&gt3.2-SNAPSHOT&lt/version&gt &lt/dependency&gt Copied to clipboard! SQL Client JAR # Download link is available only for stable releases.
Download flink-sql-connector-postgres-cdc-3.0.1.jar and put it under &lt;FLINK_HOME&gt;/lib/.
Note: Refer to flink-sql-connector-postgres-cdc, more released versions will be available in the Maven central warehouse.
How to create a Postgres CDC table # The Postgres CDC table can be defined as following:
-- register a PostgreSQL table &#39;shipments&#39; in Flink SQL CREATE TABLE shipments ( shipment_id INT, order_id INT, origin STRING, destination STRING, is_arrived BOOLEAN ) WITH ( &#39;connector&#39; = &#39;postgres-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;5432&#39;, &#39;username&#39; = &#39;postgres&#39;, &#39;password&#39; = &#39;postgres&#39;, &#39;database-name&#39; = &#39;postgres&#39;, &#39;schema-name&#39; = &#39;public&#39;, &#39;table-name&#39; = &#39;shipments&#39;, &#39;slot.name&#39; = &#39;flink&#39;, -- experimental feature: incremental snapshot (default off) &#39;scan.incremental.snapshot.enabled&#39; = &#39;true&#39; ); -- read snapshot and binlogs from shipments table SELECT * FROM shipments; Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'postgres-cdc'. hostname required (none) String IP address or hostname of the PostgreSQL database server. username required (none) String Name of the PostgreSQL database to use when connecting to the PostgreSQL database server. password required (none) String Password to use when connecting to the PostgreSQL database server. database-name required (none) String Database name of the PostgreSQL server to monitor. schema-name required (none) String Schema name of the PostgreSQL database to monitor. table-name required (none) String Table name of the PostgreSQL database to monitor. port optional 5432 Integer Integer port number of the PostgreSQL database server. slot.name required (none) String The name of the PostgreSQL logical decoding slot that was created for streaming changes from a particular plug-in for a particular database/schema. The server uses this slot to stream events to the connector that you are configuring. Slot names must conform to PostgreSQL replication slot naming rules, which state: "Each replication slot has a name, which can contain lower-case letters, numbers, and the underscore character." decoding.plugin.name optional decoderbufs String The name of the Postgres logical decoding plug-in installed on the server. Supported values are decoderbufs, wal2json, wal2json_rds, wal2json_streaming, wal2json_rds_streaming and pgoutput. changelog-mode optional all String The changelog mode used for encoding streaming changes. Supported values are all (which encodes changes as retract stream using all RowKinds) and upsert (which encodes changes as upsert stream that describes idempotent updates on a key). upsert mode can be used for tables with primary keys when replica identity FULL is not an option. Primary keys must be set to use upsert mode. heartbeat.interval.ms optional 30s Duration The interval of sending heartbeat event for tracing the latest available replication slot offsets debezium.* optional (none) String Pass-through Debezium's properties to Debezium Embedded Engine which is used to capture data changes from Postgres server. For example: 'debezium.snapshot.mode' = 'never'. See more about the Debezium's Postgres Connector properties debezium.snapshot.select.statement.overrides optional (none) String If you encounter a situation where there is a large amount of data in the table and you don't need all the historical data. You can try to specify the underlying configuration in debezium to select the data range you want to snapshot. This parameter only affects snapshots and does not affect subsequent data reading consumption. Note: PostgreSQL must use schema name and table name. For example: 'debezium.snapshot.select.statement.overrides' = 'schema.table'. After specifying the above attributes, you must also add the following attributes: debezium.snapshot.select.statement.overrides.[schema].[table] debezium.snapshot.select.statement.overrides.[schema].[table] optional (none) String You can specify SQL statements to limit the data range of snapshot. Note1: Schema and table need to be specified in the SQL statement, and the SQL should conform to the syntax of the data source.Currently. For example: 'debezium.snapshot.select.statement.overrides.schema.table' = 'select * from schema.table where 1 != 1'. Note2: The Flink SQL client submission task does not support functions with single quotation marks in the content. For example: 'debezium.snapshot.select.statement.overrides.schema.table' = 'select * from schema.table where to_char(rq, 'yyyy-MM-dd')'. scan.incremental.snapshot.enabled optional false Boolean Incremental snapshot is a new mechanism to read snapshot of a table. Compared to the old snapshot mechanism, the incremental snapshot has many advantages, including: (1) source can be parallel during snapshot reading, (2) source can perform checkpoints in the chunk granularity during snapshot reading, (3) source doesn't need to acquire global read lock (FLUSH TABLES WITH READ LOCK) before snapshot reading. Please see Incremental Snapshot Readingsection for more detailed information. scan.incremental.close-idle-reader.enabled optional false Boolean Whether to close idle readers at the end of the snapshot phase. The flink version is required to be greater than or equal to 1.14 when 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' is set to true.
If the flink version is greater than or equal to 1.15, the default value of 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' has been changed to true, so it does not need to be explicitly configured 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' = 'true' Note: slot.name is recommended to set for different tables to avoid the potential PSQLException: ERROR: replication slot &quot;flink&quot; is active for PID 974 error. See more here.
Incremental Snapshot Options # The following options is available only when scan.incremental.snapshot.enabled=true:
Option Required Default Type Description scan.incremental.snapshot.chunk.size optional 8096 Integer The chunk size (number of rows) of table snapshot, captured tables are split into multiple chunks when read the snapshot of table. scan.startup.mode optional initial String Optional startup mode for Postgres CDC consumer, valid enumerations are "initial" and "latest-offset". Please see Startup Reading Position section for more detailed information. chunk-meta.group.size optional 1000 Integer The group size of chunk meta, if the meta size exceeds the group size, the meta will be divided into multiple groups. connect.timeout optional 30s Duration The maximum time that the connector should wait after trying to connect to the PostgreSQL database server before timing out. connect.pool.size optional 30 Integer The connection pool size. connect.max-retries optional 3 Integer The max retry times that the connector should retry to build database server connection. scan.snapshot.fetch.size optional 1024 Integer The maximum fetch size for per poll when read table snapshot. scan.incremental.snapshot.chunk.key-column optional (none) String The chunk key of table snapshot, captured tables are split into multiple chunks by a chunk key when read the snapshot of table. By default, the chunk key is the first column of the primary key. This column must be a column of the primary key. chunk-key.even-distribution.factor.lower-bound optional 0.05d Double The lower bound of chunk key distribution factor. The distribution factor is used to determine whether the table is evenly distribution or not. The table chunks would use evenly calculation optimization when the data distribution is even, and the query for splitting would happen when it is uneven. The distribution factor could be calculated by (MAX(id) - MIN(id) + 1) / rowCount. chunk-key.even-distribution.factor.upper-bound optional 1000.0d Double The upper bound of chunk key distribution factor. The distribution factor is used to determine whether the table is evenly distribution or not. The table chunks would use evenly calculation optimization when the data distribution is even, and the query for splitting would happen when it is uneven. The distribution factor could be calculated by (MAX(id) - MIN(id) + 1) / rowCount. Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Key DataType Description table_name STRING NOT NULL Name of the table that contain the row. schema_name STRING NOT NULL Name of the schema that contain the row. database_name STRING NOT NULL Name of the database that contain the row. op_ts TIMESTAMP_LTZ(3) NOT NULL It indicates the time that the change was made in the database. If the record is read from snapshot of the table instead of the change stream, the value is always 0. Limitation # Can&rsquo;t perform checkpoint during scanning snapshot of tables when incremental snapshot is disabled # When scan.incremental.snapshot.enabled=false, we have the following limitation.
During scanning snapshot of database tables, since there is no recoverable position, we can&rsquo;t perform checkpoints. In order to not perform checkpoints, Postgres CDC source will keep the checkpoint waiting to timeout. The timeout checkpoint will be recognized as failed checkpoint, by default, this will trigger a failover for the Flink job. So if the database table is large, it is recommended to add following Flink configurations to avoid failover because of the timeout checkpoints:
execution.checkpointing.interval: 10min execution.checkpointing.tolerable-failed-checkpoints: 100 restart-strategy: fixed-delay restart-strategy.fixed-delay.attempts: 2147483647 The extended CREATE TABLE example demonstrates the syntax for exposing these metadata fields:
CREATE TABLE products ( db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, table_name STRING METADATA FROM &#39;table_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, shipment_id INT, order_id INT, origin STRING, destination STRING, is_arrived BOOLEAN ) WITH ( &#39;connector&#39; = &#39;postgres-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;5432&#39;, &#39;username&#39; = &#39;postgres&#39;, &#39;password&#39; = &#39;postgres&#39;, &#39;database-name&#39; = &#39;postgres&#39;, &#39;schema-name&#39; = &#39;public&#39;, &#39;table-name&#39; = &#39;shipments&#39;, &#39;slot.name&#39; = &#39;flink&#39; ); Features # Incremental Snapshot Reading (Experimental) # Incremental snapshot reading is a new mechanism to read snapshot of a table. Compared to the old snapshot mechanism, the incremental snapshot has many advantages, including:
(1) PostgreSQL CDC Source can be parallel during snapshot reading (2) PostgreSQL CDC Source can perform checkpoints in the chunk granularity during snapshot reading (3) PostgreSQL CDC Source doesn&rsquo;t need to acquire global read lock before snapshot reading During the incremental snapshot reading, the PostgreSQL CDC Source firstly splits snapshot chunks (splits) by primary key of table, and then PostgreSQL CDC Source assigns the chunks to multiple readers to read the data of snapshot chunk.
Exactly-Once Processing # The Postgres CDC connector is a Flink Source connector which will read database snapshot first and then continues to read binlogs with exactly-once processing even failures happen. Please read How the connector works.
DataStream Source # The Postgres CDC connector can also be a DataStream source. There are two modes for the DataStream source:
incremental snapshot based, which allows parallel reading SourceFunction based, which only supports single thread reading Incremental Snapshot based DataStream (Experimental) # import org.apache.flink.cdc.connectors.base.source.jdbc.JdbcIncrementalSource; import org.apache.flink.cdc.connectors.postgres.source.PostgresSourceBuilder; import org.apache.flink.cdc.debezium.DebeziumDeserializationSchema; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; public class PostgresParallelSourceExample { public static void main(String[] args) throws Exception { DebeziumDeserializationSchema&lt;String&gt; deserializer = new JsonDebeziumDeserializationSchema(); JdbcIncrementalSource&lt;String&gt; postgresIncrementalSource = PostgresSourceBuilder.PostgresIncrementalSource.&lt;String&gt;builder() .hostname(&#34;localhost&#34;) .port(5432) .database(&#34;postgres&#34;) .schemaList(&#34;inventory&#34;) .tableList(&#34;inventory.products&#34;) .username(&#34;postgres&#34;) .password(&#34;postgres&#34;) .slotName(&#34;flink&#34;) .decodingPluginName(&#34;decoderbufs&#34;) // use pgoutput for PostgreSQL 10+ .deserializer(deserializer) .includeSchemaChanges(true) // output the schema changes as well .splitSize(2) // the split size of each snapshot split .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(3000); env.fromSource( postgresIncrementalSource, WatermarkStrategy.noWatermarks(), &#34;PostgresParallelSource&#34;) .setParallelism(2) .print(); env.execute(&#34;Output Postgres Snapshot&#34;); } } SourceFunction-based DataStream # import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.cdc.connectors.postgres.PostgreSQLSource; public class PostgreSQLSourceExample { public static void main(String[] args) throws Exception { SourceFunction&lt;String&gt; sourceFunction = PostgreSQLSource.&lt;String&gt;builder() .hostname(&#34;localhost&#34;) .port(5432) .database(&#34;postgres&#34;) // monitor postgres database .schemaList(&#34;inventory&#34;) // monitor inventory schema .tableList(&#34;inventory.products&#34;) // monitor products table .username(&#34;flinkuser&#34;) .password(&#34;flinkpw&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env .addSource(sourceFunction) .print().setParallelism(1); // use parallelism 1 for sink to keep message ordering env.execute(); } } Data Type Mapping # PostgreSQL type Flink SQL type TINYINT SMALLINT
INT2
SMALLSERIAL
SERIAL2 SMALLINT INTEGER
SERIAL INT BIGINT
BIGSERIAL BIGINT DECIMAL(20, 0) BIGINT BIGINT REAL
FLOAT4 FLOAT FLOAT8
DOUBLE PRECISION DOUBLE NUMERIC(p, s)
DECIMAL(p, s) DECIMAL(p, s) BOOLEAN BOOLEAN DATE DATE TIME [(p)] [WITHOUT TIMEZONE] TIME [(p)] [WITHOUT TIMEZONE] TIMESTAMP [(p)] [WITHOUT TIMEZONE] TIMESTAMP [(p)] [WITHOUT TIMEZONE] CHAR(n)
CHARACTER(n)
VARCHAR(n)
CHARACTER VARYING(n)
TEXT STRING BYTEA BYTES Back to top
`}),e.add({id:36,href:"/flink/flink-cdc-docs-master/zh/docs/core-concept/transform/",title:"Transform",section:"核心概念",content:` Definition # Transform module helps users delete and expand data columns based on the data columns in the table. What&rsquo;s more, it also helps users filter some unnecessary data during the synchronization process.
Parameters # To describe a transform rule, the following parameters can be used:
Parameter Meaning Optional/Required source-table Source table id, supports regular expressions required projection Projection rule, supports syntax similar to the select clause in SQL optional filter Filter rule, supports syntax similar to the where clause in SQL optional primary-keys Sink table primary keys, separated by commas optional partition-keys Sink table partition keys, separated by commas optional table-options used to the configure table creation statement when automatically creating tables optional description Transform rule description optional Multiple rules can be declared in one single pipeline YAML file.
Metadata Fields # Fields definition # There are some hidden columns used to access metadata information. They will only take effect when explicitly referenced in the transform rules.
Field Data Type Description namespace_name String Name of the namespace that contains the row. schema_name String Name of the schema that contains the row. table_name String Name of the table that contains the row. Metadata relationship # Type Namespace SchemaName Table JDBC Catalog Schema Table Debezium Catalog Schema Table MySQL Database - Table Postgres Database Schema Table Oracle - Schema Table Microsoft SQL Server Database Schema Table StarRocks Database - Table Doris Database - Table Functions # Flink CDC uses Calcite to parse expressions and Janino script to evaluate expressions with function call.
Comparison Functions # Function Janino Code Description value1 = value2 valueEquals(value1, value2) Returns TRUE if value1 is equal to value2; returns FALSE if value1 or value2 is NULL. value1 &lt;&gt; value2 !valueEquals(value1, value2) Returns TRUE if value1 is not equal to value2; returns FALSE if value1 or value2 is NULL. value1 &gt; value2 value1 &gt; value2 Returns TRUE if value1 is greater than value2; returns FALSE if value1 or value2 is NULL. value1 &gt;= value2 value1 &gt;= value2 Returns TRUE if value1 is greater than or equal to value2; returns FALSE if value1 or value2 is NULL. value1 &lt; value2 value1 &lt; value2 Returns TRUE if value1 is less than value2; returns FALSE if value1 or value2 is NULL. value1 &lt;= value2 value1 &lt;= value2 Returns TRUE if value1 is less than or equal to value2; returns FALSE if value1 or value2 is NULL. value IS NULL null == value Returns TRUE if value is NULL. value IS NOT NULL null != value Returns TRUE if value is not NULL. value1 BETWEEN value2 AND value3 betweenAsymmetric(value1, value2, value3) Returns TRUE if value1 is greater than or equal to value2 and less than or equal to value3. value1 NOT BETWEEN value2 AND value3 notBetweenAsymmetric(value1, value2, value3) Returns TRUE if value1 is less than value2 or greater than value3. string1 LIKE string2 like(string1, string2) Returns TRUE if string1 matches pattern string2. string1 NOT LIKE string2 notLike(string1, string2) Returns TRUE if string1 does not match pattern string2. value1 IN (value2 [, value3]* ) in(value1, value2 [, value3]*) Returns TRUE if value1 exists in the given list (value2, value3, …). value1 NOT IN (value2 [, value3]* ) notIn(value1, value2 [, value3]*) Returns TRUE if value1 does not exist in the given list (value2, value3, …). Logical Functions # Function Janino Code Description boolean1 OR boolean2 boolean1 || boolean2 Returns TRUE if BOOLEAN1 is TRUE or BOOLEAN2 is TRUE. boolean1 AND boolean2 boolean1 &amp;&amp; boolean2 Returns TRUE if BOOLEAN1 and BOOLEAN2 are both TRUE. NOT boolean !boolean Returns TRUE if boolean is FALSE; returns FALSE if boolean is TRUE. boolean IS FALSE false == boolean Returns TRUE if boolean is FALSE; returns FALSE if boolean is TRUE. boolean IS NOT FALSE true == boolean Returns TRUE if BOOLEAN is TRUE; returns FALSE if BOOLEAN is FALSE. boolean IS TRUE true == boolean Returns TRUE if BOOLEAN is TRUE; returns FALSE if BOOLEAN is FALSE. boolean IS NOT TRUE false == boolean Returns TRUE if boolean is FALSE; returns FALSE if boolean is TRUE. Arithmetic Functions # Function Janino Code Description numeric1 + numeric2 numeric1 + numeric2 Returns NUMERIC1 plus NUMERIC2. numeric1 - numeric2 numeric1 - numeric2 Returns NUMERIC1 minus NUMERIC2. numeric1 * numeric2 numeric1 * numeric2 Returns NUMERIC1 multiplied by NUMERIC2. numeric1 / numeric2 numeric1 / numeric2 Returns NUMERIC1 divided by NUMERIC2. numeric1 % numeric2 numeric1 % numeric2 Returns the remainder (modulus) of numeric1 divided by numeric2. ABS(numeric) abs(numeric) Returns the absolute value of numeric. CEIL(numeric) ceil(numeric) Rounds numeric up, and returns the smallest number that is greater than or equal to numeric. FLOOR(numeric) floor(numeric) Rounds numeric down, and returns the largest number that is less than or equal to numeric. ROUND(numeric, int) round(numeric) Returns a number rounded to INT decimal places for NUMERIC. UUID() uuid() Returns an UUID (Universally Unique Identifier) string (e.g., &ldquo;3d3c68f7-f608-473f-b60c-b0c44ad4cc4e&rdquo;) according to RFC 4122 type 4 (pseudo randomly generated) UUID. String Functions # Function Janino Code Description string1 || string2 concat(string1, string2) Returns the concatenation of STRING1 and STRING2. CHAR_LENGTH(string) charLength(string) Returns the number of characters in STRING. UPPER(string) upper(string) Returns string in uppercase. LOWER(string) lower(string) Returns string in lowercase. TRIM(string1) trim(&lsquo;BOTH&rsquo;,string1) Returns a string that removes whitespaces at both sides. REGEXP_REPLACE(string1, string2, string3) regexpReplace(string1, string2, string3) Returns a string from STRING1 with all the substrings that match a regular expression STRING2 consecutively being replaced with STRING3. E.g., &lsquo;foobar&rsquo;.regexpReplace(&lsquo;oo|ar&rsquo;, &lsquo;&rsquo;) returns &ldquo;fb&rdquo;. SUBSTRING(string FROM integer1 [ FOR integer2 ]) substring(string,integer1,integer2) Returns a substring of STRING starting from position INT1 with length INT2 (to the end by default). CONCAT(string1, string2,…) concat(string1, string2,…) Returns a string that concatenates string1, string2, …. E.g., CONCAT(&lsquo;AA&rsquo;, &lsquo;BB&rsquo;, &lsquo;CC&rsquo;) returns &lsquo;AABBCC&rsquo;. Temporal Functions # Function Janino Code Description LOCALTIME localtime() Returns the current SQL time in the local time zone, the return type is TIME(0). LOCALTIMESTAMP localtimestamp() Returns the current SQL timestamp in local time zone, the return type is TIMESTAMP(3). CURRENT_TIME currentTime() Returns the current SQL time in the local time zone, this is a synonym of LOCAL_TIME. CURRENT_DATE currentDate() Returns the current SQL date in the local time zone. CURRENT_TIMESTAMP currentTimestamp() Returns the current SQL timestamp in the local time zone, the return type is TIMESTAMP_LTZ(3). NOW() now() Returns the current SQL timestamp in the local time zone, this is a synonym of CURRENT_TIMESTAMP. DATE_FORMAT(timestamp, string) dateFormat(timestamp, string) Converts timestamp to a value of string in the format specified by the date format string. The format string is compatible with Java&rsquo;s SimpleDateFormat. TIMESTAMPDIFF(timepointunit, timepoint1, timepoint2) timestampDiff(timepointunit, timepoint1, timepoint2) Returns the (signed) number of timepointunit between timepoint1 and timepoint2. The unit for the interval is given by the first argument, which should be one of the following values: SECOND, MINUTE, HOUR, DAY, MONTH, or YEAR. TO_DATE(string1[, string2]) toDate(string1[, string2]) Converts a date string string1 with format string2 (by default &lsquo;yyyy-MM-dd&rsquo;) to a date. TO_TIMESTAMP(string1[, string2]) toTimestamp(string1[, string2]) Converts date time string string1 with format string2 (by default: &lsquo;yyyy-MM-dd HH:mm:ss&rsquo;) to a timestamp, without time zone. Conditional Functions # Function Janino Code Description CASE value WHEN value1_1 [, value1_2]* THEN RESULT1 (WHEN value2_1 [, value2_2 ]* THEN result_2)* (ELSE result_z) END Nested ternary expression Returns resultX when the first time value is contained in (valueX_1, valueX_2, …). When no value matches, returns result_z if it is provided and returns NULL otherwise. CASE WHEN condition1 THEN result1 (WHEN condition2 THEN result2)* (ELSE result_z) END Nested ternary expression Returns resultX when the first conditionX is met. When no condition is met, returns result_z if it is provided and returns NULL otherwise. COALESCE(value1 [, value2]*) coalesce(Object&hellip; objects) Returns the first argument that is not NULL.If all arguments are NULL, it returns NULL as well. The return type is the least restrictive, common type of all of its arguments. The return type is nullable if all arguments are nullable as well. IF(condition, true_value, false_value) condition ? true_value : false_value Returns the true_value if condition is met, otherwise false_value. E.g., IF(5 &gt; 3, 5, 3) returns 5. Example # Add computed columns # Evaluation expressions can be used to generate new columns. For example, if we want to append two computed columns based on the table web_order in the database mydb, we may define a transform rule as follows:
transform: - source-table: mydb.web_order projection: id, order_id, UPPER(product_name) as product_name, localtimestamp as new_timestamp description: append calculated columns based on source table Reference metadata columns # We may reference metadata column in projection expressions. For example, given a table web_order in the database mydb, we may define a transform rule as follows:
transform: - source-table: mydb.web_order projection: id, order_id, __namespace_name__ || &#39;.&#39; || __schema_name__ || &#39;.&#39; || __table_name__ identifier_name description: access metadata columns from source table Use wildcard character to project all fields # A wildcard character (*) can be used to reference all fields in a table. For example, given two tables web_order and app_order in the database mydb, we may define a transform rule as follows:
transform: - source-table: mydb.web_order projection: \\*, UPPER(product_name) as product_name description: project fields with wildcard character from source table - source-table: mydb.app_order projection: UPPER(product_name) as product_name, * description: project fields with wildcard character from source table Notice: When * character presents at the beginning of expressions, an escaping backslash is required.
Add filter rule # Use reference columns when adding filtering rules to the table web_order in the database mydb, we may define a transform rule as follows:
transform: - source-table: mydb.web_order filter: id &gt; 10 AND order_id &gt; 100 description: filtering rows from source table Computed columns can be used in filtering conditions, too. For example, given a table web_order in the database mydb, we may define a transform rule as follows:
transform: - source-table: mydb.web_order projection: id, order_id, UPPER(province) as new_province filter: new_province = &#39;SHANGHAI&#39; description: filtering rows based on computed columns Reassign primary key # We can reassign the primary key in transform rules. For example, given a table web_order in the database mydb, we may define a transform rule as follows:
transform: - source-table: mydb.web_order projection: id, order_id primary-keys: order_id description: reassign primary key example Composite primary keys are also supported:
transform: - source-table: mydb.web_order projection: id, order_id, UPPER(product_name) as product_name primary-keys: order_id, product_name description: reassign composite primary keys example Reassign partition key # We can reassign the partition key in transform rules. For example, given a table web_order in the database mydb, we may define a transform rule as follows:
transform: - source-table: mydb.web_order projection: id, order_id, UPPER(product_name) as product_name partition-keys: product_name description: reassign partition key example Specify table creation configuration # Extra options can be defined in a transform rule, and will be applied when creating downstream tables. Given a table web_order in the database mydb, we may define a transform rule as follows:
transform: - source-table: mydb.web_order projection: id, order_id, UPPER(product_name) as product_name table-options: comment=web order description: auto creating table options example Tips: The format of table-options is key1=value1,key2=value2.
Classification mapping # Multiple transform rules can be defined to classify input data rows and apply different processings. For example, we may define a transform rule as follows:
transform: - source-table: mydb.web_order projection: id, order_id filter: UPPER(province) = &#39;SHANGHAI&#39; description: classification mapping example - source-table: mydb.web_order projection: order_id as id, id as order_id filter: UPPER(province) = &#39;BEIJING&#39; description: classification mapping example Known limitations # Currently, transform doesn&rsquo;t work with route rules. It will be supported in future versions. Computed columns cannot reference trimmed columns that do not present in final projection results. This will be fixed in future versions. Regular matching of tables with different schemas is not supported. If necessary, multiple rules need to be written. `}),e.add({id:37,href:"/flink/flink-cdc-docs-master/zh/docs/developer-guide/",title:"开发者指南",section:"Docs",content:" "}),e.add({id:38,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/mongodb-cdc/",title:"MongoDB",section:"Flink Source 连接器",content:` MongoDB CDC 连接器 # MongoDB CDC 连接器允许从 MongoDB 读取快照数据和增量数据。 本文档描述了如何设置 MongoDB CDC 连接器以针对 MongoDB 运行 SQL 查询。
依赖 # 为了设置 MongoDB CDC 连接器, 下表提供了使用构建自动化工具（如 Maven 或 SBT ）和带有 SQLJar 捆绑包的 SQLClient 的两个项目的依赖关系信息。
Maven dependency # &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-mongodb-cdc&lt;/artifactId&gt; &lt;!-- 请使用已发布的版本依赖，snapshot 版本的依赖需要本地自行编译。 --&gt; &lt;version&gt;3.2-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; SQL Client JAR # 下载链接仅适用于稳定版本。
下载 flink-sql-connector-mongodb-cdc-3.0.1.jar， 把它放在 &lt;FLINK_HOME&gt;/lib/.
注意: 参考 flink-sql-connector-mongodb-cdc， 当前已发布的版本将在 Maven 中央仓库中提供。
设置 MongoDB # 可用性 # MongoDB 版本
MongoDB 版本 &gt;= 3.6 我们使用 更改流 功能（3.6 版中新增），以捕获更改数据。
集群部署
副本集 或者 分片集群 是必需的。
存储引擎
WiredTiger 存储引擎是必需的。
副本集协议版本
副本集协议版本 1 (pv1) 是必需的。 从 4.0 版本开始，MongoDB 只支持pv1。 pv1 是使用 MongoDB 3.2 或更高版本创建的所有新副本集的默认值。
权限
changeStream and read 是 MongoDB Kafka Connector 必需权限。
你可以使用以下示例进行简单的授权。
有关更详细的授权, 请参照 MongoDB 数据库用户角色.
use admin; db.createRole( { role: &#34;flinkrole&#34;, privileges: [{ // 所有数据库中所有非系统集合的 grant 权限 resource: { db: &#34;&#34;, collection: &#34;&#34; }, actions: [ &#34;splitVector&#34;, &#34;listDatabases&#34;, &#34;listCollections&#34;, &#34;collStats&#34;, &#34;find&#34;, &#34;changeStream&#34; ] }], roles: [ // 阅读 config.collections 和 config.chunks // 用于分片集群快照拆分。 { role: &#39;read&#39;, db: &#39;config&#39; } ] } ); db.createUser( { user: &#39;flinkuser&#39;, pwd: &#39;flinkpw&#39;, roles: [ { role: &#39;flinkrole&#39;, db: &#39;admin&#39; } ] } ); 如何创建 MongoDB CDC 表 # MongoDB CDC 表可以定义如下：
-- 在 Flink SQL 中注册 MongoDB 表 \`products\` CREATE TABLE products ( _id STRING, // 必须声明 name STRING, weight DECIMAL(10,3), tags ARRAY&lt;STRING&gt;, -- array price ROW&lt;amount DECIMAL(10,2), currency STRING&gt;, -- 嵌入式文档 suppliers ARRAY&lt;ROW&lt;name STRING, address STRING&gt;&gt;, -- 嵌入式文档 PRIMARY KEY(_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mongodb-cdc&#39;, &#39;hosts&#39; = &#39;localhost:27017,localhost:27018,localhost:27019&#39;, &#39;username&#39; = &#39;flinkuser&#39;, &#39;password&#39; = &#39;flinkpw&#39;, &#39;database&#39; = &#39;inventory&#39;, &#39;collection&#39; = &#39;products&#39; ); -- 从 \`products\` 集合中读取快照和更改事件 SELECT * FROM products; 请注意
MongoDB 的更改事件记录在消息之前没有更新。因此，我们只能将其转换为 Flink 的 UPSERT 更改日志流。 upstart 流需要一个唯一的密钥，所以我们必须声明 _id 作为主键。 我们不能将其他列声明为主键, 因为删除操作不包含除 _id 和 sharding key 之外的键和值。
连接器选项 # Option Required Default Type Description connector required (none) String 指定要使用的连接器，此处应为 mongodb-cdc. scheme optional mongodb String 指定 MongoDB 连接协议。 eg. mongodb or mongodb+srv. hosts required (none) String MongoDB 服务器的主机名和端口对的逗号分隔列表。
eg. localhost:27017,localhost:27018 username optional (none) String 连接到 MongoDB 时要使用的数据库用户的名称。
只有当 MongoDB 配置为使用身份验证时，才需要这样做。 password optional (none) String 连接到 MongoDB 时要使用的密码。
只有当 MongoDB 配置为使用身份验证时，才需要这样做。 database optional (none) String 要监视更改的数据库的名称。 如果未设置，则将捕获所有数据库。 该数据库还支持正则表达式来监视与正则表达式匹配的多个数据库。 collection optional (none) String 数据库中要监视更改的集合的名称。 如果未设置，则将捕获所有集合。
该集合还支持正则表达式来监视与完全限定的集合标识符匹配的多个集合。 connection.options optional (none) String MongoDB连接选项。 例如: replicaSet=test&connectTimeoutMS=300000 scan.startup.mode optional initial String MongoDB CDC 消费者可选的启动模式， 合法的模式为 "initial"，"latest-offset" 和 "timestamp"。 请查阅 启动模式 章节了解更多详细信息。 scan.startup.timestamp-millis optional (none) Long 起始毫秒数, 仅适用于 'timestamp' 启动模式. batch.size optional 1024 Integer Cursor 批次大小。 poll.max.batch.size optional 1024 Integer 轮询新数据时，单个批处理中要包含的更改流文档的最大数量。 poll.await.time.ms optional 1000 Integer 在更改流上检查新结果之前等待的时间。 heartbeat.interval.ms optional 0 Integer 心跳间隔（毫秒）。使用 0 禁用。 scan.incremental.snapshot.enabled optional false Boolean 是否启用增量快照。增量快照功能仅支持 MongoDB 4.0 之后的版本。 scan.incremental.snapshot.chunk.size.mb optional 64 Integer 增量快照的区块大小 mb。 scan.incremental.close-idle-reader.enabled optional false Boolean 是否在快照结束后关闭空闲的 Reader。 此特性需要 flink 版本大于等于 1.14 并且 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' 需要设置为 true。 注意: heartbeat.interval.ms 强烈建议设置一个大于 0 的适当值 如果集合更改缓慢. 当我们从检查点或保存点恢复 Flink 作业时，心跳事件可以向前推送 resumeToken，以避免 resumeToken 过期。
可用元数据 # 以下格式元数据可以在表定义中公开为只读（VIRTUAL）列。
Key DataType Description database_name STRING NOT NULL 包含该行的数据库的名称。 collection_name STRING NOT NULL 包含该行的集合的名称。 op_ts TIMESTAMP_LTZ(3) NOT NULL 它指示在数据库中进行更改的时间。 如果记录是从表的快照而不是改变流中读取的，该值将始终为0。 扩展的 CREATE TABLE 示例演示了用于公开这些元数据字段的语法：
CREATE TABLE products ( db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, collection_name STRING METADATA FROM &#39;collection_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, _id STRING, // 必须声明 name STRING, weight DECIMAL(10,3), tags ARRAY&lt;STRING&gt;, -- array price ROW&lt;amount DECIMAL(10,2), currency STRING&gt;, -- 嵌入式文档 suppliers ARRAY&lt;ROW&lt;name STRING, address STRING&gt;&gt;, -- 嵌入式文档 PRIMARY KEY(_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mongodb-cdc&#39;, &#39;hosts&#39; = &#39;localhost:27017,localhost:27018,localhost:27019&#39;, &#39;username&#39; = &#39;flinkuser&#39;, &#39;password&#39; = &#39;flinkpw&#39;, &#39;database&#39; = &#39;inventory&#39;, &#39;collection&#39; = &#39;products&#39; ); 特性 # 精确一次处理 # MongoDB CDC 连接器是一个 Flink Source 连接器，它将首先读取数据库快照，然后在处理甚至失败时继续读取带有的更改流事件。
启动模式 # 配置选项scan.startup.mode指定 MySQL CDC 使用者的启动模式。有效枚举包括：
initial （默认）：在第一次启动时对受监视的数据库表执行初始快照，并继续读取最新的 oplog。 latest-offset：首次启动时，从不对受监视的数据库表执行快照， 连接器仅从 oplog 的结尾处开始读取，这意味着连接器只能读取在连接器启动之后的数据更改。 timestamp：跳过快照阶段，从指定的时间戳开始读取 oplog 事件。 例如使用 DataStream API:
MongoDBSource.builder() .startupOptions(StartupOptions.latest()) // Start from latest offset .startupOptions(StartupOptions.timestamp(1667232000000L) // Start from timestamp .build() and with SQL:
CREATE TABLE mongodb_source (...) WITH ( &#39;connector&#39; = &#39;mongodb-cdc&#39;, &#39;scan.startup.mode&#39; = &#39;latest-offset&#39;, -- 从最晚位点启动 ... &#39;scan.incremental.snapshot.enabled&#39; = &#39;true&#39;, -- 指定时间戳启动，需要开启增量快照读 &#39;scan.startup.mode&#39; = &#39;timestamp&#39;, -- 指定时间戳启动模式 &#39;scan.startup.timestamp-millis&#39; = &#39;1667232000000&#39; -- 启动毫秒时间 ... ) Notes:
&rsquo;timestamp&rsquo; 指定时间戳启动模式，需要开启增量快照读。 更改流 # 我们将 MongoDB&rsquo;s official Kafka Connector 从 MongoDB 中读取快照或更改事件，并通过 Debezium 的 EmbeddedEngine 进行驱动。
Debezium 的 EmbeddedEngine 提供了一种在应用程序进程中运行单个 Kafka Connect SourceConnector 的机制，并且它可以正确地驱动任何标准的 Kafka Connect SourceConnector，即使它不是由 Debezium 提供的。
我们选择 MongoDB 的官方 Kafka连接器，而不是 Debezium 的MongoDB 连接器，因为它们使用了不同的更改数据捕获机制。
对于 Debezium 的 MongoDB 连接器，它读取每个复制集主节点的 oplog.rs 集合。 对于 MongoDB 的 Kafka 连接器，它订阅了 MongoDB 的 更改流。 MongoDB 的oplog.rs 集合没有在状态之前保持更改记录的更新， 因此，很难通过单个 oplog.rs 记录提取完整的文档状态，并将其转换为 Flink 接受的更改日志流（Insert Only，Upsert，All）。 此外，MongoDB 5（2021 7月发布）改变了 oplog 格式，因此当前的 Debezium 连接器不能与其一起使用。
Change Stream是 MongoDB 3.6 为副本集和分片集群提供的一项新功能，它允许应用程序访问实时数据更改，而不会带来跟踪操作日志的复杂性和风险。
应用程序可以使用更改流来订阅单个集合上的所有数据更改， 数据库或整个部署，并立即对其做出反应。
查找更新操作的完整文档是变更流提供的一项功能，它可以配置变更流以返回更新文档的最新多数提交版本。由于该功能，我们可以轻松收集最新的完整文档，并将更改日志转换为 Flink 的Upsert Changelog Stream。
顺便说一句，DBZ-435提到的Debezium的MongoDB变更流探索,正在制定路线图。
如果完成了，我们可以考虑集成两种源连接器供用户选择。
DataStream Source # MongoDB CDC 连接器也可以是一个数据流源。 你可以创建 SourceFunction，如下所示：
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.cdc.connectors.mongodb.MongoDBSource; public class MongoDBSourceExample { public static void main(String[] args) throws Exception { SourceFunction&lt;String&gt; sourceFunction = MongoDBSource.&lt;String&gt;builder() .hosts(&#34;localhost:27017&#34;) .username(&#34;flink&#34;) .password(&#34;flinkpw&#34;) .databaseList(&#34;inventory&#34;) // 设置捕获的数据库，支持正则表达式 .collectionList(&#34;inventory.products&#34;, &#34;inventory.orders&#34;) //设置捕获的集合，支持正则表达式 .deserializer(new JsonDebeziumDeserializationSchema()) .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.addSource(sourceFunction) .print().setParallelism(1); // 对 sink 使用并行度 1 以保持消息顺序 env.execute(); } } MongoDB CDC 增量连接器（2.3.0 之后）可以使用，如下所示：
import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.cdc.connectors.mongodb.source.MongoDBSource; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; public class MongoDBIncrementalSourceExample { public static void main(String[] args) throws Exception { MongoDBSource&lt;String&gt; mongoSource = MongoDBSource.&lt;String&gt;builder() .hosts(&#34;localhost:27017&#34;) .databaseList(&#34;inventory&#34;) // 设置捕获的数据库，支持正则表达式 .collectionList(&#34;inventory.products&#34;, &#34;inventory.orders&#34;) //设置捕获的集合，支持正则表达式 .username(&#34;flink&#34;) .password(&#34;flinkpw&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 启用检查点 env.enableCheckpointing(3000); // 将 source 并行度设置为 2 env.fromSource(mongoSource, WatermarkStrategy.noWatermarks(), &#34;MongoDBIncrementalSource&#34;) .setParallelism(2) .print() .setParallelism(1); env.execute(&#34;Print MongoDB Snapshot + Change Stream&#34;); } } 注意:
如果使用数据库正则表达式，则需要 readAnyDatabase 角色。 增量快照功能仅支持 MongoDB 4.0 之后的版本。 数据类型映射 # BSON 二进制 JSON的缩写是一种类似 JSON 格式的二进制编码序列，用于在 MongoDB 中存储文档和进行远程过程调用。
Flink SQL Data Type 类似于 SQL 标准的数据类型术语，该术语描述了表生态系统中值的逻辑类型。它可以用于声明操作的输入和/或输出类型。
为了使 Flink SQL 能够处理来自异构数据源的数据，异构数据源的数据类型需要统一转换为 Flink SQL 数据类型。
以下是 BSON 类型和 Flink SQL 类型的映射。
BSON type Flink SQL type TINYINT SMALLINT Int
INT Long BIGINT FLOAT Double DOUBLE Decimal128 DECIMAL(p, s) Boolean BOOLEAN DateTimestamp DATE DateTimestamp TIME Date TIMESTAMP(3)TIMESTAMP_LTZ(3) Timestamp TIMESTAMP(0)TIMESTAMP_LTZ(0) String
ObjectId
UUID
Symbol
MD5
JavaScript Regex STRING BinData BYTES Object ROW Array ARRAY DBPointer ROW&lt;$ref STRING, $id STRING&gt; GeoJSON Point : ROW&lt;type STRING, coordinates ARRAY&lt;DOUBLE&gt;&gt; Line : ROW&lt;type STRING, coordinates ARRAY&lt;ARRAY&lt; DOUBLE&gt;&gt;&gt; ... 参考 # MongoDB Kafka Connector Change Streams Replication Sharding Database User Roles WiredTiger Replica set protocol Connection String Options BSON Types Flink DataTypes Back to top
`}),e.add({id:39,href:"/flink/flink-cdc-docs-master/zh/docs/core-concept/route/",title:"Route",section:"核心概念",content:` Definition # Route specifies the rule of matching a list of source-table and mapping to sink-table. The most typical scenario is the merge of sub-databases and sub-tables, routing multiple upstream source tables to the same sink table.
Parameters # To describe a route, the follows are required:
parameter meaning optional/required source-table Source table id, supports regular expressions required sink-table Sink table id, supports regular expressions required description Routing rule description(a default value provided) optional A route module can contain a list of source-table/sink-table rules.
Example # Route one Data Source table to one Data Sink table # if synchronize the table web_order in the database mydb to a Doris table ods_web_order, we can use this yaml file to define this route：
route: - source-table: mydb.web_order sink-table: mydb.ods_web_order description: sync table to one destination table with given prefix ods_ Route multiple Data Source tables to one Data Sink table # What&rsquo;s more, if you want to synchronize the sharding tables in the database mydb to a Doris table ods_web_order, we can use this yaml file to define this route：
route: - source-table: mydb\\.* sink-table: mydb.ods_web_order description: sync sharding tables to one destination table Complex Route via combining route rules # What&rsquo;s more, if you want to specify many different mapping rules, we can use this yaml file to define this route：
route: - source-table: mydb.orders sink-table: ods_db.ods_orders description: sync orders table to orders - source-table: mydb.shipments sink-table: ods_db.ods_shipments description: sync shipments table to ods_shipments - source-table: mydb.products sink-table: ods_db.ods_products description: sync products table to ods_products `}),e.add({id:40,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/sqlserver-tutorial/",title:"SqlServer 教程",section:"Flink CDC Sources 教程",content:` 演示: SqlServer CDC 导入 Elasticsearch # 创建 docker-compose.yml 文件，内容如下所示：
version: &#39;2.1&#39; services: sqlserver: image: mcr.microsoft.com/mssql/server:2019-latest container_name: sqlserver ports: - &#34;1433:1433&#34; environment: - &#34;MSSQL_AGENT_ENABLED=true&#34; - &#34;MSSQL_PID=Standard&#34; - &#34;ACCEPT_EULA=Y&#34; - &#34;SA_PASSWORD=Password!&#34; elasticsearch: image: elastic/elasticsearch:7.6.0 container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34; - discovery.type=single-node ports: - &#34;9200:9200&#34; - &#34;9300:9300&#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 container_name: kibana ports: - &#34;5601:5601&#34; volumes: - /var/run/docker.sock:/var/run/docker.sock 该 Docker Compose 中包含的容器有：
SqlServer：SqlServer 数据库。 Elasticsearch：orders 表将和 products 表进行 join，join 的结果写入 Elasticsearch 中。 Kibana：可视化 Elasticsearch 中的数据。 在 docker-compose.yml 所在目录下运行如下命令以启动所有容器：
docker-compose up -d 该命令会以 detached 模式自动启动 Docker Compose 配置中定义的所有容器。 你可以通过 docker ps 来观察上述的容器是否正常启动了。 也可以访问 http://localhost:5601/ 来查看 Kibana 是否运行正常。
另外可以通过如下命令停止并删除所有的容器：
docker-compose down 下载以下 jar 包到 &lt;FLINK_HOME&gt;/lib/：
下载链接只对已发布的版本有效, SNAPSHOT 版本需要本地编译
flink-sql-connector-elasticsearch7-3.0.1-1.17.jar flink-sql-connector-sqlserver-cdc-2.4.0.jar 在 SqlServer 数据库中准备数据
创建数据库和表 products，orders，并插入数据：
-- Sqlserver CREATE DATABASE inventory; GO USE inventory; EXEC sys.sp_cdc_enable_db; -- Create and populate our products using a single insert with many rows CREATE TABLE products ( id INTEGER IDENTITY(101,1) NOT NULL PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(512), weight FLOAT ); INSERT INTO products(name,description,weight) VALUES (&#39;scooter&#39;,&#39;Small 2-wheel scooter&#39;,3.14); INSERT INTO products(name,description,weight) VALUES (&#39;car battery&#39;,&#39;12V car battery&#39;,8.1); INSERT INTO products(name,description,weight) VALUES (&#39;12-pack drill bits&#39;,&#39;12-pack of drill bits with sizes ranging from #40 to #3&#39;,0.8); INSERT INTO products(name,description,weight) VALUES (&#39;hammer&#39;,&#39;12oz carpenter&#39;&#39;s hammer&#39;,0.75); INSERT INTO products(name,description,weight) VALUES (&#39;hammer&#39;,&#39;14oz carpenter&#39;&#39;s hammer&#39;,0.875); INSERT INTO products(name,description,weight) VALUES (&#39;hammer&#39;,&#39;16oz carpenter&#39;&#39;s hammer&#39;,1.0); INSERT INTO products(name,description,weight) VALUES (&#39;rocks&#39;,&#39;box of assorted rocks&#39;,5.3); INSERT INTO products(name,description,weight) VALUES (&#39;jacket&#39;,&#39;water resistent black wind breaker&#39;,0.1); INSERT INTO products(name,description,weight) VALUES (&#39;spare tire&#39;,&#39;24 inch spare tire&#39;,22.2); EXEC sys.sp_cdc_enable_table @source_schema = &#39;dbo&#39;, @source_name = &#39;products&#39;, @role_name = NULL, @supports_net_changes = 0; -- Create some very simple orders CREATE TABLE orders ( id INTEGER IDENTITY(10001,1) NOT NULL PRIMARY KEY, order_date DATE NOT NULL, purchaser INTEGER NOT NULL, quantity INTEGER NOT NULL, product_id INTEGER NOT NULL, FOREIGN KEY (product_id) REFERENCES products(id) ); INSERT INTO orders(order_date,purchaser,quantity,product_id) VALUES (&#39;16-JAN-2016&#39;, 1001, 1, 102); INSERT INTO orders(order_date,purchaser,quantity,product_id) VALUES (&#39;17-JAN-2016&#39;, 1002, 2, 105); INSERT INTO orders(order_date,purchaser,quantity,product_id) VALUES (&#39;19-FEB-2016&#39;, 1002, 2, 106); INSERT INTO orders(order_date,purchaser,quantity,product_id) VALUES (&#39;21-FEB-2016&#39;, 1003, 1, 107); EXEC sys.sp_cdc_enable_table @source_schema = &#39;dbo&#39;, @source_name = &#39;orders&#39;, @role_name = NULL, @supports_net_changes = 0; GO 然后启动 Flink 集群，再启动 SQL CLI：
-- Flink SQL -- checkpoint every 3000 milliseconds Flink SQL&gt; SET execution.checkpointing.interval = 3s; Flink SQL&gt; CREATE TABLE products ( id INT, name STRING, description STRING, PRIMARY KEY (id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;sqlserver-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;1433&#39;, &#39;username&#39; = &#39;sa&#39;, &#39;password&#39; = &#39;Password!&#39;, &#39;database-name&#39; = &#39;inventory&#39;, &#39;table-name&#39; = &#39;dbo.products&#39; ); Flink SQL&gt; CREATE TABLE orders ( id INT, order_date DATE, purchaser INT, quantity INT, product_id INT, PRIMARY KEY (id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;sqlserver-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;1433&#39;, &#39;username&#39; = &#39;sa&#39;, &#39;password&#39; = &#39;Password!&#39;, &#39;database-name&#39; = &#39;inventory&#39;, &#39;table-name&#39; = &#39;dbo.orders&#39; ); Flink SQL&gt; CREATE TABLE enriched_orders ( order_id INT, order_date DATE, purchaser INT, quantity INT, product_name STRING, product_description STRING, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;elasticsearch-7&#39;, &#39;hosts&#39; = &#39;http://localhost:9200&#39;, &#39;index&#39; = &#39;enriched_orders_1&#39; ); Flink SQL&gt; INSERT INTO enriched_orders SELECT o.id,o.order_date,o.purchaser,o.quantity, p.name, p.description FROM orders AS o LEFT JOIN products AS p ON o.product_id = p.id; 检查 ElasticSearch 中的结果
检查最终的结果是否写入 ElasticSearch 中，可以在 Kibana 看到 ElasticSearch 中的数据。
在 SqlServer 制造一些变更，观察 ElasticSearch 中的结果
通过如下的 SQL 语句对 SqlServer 数据库进行一些修改，然后就可以看到每执行一条 SQL 语句，Elasticsearch 中的数据都会实时更新。
INSERT INTO orders(order_date,purchaser,quantity,product_id) VALUES (&#39;22-FEB-2016&#39;, 1006, 22, 107); GO UPDATE orders SET quantity = 11 WHERE id = 10001; GO DELETE FROM orders WHERE id = 10004; GO Back to top
`}),e.add({id:41,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/starrocks/",title:"StarRocks",section:"Pipeline 连接器",content:` StarRocks Connector # StarRocks Pipeline 连接器可以用作 Pipeline 的 Data Sink，将数据写入StarRocks。 本文档介绍如何设置 StarRocks Pipeline 连接器。
连接器的功能 # 自动建表 表结构变更同步 数据实时同步 示例 # 从 MySQL 读取数据同步到 StarRocks 的 Pipeline 可以定义如下：
source: type: mysql name: MySQL Source hostname: 127.0.0.1 port: 3306 username: admin password: pass tables: adb.\\.*, bdb.user_table_[0-9]+, [app|web].order_\\.* server-id: 5401-5404 sink: type: starrocks name: StarRocks Sink jdbc-url: jdbc:mysql://127.0.0.1:9030 load-url: 127.0.0.1:8030 username: root password: pass pipeline: name: MySQL to StarRocks Pipeline parallelism: 2 连接器配置项 # Option Required Default Type Description type required (none) String 指定要使用的连接器, 这里需要设置成 'starrocks'. name optional (none) String Sink 的名称. jdbc-url required (none) String 用于访问 FE 节点上的 MySQL 服务器。多个地址用英文逗号（,）分隔。格式：\`jdbc:mysql://fe_host1:fe_query_port1,fe_host2:fe_query_port2\`。 load-url required (none) String 用于访问 FE 节点上的 HTTP 服务器。多个地址用英文分号（;）分隔。格式：\`fe_host1:fe_http_port1;fe_host2:fe_http_port2\`。 username required (none) String StarRocks 集群的用户名。 password required (none) String StarRocks 集群的用户密码。 sink.label-prefix optional (none) String 指定 Stream Load 使用的 label 前缀。 sink.connect.timeout-ms optional 30000 String 与 FE 建立 HTTP 连接的超时时间。取值范围：[100, 60000]。 sink.wait-for-continue.timeout-ms optional 30000 String 等待 FE HTTP 100-continue 应答的超时时间。取值范围：[3000, 60000]。 sink.buffer-flush.max-bytes optional 157286400 Long 内存中缓冲的数据量大小，缓冲区由所有导入的表共享，达到阈值后将选择一个或多个表的数据写入到StarRocks。 达到阈值后取值范围：[64MB, 10GB]。 sink.buffer-flush.interval-ms optional 300000 Long 每个表缓冲数据发送的间隔，用于控制数据写入 StarRocks 的延迟。单位是毫秒，取值范围：[1000, 3600000]。 sink.scan-frequency.ms optional 50 Long 连接器会定期检查每个表是否到达发送间隔，该配置控制检查频率，单位为毫秒。 sink.io.thread-count optional 2 Integer 用来执行 Stream Load 的线程数，不同表之间的导入可以并发执行。 sink.at-least-once.use-transaction-stream-load optional true Boolean at-least-once 下是否使用 transaction stream load。 sink.properties.* optional (none) String Stream Load 的参数，控制 Stream Load 导入行为。例如 参数 \`sink.properties.timeout\` 用来控制导入的超时时间。 全部参数和解释请参考 STREAM LOAD。 table.create.num-buckets optional (none) Integer 自动创建 StarRocks 表时使用的桶数。对于 StarRocks 2.5 及之后的版本可以不设置，StarRocks 将会 自动设置分桶数量；对于 StarRocks 2.5 之前的版本必须设置。 table.create.properties.* optional (none) String 自动创建 StarRocks 表时使用的属性。比如: 如果使用 StarRocks 3.2 及之后的版本，'table.create.properties.fast_schema_evolution' = 'true' 将会打开 fast schema evolution 功能。 更多信息请参考 主键模型。 table.schema-change.timeout optional 30min Duration StarRocks 侧执行 schema change 的超时时间，必须是秒的整数倍。超时后 StarRocks 将会取消 schema change，从而导致作业失败。 使用说明 # 只支持主键表，因此源表必须有主键
暂不支持 exactly-once，连接器 通过 at-least-once 和主键表实现幂等写
对于自动建表
分桶键和主键相同 没有分区键 分桶数由 table.create.num-buckets 控制。如果使用的 StarRocks 2.5 及之后的版本可以不设置，StarRocks 能够 自动设置分桶数量。对于 StarRocks 2.5 之前的版本必须设置，否则无法自动创建表。 对于表结构变更同步
只支持增删列 新增列只能添加到最后一列 如果使用 StarRocks 3.2 及之后版本，并且通过连接器来自动建表, 可以通过配置 table.create.properties.fast_schema_evolution 为 true 来加速 StarRocks 执行变更。 对于数据同步，pipeline 连接器使用 StarRocks Sink 连接器 将数据写入 StarRocks，具体可以参考 Sink 文档。
数据类型映射 # CDC type StarRocks type NOTE TINYINT TINYINT SMALLINT SMALLINT INT INT BIGINT BIGINT FLOAT FLOAT DOUBLE DOUBLE DECIMAL(p, s) DECIMAL(p, s) BOOLEAN BOOLEAN DATE DATE TIMESTAMP DATETIME TIMESTAMP_LTZ DATETIME CHAR(n) where n <= 85 CHAR(n * 3) CDC 中长度表示字符数，而 StarRocks 中长度表示字节数。根据 UTF-8 编码，一个中文字符占用三个字节，因此 CDC 中的长度对应到 StarRocks 中为 n * 3。由于 StarRocks CHAR 类型的最大长度为255，所以只有当 CDC 中长度不超过85时，才将 CDC CHAR 映射到 StarRocks CHAR。 CHAR(n) where n > 85 VARCHAR(n * 3) CDC 中长度表示字符数，而 StarRocks 中长度表示字节数。根据 UTF-8 编码，一个中文字符占用三个字节，因此 CDC 中的长度对应到 StarRocks 中为 n * 3。由于 StarRocks CHAR 类型的最大长度为255，所以当 CDC 中长度超过85时，才将 CDC CHAR 映射到 StarRocks VARCHAR。 VARCHAR(n) VARCHAR(n * 3) CDC 中长度表示字符数，而 StarRocks 中长度表示字节数。根据 UTF-8 编码，一个中文字符占用三个字节，因此 CDC 中的长度对应到 StarRocks 中为 n * 3。 Back to top
`}),e.add({id:42,href:"/flink/flink-cdc-docs-master/zh/docs/faq/",title:"常见问题",section:"Docs",content:" "}),e.add({id:43,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/db2-cdc/",title:"Db2",section:"Flink Source 连接器",content:` Db2 CDC Connector # The Db2 CDC connector allows for reading snapshot data and incremental data from Db2 database. This document describes how to setup the db2 CDC connector to run SQL queries against Db2 databases.
Supported Databases # Connector Database Driver Db2-cdc Db2: 11.5 Db2 Driver: 11.5.0.0 Dependencies # In order to set up the Db2 CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency # &ltdependency&gt &ltgroupId&gtorg.apache.flink&lt/groupId&gt &ltartifactId&gtflink-connector-db2-cdc&lt/artifactId&gt &ltversion&gt3.2-SNAPSHOT&lt/version&gt &lt/dependency&gt Copied to clipboard! SQL Client JAR # Download flink-sql-connector-db2-cdc-3.0.1.jar and put it under &lt;FLINK_HOME&gt;/lib/.
Note: Refer to flink-sql-connector-db2-cdc, more released versions will be available in the Maven central warehouse.
由于 Db2 Connector 采用的 IPLA 协议与 Flink CDC 项目不兼容，我们无法在 jar 包中提供 Db2 连接器。 您可能需要手动配置以下依赖：
依赖名称 说明 com.ibm.db2.jcc:db2jcc:db2jcc4 用于连接到 Db2 数据库。 Setup Db2 server # Follow the steps in the Debezium Db2 Connector.
Notes # Not support BOOLEAN type in SQL Replication on Db2 # Only snapshots can be taken from tables with BOOLEAN type columns. Currently, SQL Replication on Db2 does not support BOOLEAN, so Debezium can not perform CDC on those tables. Consider using another type to replace BOOLEAN type.
How to create a Db2 CDC table # The Db2 CDC table can be defined as following:
-- checkpoint every 3 seconds Flink SQL&gt; SET &#39;execution.checkpointing.interval&#39; = &#39;3s&#39;; -- register a Db2 table &#39;products&#39; in Flink SQL Flink SQL&gt; CREATE TABLE products ( ID INT NOT NULL, NAME STRING, DESCRIPTION STRING, WEIGHT DECIMAL(10,3), PRIMARY KEY(ID) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;db2-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;50000&#39;, &#39;username&#39; = &#39;root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;myschema.products&#39;); -- read snapshot and redo logs from products table Flink SQL&gt; SELECT * FROM products; Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'db2-cdc'. hostname required (none) String IP address or hostname of the Db2 database server. username required (none) String Name of the Db2 database to use when connecting to the Db2 database server. password required (none) String Password to use when connecting to the Db2 database server. database-name required (none) String Database name of the Db2 server to monitor. table-name required (none) String Table name of the Db2 database to monitor, e.g.: "db1.table1" port optional 50000 Integer Integer port number of the Db2 database server. scan.startup.mode optional initial String Optional startup mode for Db2 CDC consumer, valid enumerations are "initial" and "latest-offset". Please see Startup Reading Position section for more detailed information. server-time-zone optional (none) String The session time zone in database server, e.g. "Asia/Shanghai". It controls how the TIMESTAMP type in Db2 converted to STRING. See more here. If not set, then ZoneId.systemDefault() is used to determine the server time zone. scan.incremental.snapshot.enabled optional true Boolean Whether enable parallelism snapshot. chunk-meta.group.size optional 1000 Integer The group size of chunk meta, if the meta size exceeds the group size, the meta will be divided into multiple groups. chunk-key.even-distribution.factor.lower-bound optional 0.05d Double The lower bound of chunk key distribution factor. The distribution factor is used to determine whether the table is evenly distribution or not. The table chunks would use evenly calculation optimization when the data distribution is even, and the query for splitting would happen when it is uneven. The distribution factor could be calculated by (MAX(id) - MIN(id) + 1) / rowCount. chunk-key.even-distribution.factor.upper-bound optional 1000.0d Double The upper bound of chunk key distribution factor. The distribution factor is used to determine whether the table is evenly distribution or not. The table chunks would use evenly calculation optimization when the data distribution is even, and the query for splitting would happen when it is uneven. The distribution factor could be calculated by (MAX(id) - MIN(id) + 1) / rowCount. scan.incremental.snapshot.chunk.key-column optional (none) String The chunk key of table snapshot, captured tables are split into multiple chunks by a chunk key when read the snapshot of table. By default, the chunk key is the first column of the primary key. This column must be a column of the primary key. debezium.* optional (none) String Pass-through Debezium's properties to Debezium Embedded Engine which is used to capture data changes from Db2 server. For example: 'debezium.snapshot.mode' = 'never'. See more about the Debezium's Db2 Connector properties scan.incremental.close-idle-reader.enabled optional false Boolean Whether to close idle readers at the end of the snapshot phase. The flink version is required to be greater than or equal to 1.14 when 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' is set to true.
If the flink version is greater than or equal to 1.15, the default value of 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' has been changed to true, so it does not need to be explicitly configured 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' = 'true' Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Key DataType Description table_name STRING NOT NULL Name of the table that contain the row. schema_name STRING NOT NULL Name of the schema that contain the row. database_name STRING NOT NULL Name of the database that contain the row. op_ts TIMESTAMP_LTZ(3) NOT NULL It indicates the time that the change was made in the database. If the record is read from snapshot of the table instead of the change stream, the value is always 0. Features # Startup Reading Position # The config option scan.startup.mode specifies the startup mode for DB2 CDC consumer. The valid enumerations are:
initial (default): Performs an initial snapshot on the monitored database tables upon first startup, and continue to read the latest redo logs. latest-offset: Never to perform snapshot on the monitored database tables upon first startup, just read from the end of the redo logs which means only have the changes since the connector was started. Note: the mechanism of scan.startup.mode option relying on Debezium&rsquo;s snapshot.mode configuration. So please do not using them together. If you speicifying both scan.startup.mode and debezium.snapshot.mode options in the table DDL, it may make scan.startup.mode doesn&rsquo;t work.
DataStream Source # import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; public class Db2SourceExample { public static void main(String[] args) throws Exception { SourceFunction&lt;String&gt; db2Source = Db2Source.&lt;String&gt;builder() .hostname(&#34;yourHostname&#34;) .port(50000) .database(&#34;yourDatabaseName&#34;) // set captured database .tableList(&#34;yourSchemaName.yourTableName&#34;) // set captured table .username(&#34;yourUsername&#34;) .password(&#34;yourPassword&#34;) .deserializer( new JsonDebeziumDeserializationSchema()) // converts SourceRecord to // JSON String .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000); env.addSource(db2Source) .print() .setParallelism(1); // use parallelism 1 for sink to keep message ordering env.execute(&#34;Print Db2 Snapshot + Change Stream&#34;); } } The DB2 CDC incremental connector (after 3.1.0) can be used as the following shows:
import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.cdc.connectors.base.options.StartupOptions; import org.apache.flink.cdc.connectors.db2.source.Db2SourceBuilder; import org.apache.flink.cdc.connectors.db2.source.Db2SourceBuilder.Db2IncrementalSource; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; public class Db2ParallelSourceExample { public static void main(String[] args) throws Exception { Db2IncrementalSource&lt;String&gt; sqlServerSource = new Db2SourceBuilder() .hostname(&#34;localhost&#34;) .port(50000) .databaseList(&#34;TESTDB&#34;) .tableList(&#34;DB2INST1.CUSTOMERS&#34;) .username(&#34;flink&#34;) .password(&#34;flinkpw&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) .startupOptions(StartupOptions.initial()) .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000); // set the source parallelism to 2 env.fromSource(sqlServerSource, WatermarkStrategy.noWatermarks(), &#34;Db2IncrementalSource&#34;) .setParallelism(2) .print() .setParallelism(1); env.execute(&#34;Print DB2 Snapshot + Change Stream&#34;); } } Data Type Mapping # Db2 type Flink SQL type NOTE SMALLINT
SMALLINT INTEGER INT BIGINT BIGINT REAL FLOAT DOUBLE DOUBLE NUMERIC(p, s)
DECIMAL(p, s) DECIMAL(p, s) DATE DATE TIME TIME TIMESTAMP [(p)] TIMESTAMP [(p)] CHARACTER(n) CHAR(n) VARCHAR(n) VARCHAR(n) BINARY(n) BINARY(n) VARBINARY(N) VARBINARY(N) BLOB
CLOB
DBCLOB
BYTES VARGRAPHIC
XML STRING Back to top
`}),e.add({id:44,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/tidb-tutorial/",title:"TiDB 教程",section:"Flink CDC Sources 教程",content:` 演示: TiDB CDC 导入 Elasticsearch # 首先我们得通过 docker 来启动 TiDB 集群。
$ git clone https://github.com/pingcap/tidb-docker-compose.git 其次替换目录 tidb-docker-compose 里面的 docker-compose.yml 文件，内容如下所示：
version: &#34;2.1&#34; services: pd: image: pingcap/pd:v5.3.1 ports: - &#34;2379:2379&#34; volumes: - ./config/pd.toml:/pd.toml - ./logs:/logs command: - --client-urls=http://0.0.0.0:2379 - --peer-urls=http://0.0.0.0:2380 - --advertise-client-urls=http://pd:2379 - --advertise-peer-urls=http://pd:2380 - --initial-cluster=pd=http://pd:2380 - --data-dir=/data/pd - --config=/pd.toml - --log-file=/logs/pd.log restart: on-failure tikv: image: pingcap/tikv:v5.3.1 ports: - &#34;20160:20160&#34; volumes: - ./config/tikv.toml:/tikv.toml - ./logs:/logs command: - --addr=0.0.0.0:20160 - --advertise-addr=tikv:20160 - --data-dir=/data/tikv - --pd=pd:2379 - --config=/tikv.toml - --log-file=/logs/tikv.log depends_on: - &#34;pd&#34; restart: on-failure tidb: image: pingcap/tidb:v5.3.1 ports: - &#34;4000:4000&#34; volumes: - ./config/tidb.toml:/tidb.toml - ./logs:/logs command: - --store=tikv - --path=pd:2379 - --config=/tidb.toml - --log-file=/logs/tidb.log - --advertise-address=tidb depends_on: - &#34;tikv&#34; restart: on-failure elasticsearch: image: elastic/elasticsearch:7.6.0 container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34; - discovery.type=single-node ports: - &#34;9200:9200&#34; - &#34;9300:9300&#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 container_name: kibana ports: - &#34;5601:5601&#34; volumes: - /var/run/docker.sock:/var/run/docker.sock 该 Docker Compose 中包含的容器有：
TiDB 集群: tikv、pd、tidb。 Elasticsearch：orders 表将和 products 表进行 join，join 的结果写入 Elasticsearch 中。 Kibana：可视化 Elasticsearch 中的数据。 本机添加 host 映射 pd 和 tikv 映射 127.0.0.1。 在 docker-compose.yml 所在目录下运行如下命令以启动所有容器：
docker-compose up -d mysql -h 127.0.0.1 -P 4000 -u root # Just test tidb cluster is ready,if you have install mysql local. 该命令会以 detached 模式自动启动 Docker Compose 配置中定义的所有容器。 你可以通过 docker ps 来观察上述的容器是否正常启动了。 也可以访问 http://localhost:5601/ 来查看 Kibana 是否运行正常。
另外可以通过如下命令停止并删除所有的容器：
docker-compose down 下载以下 jar 包到 &lt;FLINK_HOME&gt;/lib/：
下载链接只对已发布的版本有效, SNAPSHOT 版本需要本地编译
flink-sql-connector-elasticsearch7-3.0.1-1.17.jar flink-sql-connector-tidb-cdc-2.4.0.jar 在 TiDB 数据库中准备数据
创建数据库和表 products，orders，并插入数据：
-- TiDB CREATE DATABASE mydb; USE mydb; CREATE TABLE products ( id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(512) ) AUTO_INCREMENT = 101; INSERT INTO products VALUES (default,&#34;scooter&#34;,&#34;Small 2-wheel scooter&#34;), (default,&#34;car battery&#34;,&#34;12V car battery&#34;), (default,&#34;12-pack drill bits&#34;,&#34;12-pack of drill bits with sizes ranging from #40 to #3&#34;), (default,&#34;hammer&#34;,&#34;12oz carpenter&#39;s hammer&#34;), (default,&#34;hammer&#34;,&#34;14oz carpenter&#39;s hammer&#34;), (default,&#34;hammer&#34;,&#34;16oz carpenter&#39;s hammer&#34;), (default,&#34;rocks&#34;,&#34;box of assorted rocks&#34;), (default,&#34;jacket&#34;,&#34;water resistent black wind breaker&#34;), (default,&#34;spare tire&#34;,&#34;24 inch spare tire&#34;); CREATE TABLE orders ( order_id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, order_date DATETIME NOT NULL, customer_name VARCHAR(255) NOT NULL, price DECIMAL(10, 5) NOT NULL, product_id INTEGER NOT NULL, order_status BOOLEAN NOT NULL -- Whether order has been placed ) AUTO_INCREMENT = 10001; INSERT INTO orders VALUES (default, &#39;2020-07-30 10:08:22&#39;, &#39;Jark&#39;, 50.50, 102, false), (default, &#39;2020-07-30 10:11:09&#39;, &#39;Sally&#39;, 15.00, 105, false), (default, &#39;2020-07-30 12:00:30&#39;, &#39;Edward&#39;, 25.25, 106, false); 然后启动 Flink 集群，再启动 SQL CLI：
-- Flink SQL -- checkpoint every 3000 milliseconds Flink SQL&gt; SET execution.checkpointing.interval = 3s; Flink SQL&gt; CREATE TABLE products ( id INT, name STRING, description STRING, PRIMARY KEY (id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;tidb-cdc&#39;, &#39;tikv.grpc.timeout_in_ms&#39; = &#39;20000&#39;, &#39;pd-addresses&#39; = &#39;127.0.0.1:2379&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;products&#39; ); Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(3), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;tidb-cdc&#39;, &#39;tikv.grpc.timeout_in_ms&#39; = &#39;20000&#39;, &#39;pd-addresses&#39; = &#39;127.0.0.1:2379&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;orders&#39; ); Flink SQL&gt; CREATE TABLE enriched_orders ( order_id INT, order_date DATE, customer_name STRING, order_status BOOLEAN, product_name STRING, product_description STRING, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;elasticsearch-7&#39;, &#39;hosts&#39; = &#39;http://localhost:9200&#39;, &#39;index&#39; = &#39;enriched_orders_1&#39; ); Flink SQL&gt; INSERT INTO enriched_orders SELECT o.order_id, o.order_date, o.customer_name, o.order_status, p.name, p.description FROM orders AS o LEFT JOIN products AS p ON o.product_id = p.id; 检查 ElasticSearch 中的结果
检查最终的结果是否写入 ElasticSearch 中，可以在 Kibana 看到 ElasticSearch 中的数据。
在 TiDB 制造一些变更，观察 ElasticSearch 中的结果
通过如下的 SQL 语句对 TiDB 数据库进行一些修改，然后就可以看到每执行一条 SQL 语句，Elasticsearch 中的数据都会实时更新。
INSERT INTO orders VALUES (default, &#39;2020-07-30 15:22:00&#39;, &#39;Jark&#39;, 29.71, 104, false); UPDATE orders SET order_status = true WHERE order_id = 10004; DELETE FROM orders WHERE order_id = 10004; Back to top
`}),e.add({id:45,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tidb-cdc/",title:"TiDB",section:"Flink Source 连接器",content:` TiDB CDC Connector # The TiDB CDC connector allows for reading snapshot data and incremental data from TiDB database. This document describes how to setup the TiDB CDC connector to run SQL queries against TiDB databases.
Dependencies # In order to setup the TiDB CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency # &ltdependency&gt &ltgroupId&gtorg.apache.flink&lt/groupId&gt &ltartifactId&gtflink-connector-tidb-cdc&lt/artifactId&gt &ltversion&gt3.2-SNAPSHOT&lt/version&gt &lt/dependency&gt Copied to clipboard! SQL Client JAR # Download link is available only for stable releases.
Download flink-sql-connector-tidb-cdc-3.0.1.jar and put it under &lt;FLINK_HOME&gt;/lib/.
Note: Refer to flink-sql-connector-tidb-cdc, more released versions will be available in the Maven central warehouse.
How to create a TiDB CDC table # The TiDB CDC table can be defined as following:
-- checkpoint every 3000 milliseconds Flink SQL&gt; SET &#39;execution.checkpointing.interval&#39; = &#39;3s&#39;; -- register a TiDB table &#39;orders&#39; in Flink SQL Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(3), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY(order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;tidb-cdc&#39;, &#39;tikv.grpc.timeout_in_ms&#39; = &#39;20000&#39;, &#39;pd-addresses&#39; = &#39;localhost:2379&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;orders&#39; ); -- read snapshot and binlogs from orders table Flink SQL&gt; SELECT * FROM orders; Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'tidb-cdc'. database-name required (none) String Database name of the TiDB server to monitor. table-name required (none) String Table name of the TiDB database to monitor. scan.startup.mode optional initial String Optional startup mode for TiDB CDC consumer, valid enumerations are "initial" and "latest-offset". pd-addresses required (none) String TiKV cluster's PD address. host-mapping optional (none) String TiKV cluster's host-mapping used to configure public IP and intranet IP mapping. When the TiKV cluster is running on the intranet, you can map a set of intranet IPs to public IPs for an outside Flink cluster to access. The format is {Intranet IP1}:{Public IP1};{Intranet IP2}:{Public IP2}, e.g. 192.168.0.2:8.8.8.8;192.168.0.3:9.9.9.9. tikv.grpc.timeout_in_ms optional (none) Long TiKV GRPC timeout in ms. tikv.grpc.scan_timeout_in_ms optional (none) Long TiKV GRPC scan timeout in ms. tikv.batch_get_concurrency optional 20 Integer TiKV GRPC batch get concurrency. tikv.* optional (none) String Pass-through TiDB client's properties. Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Key DataType Description table_name STRING NOT NULL Name of the table that contain the row. database_name STRING NOT NULL Name of the database that contain the row. op_ts TIMESTAMP_LTZ(3) NOT NULL It indicates the time that the change was made in the database. If the record is read from snapshot of the table instead of the binlog, the value is always 0. The extended CREATE TABLE example demonstrates the syntax for exposing these metadata fields:
CREATE TABLE products ( db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, table_name STRING METADATA FROM &#39;table_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY(order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;tidb-cdc&#39;, &#39;tikv.grpc.timeout_in_ms&#39; = &#39;20000&#39;, &#39;pd-addresses&#39; = &#39;localhost:2379&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;orders&#39; ); Features # Exactly-Once Processing # The TiDB CDC connector is a Flink Source connector which will read database snapshot first and then continues to read change events with exactly-once processing even failures happen.
Startup Reading Position # The config option scan.startup.mode specifies the startup mode for TiDB CDC consumer. The valid enumerations are:
initial (default): Takes a snapshot of structure and data of captured tables; useful if you want fetch a complete representation of the data from the captured tables. latest-offset: Takes a snapshot of the structure of captured tables only; useful if only changes happening from now onwards should be fetched. Multi Thread Reading # The TiDB CDC source can work in parallel reading, because there is multiple tasks can receive change events.
DataStream Source # The TiDB CDC connector can also be a DataStream source. You can create a SourceFunction as the following shows:
DataStream Source # import org.apache.flink.api.common.typeinfo.BasicTypeInfo; import org.apache.flink.api.common.typeinfo.TypeInformation; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.util.Collector; import org.apache.flink.cdc.connectors.tidb.TDBSourceOptions; import org.apache.flink.cdc.connectors.tidb.TiDBSource; import org.apache.flink.cdc.connectors.tidb.TiKVChangeEventDeserializationSchema; import org.apache.flink.cdc.connectors.tidb.TiKVSnapshotEventDeserializationSchema; import org.tikv.kvproto.Cdcpb; import org.tikv.kvproto.Kvrpcpb; import java.util.HashMap; public class TiDBSourceExample { public static void main(String[] args) throws Exception { SourceFunction&lt;String&gt; tidbSource = TiDBSource.&lt;String&gt;builder() .database(&#34;mydb&#34;) // set captured database .tableName(&#34;products&#34;) // set captured table .tiConf( TDBSourceOptions.getTiConfiguration( &#34;localhost:2399&#34;, new HashMap&lt;&gt;())) .snapshotEventDeserializer( new TiKVSnapshotEventDeserializationSchema&lt;String&gt;() { @Override public void deserialize( Kvrpcpb.KvPair record, Collector&lt;String&gt; out) throws Exception { out.collect(record.toString()); } @Override public TypeInformation&lt;String&gt; getProducedType() { return BasicTypeInfo.STRING_TYPE_INFO; } }) .changeEventDeserializer( new TiKVChangeEventDeserializationSchema&lt;String&gt;() { @Override public void deserialize( Cdcpb.Event.Row record, Collector&lt;String&gt; out) throws Exception { out.collect(record.toString()); } @Override public TypeInformation&lt;String&gt; getProducedType() { return BasicTypeInfo.STRING_TYPE_INFO; } }) .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000); env.addSource(tidbSource).print().setParallelism(1); env.execute(&#34;Print TiDB Snapshot + Binlog&#34;); } } Data Type Mapping # TiDB type Flink SQL type NOTE TINYINT TINYINT SMALLINT
TINYINT UNSIGNED SMALLINT INT
MEDIUMINT
SMALLINT UNSIGNED INT BIGINT
INT UNSIGNED BIGINT BIGINT UNSIGNED DECIMAL(20, 0) FLOAT
FLOAT REAL
DOUBLE DOUBLE NUMERIC(p, s)
DECIMAL(p, s)
where p <= 38
DECIMAL(p, s) NUMERIC(p, s)
DECIMAL(p, s)
where 38 < p <= 65
STRING The precision for DECIMAL data type is up to 65 in TiDB, but the precision for DECIMAL is limited to 38 in Flink. So if you define a decimal column whose precision is greater than 38, you should map it to STRING to avoid precision loss. BOOLEAN
TINYINT(1)
BIT(1) BOOLEAN DATE DATE TIME [(p)] TIME [(p)] TIMESTAMP [(p)] TIMESTAMP_LTZ [(p)] DATETIME [(p)] TIMESTAMP [(p)] CHAR(n) CHAR(n) VARCHAR(n) VARCHAR(n) BIT(n) BINARY(⌈n/8⌉) BINARY(n) BINARY(n) TINYTEXT
TEXT
MEDIUMTEXT
LONGTEXT
STRING TINYBLOB
BLOB
MEDIUMBLOB
LONGBLOB
BYTES Currently, for BLOB data type in TiDB, only the blob whose length isn't greater than 2,147,483,647(2 ** 31 - 1) is supported. YEAR INT ENUM STRING JSON STRING The JSON data type will be converted into STRING with JSON format in Flink. SET ARRAY&lt;STRING&gt; As the SET data type in TiDB is a string object that can have zero or more values, it should always be mapped to an array of string Back to top
`}),e.add({id:46,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/oceanbase-cdc/",title:"OceanBase",section:"Flink Source 连接器",content:` OceanBase CDC 连接器 # OceanBase CDC 连接器允许从 OceanBase 读取快照数据和增量数据。本文介绍了如何设置 OceanBase CDC 连接器以对 OceanBase 进行 SQL 查询。
OceanBase CDC 方案 # 名词解释:
OceanBase CE: OceanBase 社区版。OceanBase 的开源版本，兼容 MySQL https://github.com/oceanbase/oceanbase 。 OceanBase EE: OceanBase 企业版。OceanBase 的商业版本，支持 MySQL 和 Oracle 两种兼容模式 https://www.oceanbase.com 。 OceanBase Cloud: OceanBase 云数据库 https://www.oceanbase.com/product/cloud 。 Log Proxy CE: OceanBase 日志代理服务社区版。单独使用时支持 CDC 模式，是一个获取 OceanBase 社区版事务日志（commit log）的代理服务 https://github.com/oceanbase/oblogproxy 。 Log Proxy EE: OceanBase 日志代理服务企业版。单独使用时支持 CDC 模式，是一个获取 OceanBase 企业版事务日志（commit log）的代理服务，目前仅在 OceanBase Cloud 上提供有限的支持, 详情请咨询相关技术支持。 Binlog Service CE: OceanBase Binlog 服务社区版。OceanBase 社区版的一个兼容 MySQL 复制协议的解决方案，详情参考 Log Proxy CE Binlog 模式的文档。 Binlog Service EE: OceanBase Binlog 服务企业版。OceanBase 企业版 MySQL 模式的一个兼容 MySQL 复制协议的解决方案，仅可在阿里云使用，详情见操作指南。 MySQL Driver: mysql-connector-java，可用于 OceanBase 社区版和 OceanBase 企业版 MySQL 模式。 OceanBase Driver: OceanBase JDBC 驱动，支持所有版本的 MySQL 和 Oracle 兼容模式 https://github.com/oceanbase/obconnector-j 。 OceanBase CDC 源端读取方案：
数据库类型 支持的驱动 CDC 连接器 其他用到的组件 OceanBase CE MySQL Driver: 5.1.4x, 8.0.x OceanBase Driver: 2.4.x OceanBase CDC Connector Log Proxy CE MySQL Driver: 8.0.x MySQL CDC Connector Binlog Service CE OceanBase EE (MySQL 模式) MySQL Driver: 5.1.4x, 8.0.x OceanBase Driver: 2.4.x OceanBase CDC Connector Log Proxy EE MySQL Driver: 8.0.x MySQL CDC Connector Binlog Service EE OceanBase EE (Oracle 模式) OceanBase Driver: 2.4.x OceanBase CDC Connector Log Proxy EE (CDC 模式) 注意: 对于使用 OceanBase 社区版或 OceanBase 企业版 MySQL 模式的用户，我们推荐参考 MySQL CDC 的文档，使用 MySQL CDC 连接器搭配 Binlog 服务。
依赖 # 为了使用 OceanBase CDC 连接器，您必须提供相关的依赖信息。以下依赖信息适用于使用自动构建工具（如 Maven 或 SBT）构建的项目和带有 SQL JAR 包的 SQL 客户端。
Maven dependency # &ltdependency&gt &ltgroupId&gtorg.apache.flink&lt/groupId&gt &ltartifactId&gtflink-connector-oceanbase-cdc&lt/artifactId&gt &ltversion&gt3.2-SNAPSHOT&lt/version&gt &lt/dependency&gt Copied to clipboard! SQL Client JAR # 下载链接仅在已发布版本可用，请在文档网站左下角选择浏览已发布的版本。
下载flink-sql-connector-oceanbase-cdc-3.0.1.jar 到 &lt;FLINK_HOME&gt;/lib/ 目录下。
注意: 参考 flink-sql-connector-oceanbase-cdc 当前已发布的所有版本都可以在 Maven 中央仓库获取。
由于 MySQL Driver 和 OceanBase Driver 使用的开源协议都与 Flink CDC 项目不兼容，我们无法在 jar 包中提供驱动。 您可能需要手动配置以下依赖：
依赖名称 说明 mysql:mysql-connector-java:8.0.27 用于连接到 OceanBase 数据库的 MySQL 租户。 com.oceanbase:oceanbase-client:2.4.9 用于连接到 OceanBase 数据库的 MySQL 或 Oracle 租户。 配置 OceanBase 数据库和 Log Proxy 服务 # 按照 文档 配置 OceanBase 集群。
在 sys 租户中，为 oblogproxy 创建一个带密码的用户。
mysql -h\${host} -P\${port} -uroot mysql&gt; SHOW TENANT; mysql&gt; CREATE USER \${sys_username} IDENTIFIED BY &#39;\${sys_password}&#39;; mysql&gt; GRANT ALL PRIVILEGES ON *.* TO \${sys_username} WITH GRANT OPTION; 为你想要监控的租户创建一个用户，这个用户用来读取快照数据和变化事件数据。
OceanBase 社区版用户需要获取rootserver-list，可以使用以下命令获取：
mysql&gt; SHOW PARAMETERS LIKE &#39;rootservice_list&#39;; OceanBase 企业版用户需要获取 config-url，可以使用以下命令获取：
mysql&gt; show parameters like &#39;obconfig_url&#39;; 设置 OceanBase LogProxy。 对于OceanBase社区版的用户，您可以按照此文档进行操作。
创建 OceanBase CDC 表 # 使用以下命令，创建 OceanBase CDC 表：
-- 每 3 秒做一次 checkpoint，用于测试，生产配置建议 5 到 10 分钟 Flink SQL&gt; SET &#39;execution.checkpointing.interval&#39; = &#39;3s&#39;; -- 在 Flink SQL 中创建 OceanBase 表 \`orders\` Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;oceanbase-cdc&#39;, &#39;scan.startup.mode&#39; = &#39;initial&#39;, &#39;username&#39; = &#39;user@test_tenant#cluster_name&#39;, &#39;password&#39; = &#39;pswd&#39;, &#39;tenant-name&#39; = &#39;test_tenant&#39;, &#39;database-name&#39; = &#39;^test_db$&#39;, &#39;table-name&#39; = &#39;^orders$&#39;, &#39;hostname&#39; = &#39;127.0.0.1&#39;, &#39;port&#39; = &#39;2881&#39;, &#39;rootserver-list&#39; = &#39;127.0.0.1:2882:2881&#39;, &#39;logproxy.host&#39; = &#39;127.0.0.1&#39;, &#39;logproxy.port&#39; = &#39;2983&#39;, &#39;working-mode&#39; = &#39;memory&#39; ); -- 从表 orders 中读取快照数据和 binlog 数据 Flink SQL&gt; SELECT * FROM orders; 如果您使用的是企业版的 OceanBase Oracle 模式，您需要先添加 OceanBase 的官方 JDBC 驱动 jar 包到 Flink 环境，并且部署企业版的 oblogproxy 服务，然后通过以下命令创建 OceanBase CDC 表：
Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;oceanbase-cdc&#39;, &#39;scan.startup.mode&#39; = &#39;initial&#39;, &#39;username&#39; = &#39;user@test_tenant#cluster_name&#39;, &#39;password&#39; = &#39;pswd&#39;, &#39;tenant-name&#39; = &#39;test_tenant&#39;, &#39;database-name&#39; = &#39;^test_db$&#39;, &#39;table-name&#39; = &#39;^orders$&#39;, &#39;hostname&#39; = &#39;127.0.0.1&#39;, &#39;port&#39; = &#39;2881&#39;, &#39;compatible-mode&#39; = &#39;oracle&#39;, &#39;jdbc.driver&#39; = &#39;com.oceanbase.jdbc.Driver&#39;, &#39;config-url&#39; = &#39;http://127.0.0.1:8080/services?Action=ObRootServiceInfo&amp;User_ID=xxx&amp;UID=xxx&amp;ObRegion=xxx&#39;, &#39;logproxy.host&#39; = &#39;127.0.0.1&#39;, &#39;logproxy.port&#39; = &#39;2983&#39;, &#39;working-mode&#39; = &#39;memory&#39; ); 您也可以访问 Flink CDC 官网文档，快速体验将数据从 OceanBase 导入到 Elasticsearch。更多信息，参考 Flink CDC 官网文档。
OceanBase CDC 连接器选项 # OceanBase CDC 连接器包括用于 SQL 和 DataStream API 的选项，如下表所示。
注意：连接器支持两种方式来指定需要监听的表，两种方式同时使用时会监听两种方式匹配的所有表。
使用 database-name 和 table-name 匹配正则表达式中的数据库和表名。 使用 table-list 去匹配数据库名和表名的准确列表。 配置项 是否必选 默认值 类型 描述 connector 是 无 String 指定要使用的连接器，此处为 'oceanbase-cdc'。 scan.startup.mode 否 initial String 指定 OceanBase CDC 消费者的启动模式。可取值为 'initial'，'latest-offset'，'timestamp' 或 'snapshot'。 scan.startup.timestamp 否 无 Long 起始点的时间戳，单位为秒。仅在启动模式为 'timestamp' 时可用。 username 是 无 String 连接 OceanBase 数据库的用户的名称。 password 是 无 String 连接 OceanBase 数据库时使用的密码。 tenant-name 否 无 String 待监控 OceanBase 数据库的租户名，应该填入精确值。 database-name 否 无 String 待监控 OceanBase 数据库的数据库名，应该是正则表达式。 table-name 否 无 String 待监控 OceanBase 数据库的表名，应该是正则表达式。 table-list 否 无 String 待监控 OceanBase 数据库的全路径的表名列表，逗号分隔，如："db1.table1, db2.table2"。 hostname 是 无 String OceanBase 数据库或 OceanBbase 代理 ODP 的 IP 地址或主机名。 port 是 无 Integer OceanBase 数据库服务器的整数端口号。可以是 OceanBase 服务器的 SQL 端口号（默认值为 2881）
或 OceanBase代理服务的端口号（默认值为 2883） connect.timeout 否 30s Duration 连接器在尝试连接到 OceanBase 数据库服务器超时前的最长时间。 server-time-zone 否 +00:00 String 数据库服务器中的会话时区，用户控制 OceanBase 的时间类型如何转换为 STRING。
合法的值可以是格式为"±hh:mm"的 UTC 时区偏移量，
如果 mysql 数据库中的时区信息表已创建，合法的值则可以是创建的时区。 logproxy.host 否 无 String OceanBase 日志代理服务 的 IP 地址或主机名。 logproxy.port 否 无 Integer OceanBase 日志代理服务 的端口号。 logproxy.client.id 否 规则生成 String OceanBase日志代理服务的客户端连接 ID，默认值的生成规则是 {flink_ip}_{process_id}_{timestamp}_{thread_id}_{tenant}。 rootserver-list 否 无 String OceanBase root 服务器列表，服务器格式为 \`ip:rpc_port:sql_port\`，
多个服务器地址使用英文分号 \`;\` 隔开，OceanBase 社区版本必填。 config-url 否 无 String 从配置服务器获取服务器信息的 url, OceanBase 企业版本必填。 working-mode 否 storage String 日志代理中 \`libobcdc\` 的工作模式 , 可以是 \`storage\` 或 \`memory\`。 compatible-mode 否 mysql String OceanBase 的兼容模式，可以是 \`mysql\` 或 \`oracle\`。 jdbc.driver 否 com.mysql.cj.jdbc.Driver String 全量读取时使用的 jdbc 驱动类名。 jdbc.properties.* 否 无 String 传递自定义 JDBC URL 属性的选项。用户可以传递自定义属性，如 'jdbc.properties.useSSL' = 'false'。 obcdc.properties.* 否 无 String 传递自定义 libobcdc 属性的选项，如 'obcdc.properties.sort_trans_participants' = '1'。详情参见 obcdc 配置项说明。 支持的元数据 # 在创建表时，您可以使用以下格式的元数据作为只读列（VIRTUAL）。
列名 数据类型 描述 tenant_name STRING 当前记录所属的租户名称。 database_name STRING 当前记录所属的 db 名。 schema_name STRING 当前记录所属的 schema 名。 table_name STRING NOT NULL 当前记录所属的表名称。 op_ts TIMESTAMP_LTZ(3) NOT NULL 该值表示此修改在数据库中发生的时间。如果这条记录是该表在快照阶段读取的记录，则该值返回 0。 如下 SQL 展示了如何在表中使用这些元数据列：
CREATE TABLE products ( tenant_name STRING METADATA FROM &#39;tenant_name&#39; VIRTUAL, db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, table_name STRING METADATA FROM &#39;table_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY(order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;oceanbase-cdc&#39;, &#39;scan.startup.mode&#39; = &#39;initial&#39;, &#39;username&#39; = &#39;user@test_tenant&#39;, &#39;password&#39; = &#39;pswd&#39;, &#39;tenant-name&#39; = &#39;test_tenant&#39;, &#39;database-name&#39; = &#39;^test_db$&#39;, &#39;table-name&#39; = &#39;^orders$&#39;, &#39;hostname&#39; = &#39;127.0.0.1&#39;, &#39;port&#39; = &#39;2881&#39;, &#39;rootserver-list&#39; = &#39;127.0.0.1:2882:2881&#39;, &#39;logproxy.host&#39; = &#39;127.0.0.1&#39;, &#39;logproxy.port&#39; = &#39;2983&#39;, &#39;working-mode&#39; = &#39;memory&#39; ); 特性 # At-Least-Once 处理 # OceanBase CDC 连接器是一个 Flink Source 连接器。它将首先读取数据库快照，然后再读取变化事件，并进行 At-Least-Once 处理。
OceanBase 数据库是一个分布式数据库，它的日志也分散在不同的服务器上。由于没有类似 MySQL binlog 偏移量的位置信息，OceanBase 数据库用时间戳作为位置标记。为确保读取完整的数据，liboblog（读取 OceanBase 日志记录的 C++ 库）可能会在给定的时间戳之前读取一些日志数据。因此，OceanBase 数据库可能会读到起始点附近时间戳的重复数据，可保证 At-Least-Once 处理。
启动模式 # 配置选项 scan.startup.mode 指定 OceanBase CDC 连接器的启动模式。可用取值包括：
initial（默认）：在首次启动时对受监视的数据库表执行初始快照，并继续读取最新的提交日志。 latest-offset：首次启动时，不对受监视的数据库表执行快照，仅从连接器启动时读取提交日志。 timestamp：在首次启动时不对受监视的数据库表执行初始快照，仅从指定的 scan.startup.timestamp 读取最新的提交日志。 snapshot: 仅对受监视的数据库表执行初始快照。 消费提交日志 # OceanBase CDC 连接器使用 oblogclient 消费 OceanBase日志代理服务 中的事务日志。
DataStream Source # OceanBase CDC 连接器也可以作为 DataStream Source 使用。您可以按照如下创建一个 SourceFunction：
import org.apache.flink.cdc.connectors.base.options.StartupOptions; import org.apache.flink.cdc.connectors.oceanbase.OceanBaseSource; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; public class OceanBaseSourceExample { public static void main(String[] args) throws Exception { SourceFunction&lt;String&gt; oceanBaseSource = OceanBaseSource.&lt;String&gt;builder() .startupOptions(StartupOptions.initial()) .hostname(&#34;127.0.0.1&#34;) .port(2881) .username(&#34;user@test_tenant&#34;) .password(&#34;pswd&#34;) .compatibleMode(&#34;mysql&#34;) .jdbcDriver(&#34;com.mysql.cj.jdbc.Driver&#34;) .tenantName(&#34;test_tenant&#34;) .databaseName(&#34;^test_db$&#34;) .tableName(&#34;^test_table$&#34;) .logProxyHost(&#34;127.0.0.1&#34;) .logProxyPort(2983) .rsList(&#34;127.0.0.1:2882:2881&#34;) .serverTimeZone(&#34;+08:00&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000); env.addSource(oceanBaseSource).print().setParallelism(1); env.execute(&#34;Print OceanBase Snapshot + Change Events&#34;); } } 数据类型映射 # Mysql 模式 # OceanBase 数据类型 Flink SQL 类型 描述 BOOLEAN
TINYINT(1)
BIT(1) BOOLEAN TINYINT TINYINT SMALLINT
TINYINT UNSIGNED SMALLINT INT
MEDIUMINT
SMALLINT UNSIGNED INT BIGINT
INT UNSIGNED BIGINT BIGINT UNSIGNED DECIMAL(20, 0) REAL
FLOAT FLOAT DOUBLE DOUBLE NUMERIC(p, s)
DECIMAL(p, s)
where p <= 38 DECIMAL(p, s) NUMERIC(p, s)
DECIMAL(p, s)
where 38 < p <=65
STRING DECIMAL 等同于 NUMERIC。在 OceanBase 数据库中，DECIMAL 数据类型的精度最高为 65。
但在 Flink 中，DECIMAL 的最高精度为 38。因此，
如果你定义了一个精度大于 38 的 DECIMAL 列，你应当将其映射为 STRING，以避免精度损失。 DATE DATE TIME [(p)] TIME [(p)] DATETIME [(p)] TIMESTAMP [(p)] TIMESTAMP [(p)] TIMESTAMP_LTZ [(p)] CHAR(n) CHAR(n) VARCHAR(n) VARCHAR(n) BIT(n) BINARY(⌈(n + 7) / 8⌉) BINARY(n) BINARY(n) VARBINARY(N) VARBINARY(N) TINYTEXT
TEXT
MEDIUMTEXT
LONGTEXT STRING TINYBLOB
BLOB
MEDIUMBLOB
LONGBLOB BYTES YEAR INT ENUM STRING SET ARRAY&lt;STRING&gt; 因为 OceanBase 的 SET 类型是用包含一个或多个值的字符串对象表示，
所以映射到 Flink 时是一个字符串数组 JSON STRING JSON 类型的数据在 Flink 中会转化为 JSON 格式的字符串 Oracle 模式 # OceanBase type Flink SQL type NOTE NUMBER(1) BOOLEAN NUMBER(p, s <= 0), p - s < 3 TINYINT NUMBER(p, s <= 0), p - s < 5 SMALLINT NUMBER(p, s <= 0), p - s < 10 INT NUMBER(p, s <= 0), p - s < 19 BIGINT NUMBER(p, s <= 0), 19 <=p - s <=38 DECIMAL(p - s, 0) NUMBER(p, s > 0) DECIMAL(p, s) NUMBER(p, s <= 0), p - s> 38 STRING FLOAT
BINARY_FLOAT FLOAT BINARY_DOUBLE DOUBLE DATE
TIMESTAMP [(p)] TIMESTAMP [(p)] CHAR(n)
NCHAR(n)
VARCHAR(n)
VARCHAR2(n)
NVARCHAR2(n)
CLOB
STRING RAW
BLOB
ROWID BYTES Back to top
`}),e.add({id:47,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/vitess-cdc/",title:"Vitess",section:"Flink Source 连接器",content:` Vitess CDC Connector # The Vitess CDC connector allows for reading of incremental data from Vitess cluster. The connector does not support snapshot feature at the moment. This document describes how to setup the Vitess CDC connector to run SQL queries against Vitess databases. Vitess debezium documentation
Dependencies # In order to setup the Vitess CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency # &ltdependency&gt &ltgroupId&gtorg.apache.flink&lt/groupId&gt &ltartifactId&gtflink-connector-vitess-cdc&lt/artifactId&gt &ltversion&gt3.2-SNAPSHOT&lt/version&gt &lt/dependency&gt Copied to clipboard! SQL Client JAR # Download flink-sql-connector-vitess-cdc-3.0.1.jar and put it under &lt;FLINK_HOME&gt;/lib/.
Note: Refer to flink-sql-connector-vitess-cdc, more released versions will be available in the Maven central warehouse.
Setup Vitess server # You can follow the Local Install via Docker guide, or the Vitess Operator for Kubernetes guide to install Vitess. No special setup is needed to support Vitess connector.
Checklist # Make sure that the VTGate host and its gRPC port (default is 15991) is accessible from the machine where the Vitess connector is installed gRPC authentication # Because Vitess connector reads change events from the VTGate VStream gRPC server, it does not need to connect directly to MySQL instances. Therefore, no special database user and permissions are needed. At the moment, Vitess connector only supports unauthenticated access to the VTGate gRPC server.
How to create a Vitess CDC table # The Vitess CDC table can be defined as following:
-- checkpoint every 3000 milliseconds Flink SQL&gt; SET &#39;execution.checkpointing.interval&#39; = &#39;3s&#39;; -- register a Vitess table &#39;orders&#39; in Flink SQL Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY(order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;vitess-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;keyspace&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;orders&#39;); -- read snapshot and binlogs from orders table Flink SQL&gt; SELECT * FROM orders; Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be &lsquo;vitess-cdc&rsquo;. hostname required (none) String IP address or hostname of the Vitess database server (VTGate). keyspace required (none) String The name of the keyspace from which to stream the changes. username optional (none) String An optional username of the Vitess database server (VTGate). If not configured, unauthenticated VTGate gRPC is used. password optional (none) String An optional password of the Vitess database server (VTGate). If not configured, unauthenticated VTGate gRPC is used. shard optional (none) String An optional name of the shard from which to stream the changes. If not configured, in case of unsharded keyspace, the connector streams changes from the only shard, in case of sharded keyspace, the connector streams changes from all shards in the keyspace. gtid optional current String An optional GTID position for a shard to stream from. stopOnReshard optional false Boolean Controls Vitess flag stop_on_reshard. tombstonesOnDelete optional true Boolean Controls whether a delete event is followed by a tombstone event. tombstonesOnDelete optional true Boolean Controls whether a delete event is followed by a tombstone event. schemaNameAdjustmentMode optional avro String Specifies how schema names should be adjusted for compatibility with the message converter used by the connector. table-name required (none) String Table name of the MySQL database to monitor. tablet.type optional RDONLY String The type of Tablet (hence MySQL) from which to stream the changes: MASTER represents streaming from the master MySQL instance REPLICA represents streaming from the replica slave MySQL instance RDONLY represents streaming from the read-only slave MySQL instance. Features # Incremental Reading # The Vitess connector spends all its time streaming changes from the VTGate’s VStream gRPC service to which it is subscribed. The client receives changes from VStream as they are committed in the underlying MySQL server’s binlog at certain positions, which are referred to as VGTID.
The VGTID in Vitess is the equivalent of GTID in MySQL, it describes the position in the VStream in which a change event happens. Typically, A VGTID has multiple shard GTIDs, each shard GTID is a tuple of (Keyspace, Shard, GTID), which describes the GTID position of a given shard.
When subscribing to a VStream service, the connector needs to provide a VGTID and a Tablet Type (e.g. MASTER, REPLICA). The VGTID describes the position from which VStream should starts sending change events; the Tablet type describes which underlying MySQL instance (master or replica) in each shard do we read change events from.
The first time the connector connects to a Vitess cluster, it gets and provides the current VGTID to VStream.
The Debezium Vitess connector acts as a gRPC client of VStream. When the connector receives changes it transforms the events into Debezium create, update, or delete events that include the VGTID of the event. The Vitess connector forwards these change events in records to the Kafka Connect framework, which is running in the same process. The Kafka Connect process asynchronously writes the change event records in the same order in which they were generated to the appropriate Kafka topic.
Checkpoint # Incremental snapshot reading provides the ability to perform checkpoint in chunk level. It resolves the checkpoint timeout problem in previous version with old snapshot reading mechanism.
Exactly-Once Processing # The Vitess CDC connector is a Flink Source connector which will read table snapshot chunks first and then continues to read binlog, both snapshot phase and binlog phase, Vitess CDC connector read with exactly-once processing even failures happen.
DataStream Source # The Incremental Reading feature of Vitess CDC Source only exposes in SQL currently, if you&rsquo;re using DataStream, please use Vitess Source:
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.cdc.connectors.vitess.VitessSource; public class VitessSourceExample { public static void main(String[] args) throws Exception { SourceFunction&lt;String&gt; sourceFunction = VitessSource.&lt;String&gt;builder() .hostname(&#34;localhost&#34;) .port(15991) .keyspace(&#34;inventory&#34;) .username(&#34;flinkuser&#34;) .password(&#34;flinkpw&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env .addSource(sourceFunction) .print().setParallelism(1); // use parallelism 1 for sink to keep message ordering env.execute(); } } Data Type Mapping # MySQL type Flink SQL type TINYINT TINYINT SMALLINT
TINYINT UNSIGNED SMALLINT INT
MEDIUMINT
SMALLINT UNSIGNED INT BIGINT
INT UNSIGNED BIGINT BIGINT UNSIGNED DECIMAL(20, 0) BIGINT BIGINT FLOAT FLOAT DOUBLE
DOUBLE PRECISION DOUBLE NUMERIC(p, s)
DECIMAL(p, s) DECIMAL(p, s) BOOLEAN
TINYINT(1) BOOLEAN CHAR(n)
VARCHAR(n)
TEXT STRING Back to top
`}),e.add({id:48,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/datastream-api-package-guidance/",title:"DataStream API 打包指南",section:"Flink Source 连接器",content:" DataStream API Package Guidance # This guide provides a simple pom.xml example for packaging DataStream job JARs with MySQL CDC source.\nExample for pom.xml # &lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt; &lt;project xmlns=&#34;http://maven.apache.org/POM/4.0.0&#34; xmlns:xsi=&#34;http://www.w3.org/2001/XMLSchema-instance&#34; xsi:schemaLocation=&#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&#34;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;FlinkCDCTest&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;scala.binary.version&gt;2.12&lt;/scala.binary.version&gt; &lt;maven.compiler.source&gt;${java.version}&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;${java.version}&lt;/maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;!-- Enforce single fork execution due to heavy mini cluster use in the tests --&gt; &lt;flink.forkCount&gt;1&lt;/flink.forkCount&gt; &lt;flink.reuseForks&gt;true&lt;/flink.reuseForks&gt; &lt;!-- dependencies versions --&gt; &lt;flink.version&gt;1.17.2&lt;/flink.version&gt; &lt;slf4j.version&gt;1.7.15&lt;/slf4j.version&gt; &lt;log4j.version&gt;2.17.1&lt;/log4j.version&gt; &lt;debezium.version&gt;1.9.7.Final&lt;/debezium.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner_${scala.binary.version}&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-runtime&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-core&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Checked the dependencies of the Flink project and below is a feasible reference. --&gt; &lt;!-- Use flink shaded guava 18.0-13.0 for flink 1.13 --&gt; &lt;!-- Use flink shaded guava 30.1.1-jre-14.0 for flink-1.14 --&gt; &lt;!-- Use flink shaded guava 30.1.1-jre-15.0 for flink-1.15 --&gt; &lt;!-- Use flink shaded guava 30.1.1-jre-15.0 for flink-1.16 --&gt; &lt;!-- Use flink shaded guava 30.1.1-jre-16.1 for flink-1.17 --&gt; &lt;!-- Use flink shaded guava 31.1-jre-17.0 for flink-1.18 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-shaded-guava&lt;/artifactId&gt; &lt;version&gt;30.1.1-jre-16.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-mysql-cdc&lt;/artifactId&gt; &lt;version&gt;2.4.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.debezium&lt;/groupId&gt; &lt;artifactId&gt;debezium-connector-mysql&lt;/artifactId&gt; &lt;version&gt;${debezium.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;shade-flink&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;!-- Shading test jar have bug in some previous version, so close this configuration here, see https://issues.apache.org/jira/browse/MSHADE-284 --&gt; &lt;shadeTestJar&gt;false&lt;/shadeTestJar&gt; &lt;shadedArtifactAttached&gt;false&lt;/shadedArtifactAttached&gt; &lt;createDependencyReducedPom&gt;true&lt;/createDependencyReducedPom&gt; &lt;dependencyReducedPomLocation&gt; ${project.basedir}/target/dependency-reduced-pom.xml &lt;/dependencyReducedPomLocation&gt; &lt;filters combine.children=&#34;append&#34;&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;module-info.class&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;artifactSet&gt; &lt;includes&gt; &lt;!-- include nothing --&gt; &lt;include&gt;io.debezium:debezium-api&lt;/include&gt; &lt;include&gt;io.debezium:debezium-embedded&lt;/include&gt; &lt;include&gt;io.debezium:debezium-core&lt;/include&gt; &lt;include&gt;io.debezium:debezium-ddl-parser&lt;/include&gt; &lt;include&gt;io.debezium:debezium-connector-mysql&lt;/include&gt; &lt;include&gt;org.apache.flink:flink-connector-debezium&lt;/include&gt; &lt;include&gt;org.apache.flink:flink-connector-mysql-cdc&lt;/include&gt; &lt;include&gt;org.antlr:antlr4-runtime&lt;/include&gt; &lt;include&gt;org.apache.kafka:*&lt;/include&gt; &lt;include&gt;mysql:mysql-connector-java&lt;/include&gt; &lt;include&gt;com.zendesk:mysql-binlog-connector-java&lt;/include&gt; &lt;include&gt;com.fasterxml.*:*&lt;/include&gt; &lt;include&gt;com.google.guava:*&lt;/include&gt; &lt;include&gt;com.esri.geometry:esri-geometry-api&lt;/include&gt; &lt;include&gt;com.zaxxer:HikariCP&lt;/include&gt; &lt;!-- Include fixed version 30.1.1-jre-16.0 of flink shaded guava --&gt; &lt;include&gt;org.apache.flink:flink-shaded-guava&lt;/include&gt; &lt;/includes&gt; &lt;/artifactSet&gt; &lt;relocations&gt; &lt;relocation&gt; &lt;pattern&gt;org.apache.kafka&lt;/pattern&gt; &lt;shadedPattern&gt; org.apache.flink.cdc.connectors.shaded.org.apache.kafka &lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;relocation&gt; &lt;pattern&gt;org.antlr&lt;/pattern&gt; &lt;shadedPattern&gt; org.apache.flink.cdc.connectors.shaded.org.antlr &lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;relocation&gt; &lt;pattern&gt;com.fasterxml&lt;/pattern&gt; &lt;shadedPattern&gt; org.apache.flink.cdc.connectors.shaded.com.fasterxml &lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;relocation&gt; &lt;pattern&gt;com.google&lt;/pattern&gt; &lt;shadedPattern&gt; org.apache.flink.cdc.connectors.shaded.com.google &lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;relocation&gt; &lt;pattern&gt;com.esri.geometry&lt;/pattern&gt; &lt;shadedPattern&gt;org.apache.flink.cdc.connectors.shaded.com.esri.geometry&lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;relocation&gt; &lt;pattern&gt;com.zaxxer&lt;/pattern&gt; &lt;shadedPattern&gt; org.apache.flink.cdc.connectors.shaded.com.zaxxer &lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;/relocations&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; Example for Code # package org.apache.flink.flink.cdc; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.cdc.connectors.mysql.source.MySqlSource; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; public class CdcTest { public static void main(String[] args) throws Exception { MySqlSource&lt;String&gt; mySqlSource = MySqlSource.&lt;String&gt;builder() .hostname(&#34;yourHostname&#34;) .port(yourPort) .databaseList(&#34;yourDatabaseName&#34;) // set captured database, If you need to synchronize the whole database, Please set tableList to &#34;.*&#34;. .tableList(&#34;yourDatabaseName.yourTableName&#34;) // set captured table .username(&#34;yourUsername&#34;) .password(&#34;yourPassword&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String .build(); final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000); env .fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), &#34;MySQL Source&#34;) // set 1 parallel source tasks .setParallelism(1) .print().setParallelism(1); // use parallelism 1 for sink env.execute(&#34;Print MySQL Snapshot + Binlog&#34;); } } Back to top\n"}),e.add({id:49,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/build-real-time-data-lake-tutorial/",title:"使用 Flink CDC 构建实时数据湖",section:"Flink CDC Sources 教程",content:` 使用 Flink CDC 构建实时数据湖 # 在 OLTP 系统中，为了解决单表数据量大的问题，通常采用分库分表的方式将单个大表进行拆分以提高系统的吞吐量。 但是为了方便数据分析，通常需要将分库分表拆分出的表在同步到数据仓库、数据湖时，再合并成一个大表。
这篇教程将展示如何使用 Flink CDC 构建实时数据湖来应对这种场景，本教程的演示基于 Docker，只涉及 SQL，无需一行 Java/Scala 代码，也无需安装 IDE，你可以很方便地在自己的电脑上完成本教程的全部内容。
接下来将以数据从 MySQL 同步到 Iceberg 为例展示整个流程，架构图如下所示：
你也可以使用不同的 source 比如 Oracle/Postgres 和 sink 比如 Hudi 来构建自己的 ETL 流程。
准备阶段 # 准备一台已经安装了 Docker 的 Linux 或者 MacOS 电脑。
准备教程所需要的组件 # 接下来的教程将以 docker-compose 的方式准备所需要的组件。
使用下面的内容创建一个 docker-compose.yml 文件：
version: &#39;2.1&#39; services: sql-client: user: flink:flink image: yuxialuo/flink-sql-client:1.13.2.v1 depends_on: - jobmanager - mysql environment: FLINK_JOBMANAGER_HOST: jobmanager MYSQL_HOST: mysql volumes: - shared-tmpfs:/tmp/iceberg jobmanager: user: flink:flink image: flink:1.13.2-scala_2.11 ports: - &#34;8081:8081&#34; command: jobmanager environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager volumes: - shared-tmpfs:/tmp/iceberg taskmanager: user: flink:flink image: flink:1.13.2-scala_2.11 depends_on: - jobmanager command: taskmanager environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager taskmanager.numberOfTaskSlots: 2 volumes: - shared-tmpfs:/tmp/iceberg mysql: image: debezium/example-mysql:1.1 ports: - &#34;3306:3306&#34; environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_USER=mysqluser - MYSQL_PASSWORD=mysqlpw volumes: shared-tmpfs: driver: local driver_opts: type: &#34;tmpfs&#34; device: &#34;tmpfs&#34; 该 Docker Compose 中包含的容器有：
SQL-Client: Flink SQL Client, 用来提交 SQL 查询和查看 SQL 的执行结果 Flink Cluster：包含 Flink JobManager 和 Flink TaskManager，用来执行 Flink SQL MySQL：作为分库分表的数据源，存储本教程的 user 表 注意：
为了简化整个教程，本教程需要的 jar 包都已经被打包进 SQL-Client 容器中了，镜像的构建脚本可以在 GitHub 上找到。 如果你想要在自己的 Flink 环境运行本教程，需要下载下面列出的包并且把它们放在 Flink 所在目录的 lib 目录下，即 FLINK_HOME/lib/。
下载链接只对已发布的版本有效, SNAPSHOT 版本需要本地编译
flink-sql-connector-mysql-cdc-2.4.0.jar flink-shaded-hadoop-2-uber-2.7.5-10.0.jar iceberg-flink-1.13-runtime-0.13.0-SNAPSHOT.jar 目前支持 Flink 1.13 的 iceberg-flink-runtime jar 包还没有发布，所以我们在这里提供了一个支持 Flink 1.13 的 iceberg-flink-runtime jar 包，这个 jar 包是基于 Iceberg 的 master 分支打包的。 当 Iceberg 0.13.0 版本发布后，你也可以在 apache official repository 下载到支持 Flink 1.13 的 iceberg-flink-runtime jar 包。
本教程接下来用到的容器相关的命令都需要在 docker-compose.yml 所在目录下执行
在 docker-compose.yml 所在目录下执行下面的命令来启动本教程需要的组件：
docker-compose up -d 该命令将以 detached 模式自动启动 Docker Compose 配置中定义的所有容器。你可以通过 docker ps 来观察上述的容器是否正常启动了，也可以通过访问 http://localhost:8081/ 来查看 Flink 是否运行正常。
准备数据 # 进入 MySQL 容器中
docker-compose exec mysql mysql -uroot -p123456 创建数据和表，并填充数据
创建两个不同的数据库，并在每个数据库中创建两个表，作为 user 表分库分表下拆分出的表。
CREATE DATABASE db_1; USE db_1; CREATE TABLE user_1 ( id INTEGER NOT NULL PRIMARY KEY, name VARCHAR(255) NOT NULL DEFAULT &#39;flink&#39;, address VARCHAR(1024), phone_number VARCHAR(512), email VARCHAR(255) ); INSERT INTO user_1 VALUES (110,&#34;user_110&#34;,&#34;Shanghai&#34;,&#34;123567891234&#34;,&#34;user_110@foo.com&#34;); CREATE TABLE user_2 ( id INTEGER NOT NULL PRIMARY KEY, name VARCHAR(255) NOT NULL DEFAULT &#39;flink&#39;, address VARCHAR(1024), phone_number VARCHAR(512), email VARCHAR(255) ); INSERT INTO user_2 VALUES (120,&#34;user_120&#34;,&#34;Shanghai&#34;,&#34;123567891234&#34;,&#34;user_120@foo.com&#34;); CREATE DATABASE db_2; USE db_2; CREATE TABLE user_1 ( id INTEGER NOT NULL PRIMARY KEY, name VARCHAR(255) NOT NULL DEFAULT &#39;flink&#39;, address VARCHAR(1024), phone_number VARCHAR(512), email VARCHAR(255) ); INSERT INTO user_1 VALUES (110,&#34;user_110&#34;,&#34;Shanghai&#34;,&#34;123567891234&#34;, NULL); CREATE TABLE user_2 ( id INTEGER NOT NULL PRIMARY KEY, name VARCHAR(255) NOT NULL DEFAULT &#39;flink&#39;, address VARCHAR(1024), phone_number VARCHAR(512), email VARCHAR(255) ); INSERT INTO user_2 VALUES (220,&#34;user_220&#34;,&#34;Shanghai&#34;,&#34;123567891234&#34;,&#34;user_220@foo.com&#34;); 在 Flink SQL CLI 中使用 Flink DDL 创建表 # 首先，使用如下的命令进入 Flink SQL CLI 容器中：
docker-compose exec sql-client ./sql-client 我们可以看到如下界面：
然后，进行如下步骤：
开启 checkpoint，每隔3秒做一次 checkpoint
Checkpoint 默认是不开启的，我们需要开启 Checkpoint 来让 Iceberg 可以提交事务。 并且，mysql-cdc 在 binlog 读取阶段开始前，需要等待一个完整的 checkpoint 来避免 binlog 记录乱序的情况。
-- Flink SQL Flink SQL&gt; SET execution.checkpointing.interval = 3s; 创建 MySQL 分库分表 source 表
创建 source 表 user_source 来捕获MySQL中所有 user 表的数据，在表的配置项 database-name , table-name 使用正则表达式来匹配这些表。 并且，user_source 表也定义了 metadata 列来区分数据是来自哪个数据库和表。
-- Flink SQL Flink SQL&gt; CREATE TABLE user_source ( database_name STRING METADATA VIRTUAL, table_name STRING METADATA VIRTUAL, \`id\` DECIMAL(20, 0) NOT NULL, name STRING, address STRING, phone_number STRING, email STRING, PRIMARY KEY (\`id\`) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;mysql&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;username&#39; = &#39;root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;db_[0-9]+&#39;, &#39;table-name&#39; = &#39;user_[0-9]+&#39; ); 创建 Iceberg sink 表
创建 sink 表 all_users_sink，用来将数据加载至 Iceberg 中。 在这个 sink 表，考虑到不同的 MySQL 数据库表的 id 字段的值可能相同，我们定义了复合主键 (database_name, table_name, id)。
-- Flink SQL Flink SQL&gt; CREATE TABLE all_users_sink ( database_name STRING, table_name STRING, \`id\` DECIMAL(20, 0) NOT NULL, name STRING, address STRING, phone_number STRING, email STRING, PRIMARY KEY (database_name, table_name, \`id\`) NOT ENFORCED ) WITH ( &#39;connector&#39;=&#39;iceberg&#39;, &#39;catalog-name&#39;=&#39;iceberg_catalog&#39;, &#39;catalog-type&#39;=&#39;hadoop&#39;, &#39;warehouse&#39;=&#39;file:///tmp/iceberg/warehouse&#39;, &#39;format-version&#39;=&#39;2&#39; ); 流式写入 Iceberg # 使用下面的 Flink SQL 语句将数据从 MySQL 写入 Iceberg 中
-- Flink SQL Flink SQL&gt; INSERT INTO all_users_sink select * from user_source; 上述命令将会启动一个流式作业，源源不断将 MySQL 数据库中的全量和增量数据同步到 Iceberg 中。 在 Flink UI 上可以看到这个运行的作业：
然后我们就可以使用如下的命令看到 Iceberg 中的写入的文件：
docker-compose exec sql-client tree /tmp/iceberg/warehouse/default_database/ 如下所示：
在你的运行环境中，实际的文件可能与上面的截图不相同，但是整体的目录结构应该相似。
使用下面的 Flink SQL 语句查询表 all_users_sink 中的数据
-- Flink SQL Flink SQL&gt; SELECT * FROM all_users_sink; 在 Flink SQL CLI 中我们可以看到如下查询结果：
修改 MySQL 中表的数据，Iceberg 中的表 all_users_sink 中的数据也将实时更新：
(3.1) 在 db_1.user_1 表中插入新的一行
--- db_1 INSERT INTO db_1.user_1 VALUES (111,&#34;user_111&#34;,&#34;Shanghai&#34;,&#34;123567891234&#34;,&#34;user_111@foo.com&#34;); (3.2) 更新 db_1.user_2 表的数据
--- db_1 UPDATE db_1.user_2 SET address=&#39;Beijing&#39; WHERE id=120; (3.3) 在 db_2.user_2 表中删除一行
--- db_2 DELETE FROM db_2.user_2 WHERE id=220; 每执行一步，我们就可以在 Flink Client CLI 中使用 SELECT * FROM all_users_sink 查询表 all_users_sink 来看到数据的变化。
最后的查询结果如下所示：
从 Iceberg 的最新结果中可以看到新增了(db_1, user_1, 111)的记录，(db_1, user_2, 120)的地址更新成了 Beijing，且(db_2, user_2, 220)的记录被删除了，与我们在 MySQL 做的数据更新完全一致。
环境清理 # 本教程结束后，在 docker-compose.yml 文件所在的目录下执行如下命令停止所有容器：
docker-compose down Back to top
`}),e.add({id:50,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/",title:"Flink CDC Sources 教程",section:"Flink Source 连接器",content:" "}),e.add({id:51,href:"/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/build-streaming-etl-tutorial/",title:"使用 Flink CDC 构建 Streaming ETL",section:"Flink CDC Sources 教程",content:` 使用 Flink CDC 构建 Streaming ETL # 这篇教程将展示如何基于 Flink CDC 快速构建 MySQL 和 Postgres 的流式 ETL。本教程的演示都将在 Flink SQL CLI 中进行，只涉及 SQL，无需一行 Java/Scala 代码，也无需安装 IDE。
假设我们正在经营电子商务业务，商品和订单的数据存储在 MySQL 中，订单对应的物流信息存储在 Postgres 中。 对于订单表，为了方便进行分析，我们希望让它关联上其对应的商品和物流信息，构成一张宽表，并且实时把它写到 ElasticSearch 中。
接下来的内容将介绍如何使用 Flink Mysql/Postgres CDC 来实现这个需求，系统的整体架构如下图所示： 准备阶段 # 准备一台已经安装了 Docker 的 Linux 或者 MacOS 电脑。
准备教程所需要的组件 # 接下来的教程将以 docker-compose 的方式准备所需要的组件。
使用下面的内容创建一个 docker-compose.yml 文件：
version: &#39;2.1&#39; services: postgres: image: debezium/example-postgres:1.1 ports: - &#34;5432:5432&#34; environment: - POSTGRES_DB=postgres - POSTGRES_USER=postgres - POSTGRES_PASSWORD=postgres mysql: image: debezium/example-mysql:1.1 ports: - &#34;3306:3306&#34; environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_USER=mysqluser - MYSQL_PASSWORD=mysqlpw elasticsearch: image: elastic/elasticsearch:7.6.0 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34; - discovery.type=single-node ports: - &#34;9200:9200&#34; - &#34;9300:9300&#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 ports: - &#34;5601:5601&#34; 该 Docker Compose 中包含的容器有：
MySQL: 商品表 products 和 订单表 orders 将存储在该数据库中， 这两张表将和 Postgres 数据库中的物流表 shipments进行关联，得到一张包含更多信息的订单表 enriched_orders Postgres: 物流表 shipments 将存储在该数据库中 Elasticsearch: 最终的订单表 enriched_orders 将写到 Elasticsearch Kibana: 用来可视化 ElasticSearch 的数据 在 docker-compose.yml 所在目录下执行下面的命令来启动本教程需要的组件：
docker-compose up -d 该命令将以 detached 模式自动启动 Docker Compose 配置中定义的所有容器。你可以通过 docker ps 来观察上述的容器是否正常启动了，也可以通过访问 http://localhost:5601/ 来查看 Kibana 是否运行正常。
下载 Flink 和所需要的依赖包 # 下载 Flink 1.17.0 并将其解压至目录 flink-1.17.0
下载下面列出的依赖包，并将它们放到目录 flink-1.17.0/lib/ 下：
下载链接只对已发布的版本有效, SNAPSHOT 版本需要本地编译
flink-sql-connector-elasticsearch7-3.0.1-1.17.jar flink-sql-connector-mysql-cdc-2.4.0.jar flink-sql-connector-postgres-cdc-2.4.0.jar 准备数据 # 在 MySQL 数据库中准备数据 # 进入 MySQL 容器 docker-compose exec mysql mysql -uroot -p123456 创建数据库和表 products，orders，并插入数据 -- MySQL CREATE DATABASE mydb; USE mydb; CREATE TABLE products ( id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(512) ); ALTER TABLE products AUTO_INCREMENT = 101; INSERT INTO products VALUES (default,&#34;scooter&#34;,&#34;Small 2-wheel scooter&#34;), (default,&#34;car battery&#34;,&#34;12V car battery&#34;), (default,&#34;12-pack drill bits&#34;,&#34;12-pack of drill bits with sizes ranging from #40 to #3&#34;), (default,&#34;hammer&#34;,&#34;12oz carpenter&#39;s hammer&#34;), (default,&#34;hammer&#34;,&#34;14oz carpenter&#39;s hammer&#34;), (default,&#34;hammer&#34;,&#34;16oz carpenter&#39;s hammer&#34;), (default,&#34;rocks&#34;,&#34;box of assorted rocks&#34;), (default,&#34;jacket&#34;,&#34;water resistent black wind breaker&#34;), (default,&#34;spare tire&#34;,&#34;24 inch spare tire&#34;); CREATE TABLE orders ( order_id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, order_date DATETIME NOT NULL, customer_name VARCHAR(255) NOT NULL, price DECIMAL(10, 5) NOT NULL, product_id INTEGER NOT NULL, order_status BOOLEAN NOT NULL -- Whether order has been placed ) AUTO_INCREMENT = 10001; INSERT INTO orders VALUES (default, &#39;2020-07-30 10:08:22&#39;, &#39;Jark&#39;, 50.50, 102, false), (default, &#39;2020-07-30 10:11:09&#39;, &#39;Sally&#39;, 15.00, 105, false), (default, &#39;2020-07-30 12:00:30&#39;, &#39;Edward&#39;, 25.25, 106, false); 在 Postgres 数据库中准备数据 # 进入 Postgres 容器 docker-compose exec postgres psql -h localhost -U postgres 创建表 shipments，并插入数据 -- PG CREATE TABLE shipments ( shipment_id SERIAL NOT NULL PRIMARY KEY, order_id SERIAL NOT NULL, origin VARCHAR(255) NOT NULL, destination VARCHAR(255) NOT NULL, is_arrived BOOLEAN NOT NULL ); ALTER SEQUENCE public.shipments_shipment_id_seq RESTART WITH 1001; ALTER TABLE public.shipments REPLICA IDENTITY FULL; INSERT INTO shipments VALUES (default,10001,&#39;Beijing&#39;,&#39;Shanghai&#39;,false), (default,10002,&#39;Hangzhou&#39;,&#39;Shanghai&#39;,false), (default,10003,&#39;Shanghai&#39;,&#39;Hangzhou&#39;,false); 启动 Flink 集群和 Flink SQL CLI # 使用下面的命令跳转至 Flink 目录下
cd flink-1.17.0 使用下面的命令启动 Flink 集群
./bin/start-cluster.sh 启动成功的话，可以在 http://localhost:8081/ 访问到 Flink Web UI，如下所示：
使用下面的命令启动 Flink SQL CLI
./bin/sql-client.sh 启动成功后，可以看到如下的页面：
在 Flink SQL CLI 中使用 Flink DDL 创建表 # 首先，开启 checkpoint，每隔3秒做一次 checkpoint
-- Flink SQL Flink SQL&gt; SET execution.checkpointing.interval = 3s; 然后, 对于数据库中的表 products, orders, shipments， 使用 Flink SQL CLI 创建对应的表，用于同步这些底层数据库表的数据
-- Flink SQL Flink SQL&gt; CREATE TABLE products ( id INT, name STRING, description STRING, PRIMARY KEY (id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;username&#39; = &#39;root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;products&#39; ); Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;username&#39; = &#39;root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;orders&#39; ); Flink SQL&gt; CREATE TABLE shipments ( shipment_id INT, order_id INT, origin STRING, destination STRING, is_arrived BOOLEAN, PRIMARY KEY (shipment_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;postgres-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;5432&#39;, &#39;username&#39; = &#39;postgres&#39;, &#39;password&#39; = &#39;postgres&#39;, &#39;database-name&#39; = &#39;postgres&#39;, &#39;schema-name&#39; = &#39;public&#39;, &#39;table-name&#39; = &#39;shipments&#39;, &#39;slot.name&#39; = &#39;flink&#39; ); 最后，创建 enriched_orders 表， 用来将关联后的订单数据写入 Elasticsearch 中
-- Flink SQL Flink SQL&gt; CREATE TABLE enriched_orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, product_name STRING, product_description STRING, shipment_id INT, origin STRING, destination STRING, is_arrived BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;elasticsearch-7&#39;, &#39;hosts&#39; = &#39;http://localhost:9200&#39;, &#39;index&#39; = &#39;enriched_orders&#39; ); 关联订单数据并且将其写入 Elasticsearch 中 # 使用 Flink SQL 将订单表 order 与 商品表 products，物流信息表 shipments 关联，并将关联后的订单信息写入 Elasticsearch 中
-- Flink SQL Flink SQL&gt; INSERT INTO enriched_orders SELECT o.*, p.name, p.description, s.shipment_id, s.origin, s.destination, s.is_arrived FROM orders AS o LEFT JOIN products AS p ON o.product_id = p.id LEFT JOIN shipments AS s ON o.order_id = s.order_id; 现在，就可以在 Kibana 中看到包含商品和物流信息的订单数据。
首先访问 http://localhost:5601/app/kibana#/management/kibana/index_pattern 创建 index pattern enriched_orders.
然后就可以在 http://localhost:5601/app/kibana#/discover 看到写入的数据了.
接下来，修改 MySQL 和 Postgres 数据库中表的数据，Kibana中显示的订单数据也将实时更新：
在 MySQL 的 orders 表中插入一条数据 --MySQL INSERT INTO orders VALUES (default, &#39;2020-07-30 15:22:00&#39;, &#39;Jark&#39;, 29.71, 104, false); 在 Postgres 的 shipment 表中插入一条数据 --PG INSERT INTO shipments VALUES (default,10004,&#39;Shanghai&#39;,&#39;Beijing&#39;,false); 在 MySQL 的 orders 表中更新订单的状态 --MySQL UPDATE orders SET order_status = true WHERE order_id = 10004; 在 Postgres 的 shipment 表中更新物流的状态 --PG UPDATE shipments SET is_arrived = true WHERE shipment_id = 1004; 在 MYSQL 的 orders 表中删除一条数据 --MySQL DELETE FROM orders WHERE order_id = 10004; 每执行一步就刷新一次 Kibana，可以看到 Kibana 中显示的订单数据将实时更新，如下所示： 环境清理 # 本教程结束后，在 docker-compose.yml 文件所在的目录下执行如下命令停止所有容器：
docker-compose down 在 Flink 所在目录 flink-1.17.0 下执行如下命令停止 Flink 集群：
./bin/stop-cluster.sh Back to top
`}),e.add({id:52,href:"/flink/flink-cdc-docs-master/zh/versions/",title:"Versions",section:"Apache Flink CDC",content:` Versions # An appendix of hosted documentation for all versions of Apache Flink CDC.
v3.0 `})})()