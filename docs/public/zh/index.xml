<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Flink CDC</title>
    <link>//localhost:1313/flink/flink-cdc-docs-master/zh/</link>
    <description>Recent content on Apache Flink CDC</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <atom:link href="//localhost:1313/flink/flink-cdc-docs-master/zh/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Data Pipeline</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/core-concept/data-pipeline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/core-concept/data-pipeline/</guid>
      <description>Definition # Since events in Flink CDC flow from the upstream to the downstream in a pipeline manner, the whole ETL task is referred as a Data Pipeline.&#xA;Parameters # A pipeline corresponds to a chain of operators in Flink.&#xA;To describe a Data Pipeline, the following parts are required:&#xA;source sink pipeline the following parts are optional:&#xA;route transform Example # Only required # We could use following yaml file to define a concise Data Pipeline describing synchronize all tables under MySQL app_db database to Doris :</description>
    </item>
    <item>
      <title>MongoDB æ•™ç¨‹</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/mongodb-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/mongodb-tutorial/</guid>
      <description>æ¼”ç¤º: MongoDB CDC å¯¼å…¥ Elasticsearch # ä¸‹è½½ docker-compose.yml version: &amp;#39;2.1&amp;#39; services: mongo: image: &amp;#34;mongo:4.0-xenial&amp;#34; command: --replSet rs0 --smallfiles --oplogSize 128 ports: - &amp;#34;27017:27017&amp;#34; environment: - MONGO_INITDB_ROOT_USERNAME=mongouser - MONGO_INITDB_ROOT_PASSWORD=mongopw elasticsearch: image: elastic/elasticsearch:7.6.0 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &amp;#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&amp;#34; - discovery.type=single-node ports: - &amp;#34;9200:9200&amp;#34; - &amp;#34;9300:9300&amp;#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 ports: - &amp;#34;5601:5601&amp;#34; è¿›å…¥ MongoDB å®¹å™¨ï¼Œåˆå§‹åŒ–å‰¯æœ¬é›†å’Œæ•°æ®: docker-compose exec mongo /usr/bin/mongo -u mongouser -p mongopw // 1.</description>
    </item>
    <item>
      <title>MySQL åŒæ­¥åˆ° Doris</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/get-started/quickstart/mysql-to-doris/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/get-started/quickstart/mysql-to-doris/</guid>
      <description>Streaming ELT åŒæ­¥ MySQL åˆ° Doris # è¿™ç¯‡æ•™ç¨‹å°†å±•ç¤ºå¦‚ä½•åŸºäº Flink CDC å¿«é€Ÿæ„å»º MySQL åˆ° Doris çš„ Streaming ELT ä½œä¸šï¼ŒåŒ…å«æ•´åº“åŒæ­¥ã€è¡¨ç»“æ„å˜æ›´åŒæ­¥å’Œåˆ†åº“åˆ†è¡¨åŒæ­¥çš„åŠŸèƒ½ã€‚ æœ¬æ•™ç¨‹çš„æ¼”ç¤ºéƒ½å°†åœ¨ Flink CDC CLI ä¸­è¿›è¡Œï¼Œæ— éœ€ä¸€è¡Œ Java/Scala ä»£ç ï¼Œä¹Ÿæ— éœ€å®‰è£… IDEã€‚&#xA;å‡†å¤‡é˜¶æ®µ # å‡†å¤‡ä¸€å°å·²ç»å®‰è£…äº† Docker çš„ Linux æˆ–è€… MacOS ç”µè„‘ã€‚&#xA;å‡†å¤‡ Flink Standalone é›†ç¾¤ # ä¸‹è½½ Flink 1.18.0ï¼Œè§£å‹åå¾—åˆ° flink-1.18.0 ç›®å½•ã€‚ ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤è·³è½¬è‡³ Flink ç›®å½•ä¸‹ï¼Œå¹¶ä¸”è®¾ç½® FLINK_HOME ä¸º flink-1.18.0 æ‰€åœ¨ç›®å½•ã€‚&#xA;cd flink-1.18.0 é€šè¿‡åœ¨ conf/flink-conf.yaml é…ç½®æ–‡ä»¶è¿½åŠ ä¸‹åˆ—å‚æ•°å¼€å¯ checkpointï¼Œæ¯éš” 3 ç§’åšä¸€æ¬¡ checkpointã€‚&#xA;execution.checkpointing.interval: 3000 ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å¯åŠ¨ Flink é›†ç¾¤ã€‚&#xA;./bin/start-cluster.sh å¯åŠ¨æˆåŠŸçš„è¯ï¼Œå¯ä»¥åœ¨ http://localhost:8081/è®¿é—®åˆ° Flink Web UIï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š</description>
    </item>
    <item>
      <title>Standalone</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/deployment/standalone/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/deployment/standalone/</guid>
      <description>Introduction # Standalone mode is Flinkâ€™s simplest deployment mode. This short guide will show you how to download the latest stable version of Flink, install, and run it. You will also run an example Flink CDC job and view it in the web UI.&#xA;Preparation # Flink runs on all UNIX-like environments, i.e. Linux, Mac OS X, and Cygwin (for Windows).&#xA;You can refer overview to check supported versions and download the binary release of Flink, then extract the archive:</description>
    </item>
    <item>
      <title>æ¦‚è§ˆ</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/overview/</guid>
      <description>Flink Sources è¿æ¥å™¨ # Flink CDC sources is a set of source connectors for Apache FlinkÂ®, ingesting changes from different databases using change data capture (CDC). Some CDC sources integrate Debezium as the engine to capture data changes. So it can fully leverage the ability of Debezium. See more about what is Debezium.&#xA;You can also read tutorials about how to use these sources.&#xA;Supported Connectors # Connector Database Driver mongodb-cdc MongoDB: 3.</description>
    </item>
    <item>
      <title>æ¦‚è§ˆ</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/overview/</guid>
      <description>Pipeline Connectors # Flink CDC æä¾›äº†å¯ç”¨äº YAML ä½œä¸šçš„ Pipeline Source å’Œ Sink è¿æ¥å™¨æ¥ä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’ã€‚æ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™äº›è¿æ¥å™¨ï¼Œåªéœ€å°† JAR æ–‡ä»¶æ·»åŠ åˆ°æ‚¨çš„ Flink CDC ç¯å¢ƒä¸­ï¼Œå¹¶åœ¨æ‚¨çš„ YAML Pipeline å®šä¹‰ä¸­æŒ‡å®šæ‰€éœ€çš„è¿æ¥å™¨ã€‚&#xA;Supported Connectors # è¿æ¥å™¨ ç±»å‹ æ”¯æŒçš„å¤–éƒ¨ç³»ç»Ÿ Apache Doris Sink Apache Doris: 1.2.x, 2.x.x Kafka Sink Kafka MySQL Source MySQL: 5.6, 5.7, 8.0.x RDS MySQL: 5.6, 5.7, 8.0.x PolarDB MySQL: 5.6, 5.7, 8.0.x Aurora MySQL: 5.6, 5.7, 8.0.x MariaDB: 10.x PolarDB X: 2.0.1 Paimon Sink Paimon: 0.6, 0.7, 0.8 StarRocks Sink StarRocks: 2.</description>
    </item>
    <item>
      <title>ç†è§£ Flink CDC API</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/developer-guide/understand-flink-cdc-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/developer-guide/understand-flink-cdc-api/</guid>
      <description>Understand Flink CDC API # If you are planning to build your own Flink CDC connectors, or considering contributing to Flink CDC, you might want to hava a deeper look at the APIs of Flink CDC. This document will go through some important concepts and interfaces in order to help you with your development.&#xA;Event # An event under the context of Flink CDC is a special kind of record in Flink&amp;rsquo;s data stream.</description>
    </item>
    <item>
      <title>é€šç”¨FAQ</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/faq/faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/faq/faq/</guid>
      <description>é€šç”¨FAQ # Q1: ä¸ºå•¥æ²¡æ³•ä¸‹è½½ flink-sql-connector-mysql-cdc-2.2-SNAPSHOT.jar ï¼Œmaven ä»“åº“ä¸ºå•¥æ²¡æœ‰ xxx-SNAPSHOT ä¾èµ–ï¼Ÿ # å’Œä¸»æµçš„ maven é¡¹ç›®ç‰ˆæœ¬ç®¡ç†ç›¸åŒï¼Œxxx-SNAPSHOT ç‰ˆæœ¬éƒ½æ˜¯å¯¹åº”å¼€å‘åˆ†æ”¯çš„ä»£ç ï¼Œéœ€è¦ç”¨æˆ·è‡ªå·±ä¸‹è½½æºç å¹¶ç¼–è¯‘å¯¹åº”çš„jarï¼Œ ç”¨æˆ·åº”è¯¥ä½¿ç”¨å·²ç» release è¿‡çš„ç‰ˆæœ¬ï¼Œæ¯”å¦‚ flink-sql-connector-mysql-cdc-2.1.0.jarï¼Œrelease è¿‡çš„ç‰ˆæœ¬mavenä¸­å¿ƒä»“åº“æ‰ä¼šæœ‰ã€‚&#xA;Q2: ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ flink-sql-connector-xxx.jarï¼Œä»€ä¹ˆæ—¶å€™ä½¿ç”¨ flink-connector-xxx.jarï¼Œä¸¤è€…æœ‰å•¥åŒºåˆ«? # Flink CDC é¡¹ç›®ä¸­å„ä¸ªconnectorçš„ä¾èµ–ç®¡ç†å’ŒFlink é¡¹ç›®ä¸­ connector ä¿æŒä¸€è‡´ã€‚flink-sql-connector-xx æ˜¯èƒ–åŒ…ï¼Œé™¤äº†connectorçš„ä»£ç å¤–ï¼Œè¿˜æŠŠ connector ä¾èµ–çš„æ‰€æœ‰ä¸‰æ–¹åŒ… shade åæ‰“å…¥ï¼Œæä¾›ç»™ SQL ä½œä¸šä½¿ç”¨ï¼Œç”¨æˆ·åªéœ€è¦åœ¨ libç›®å½•ä¸‹æ·»åŠ è¯¥èƒ–åŒ…å³å¯ã€‚flink-connector-xx åªæœ‰è¯¥ connector çš„ä»£ç ï¼Œä¸åŒ…å«å…¶æ‰€éœ€çš„ä¾èµ–ï¼Œæä¾› datastream ä½œä¸šä½¿ç”¨ï¼Œç”¨æˆ·éœ€è¦è‡ªå·±ç®¡ç†æ‰€éœ€çš„ä¸‰æ–¹åŒ…ä¾èµ–ï¼Œæœ‰å†²çªçš„ä¾èµ–éœ€è¦è‡ªå·±åš exclude, shade å¤„ç†ã€‚&#xA;Q3: ä¸ºå•¥æŠŠåŒ…åä» com.alibaba.ververica æ”¹æˆ org.apache.flink? ä¸ºå•¥ maven ä»“åº“é‡Œæ‰¾ä¸åˆ° 2.x ç‰ˆæœ¬ï¼Ÿ # Flink CDC é¡¹ç›® ä» 2.0.0 ç‰ˆæœ¬å°† group id ä»com.alibaba.ververica æ”¹æˆ com.ververica, è‡ª 3.</description>
    </item>
    <item>
      <title>é¡¹ç›®ä»‹ç»</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/get-started/introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/get-started/introduction/</guid>
      <description>æ¬¢è¿ä½¿ç”¨ Flink CDC ğŸ‰ # Flink CDC æ˜¯ä¸€ä¸ªåŸºäºæµçš„æ•°æ®é›†æˆå·¥å…·ï¼Œæ—¨åœ¨ä¸ºç”¨æˆ·æä¾›ä¸€å¥—åŠŸèƒ½æ›´åŠ å…¨é¢çš„ç¼–ç¨‹æ¥å£ï¼ˆAPIï¼‰ã€‚ è¯¥å·¥å…·ä½¿å¾—ç”¨æˆ·èƒ½å¤Ÿä»¥ YAML é…ç½®æ–‡ä»¶çš„å½¢å¼ï¼Œä¼˜é›…åœ°å®šä¹‰å…¶ ETLï¼ˆExtract, Transform, Loadï¼‰æµç¨‹ï¼Œå¹¶ååŠ©ç”¨æˆ·è‡ªåŠ¨åŒ–ç”Ÿæˆå®šåˆ¶åŒ–çš„ Flink ç®—å­å¹¶ä¸”æäº¤ Flink ä½œä¸šã€‚ Flink CDC åœ¨ä»»åŠ¡æäº¤è¿‡ç¨‹ä¸­è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå¹¶ä¸”å¢åŠ äº†ä¸€äº›é«˜çº§ç‰¹æ€§ï¼Œå¦‚è¡¨ç»“æ„å˜æ›´è‡ªåŠ¨åŒæ­¥ï¼ˆSchema Evolutionï¼‰ã€æ•°æ®è½¬æ¢ï¼ˆData Transformationï¼‰ã€æ•´åº“åŒæ­¥ï¼ˆFull Database Synchronizationï¼‰ä»¥åŠ ç²¾ç¡®ä¸€æ¬¡ï¼ˆExactly-onceï¼‰è¯­ä¹‰ã€‚&#xA;Flink CDC æ·±åº¦é›†æˆå¹¶ç”± Apache Flink é©±åŠ¨ï¼Œæä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š&#xA;âœ… ç«¯åˆ°ç«¯çš„æ•°æ®é›†æˆæ¡†æ¶ âœ… ä¸ºæ•°æ®é›†æˆçš„ç”¨æˆ·æä¾›äº†æ˜“äºæ„å»ºä½œä¸šçš„ API âœ… æ”¯æŒåœ¨ Source å’Œ Sink ä¸­å¤„ç†å¤šä¸ªè¡¨ âœ… æ•´åº“åŒæ­¥ âœ…å…·å¤‡è¡¨ç»“æ„å˜æ›´è‡ªåŠ¨åŒæ­¥çš„èƒ½åŠ›ï¼ˆSchema Evolutionï¼‰ï¼Œ å¦‚ä½•ä½¿ç”¨ Flink CDC # Flink CDC æä¾›äº†åŸºäº YAML æ ¼å¼çš„ç”¨æˆ· APIï¼Œæ›´é€‚åˆäºæ•°æ®é›†æˆåœºæ™¯ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ª YAML æ–‡ä»¶çš„ç¤ºä¾‹ï¼Œå®ƒå®šä¹‰äº†ä¸€ä¸ªæ•°æ®ç®¡é“(Pipeline)ï¼Œè¯¥Pipelineä» MySQL æ•è·å®æ—¶å˜æ›´ï¼Œå¹¶å°†å®ƒä»¬åŒæ­¥åˆ° Apache Dorisï¼š&#xA;source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.</description>
    </item>
    <item>
      <title>Data Source</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/core-concept/data-source/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/core-concept/data-source/</guid>
      <description>Definition # Data Source is used to access metadata and read the changed data from external systems.&#xA;A Data Source can read data from multiple tables simultaneously.&#xA;Parameters # To describe a data source, the follows are required:&#xA;parameter meaning optional/required type The type of the source, such as mysql. required name The name of the source, which is user-defined (a default value provided). optional configurations of Data Source Configurations to build the Data Source e.</description>
    </item>
    <item>
      <title>Db2 æ•™ç¨‹</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/db2-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/db2-tutorial/</guid>
      <description>Demo: Db2 CDC to Elasticsearch # 1. Create docker-compose.yml file using following contents:&#xA;version: &amp;#39;2.1&amp;#39; services: db2: image: ruanhang/db2-cdc-demo:v1 privileged: true ports: - 50000:50000 environment: - LICENSE=accept - DB2INSTANCE=db2inst1 - DB2INST1_PASSWORD=admin - DBNAME=testdb - ARCHIVE_LOGS=true elasticsearch: image: elastic/elasticsearch:7.6.0 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &amp;#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&amp;#34; - discovery.type=single-node ports: - &amp;#34;9200:9200&amp;#34; - &amp;#34;9300:9300&amp;#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 ports: - &amp;#34;5601:5601&amp;#34; volumes: - /var/run/docker.</description>
    </item>
    <item>
      <title>MySQL</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/mysql-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/mysql-cdc/</guid>
      <description>MySQL CDC è¿æ¥å™¨ # MySQL CDC è¿æ¥å™¨å…è®¸ä» MySQL æ•°æ®åº“è¯»å–å¿«ç…§æ•°æ®å’Œå¢é‡æ•°æ®ã€‚æœ¬æ–‡æè¿°äº†å¦‚ä½•è®¾ç½® MySQL CDC è¿æ¥å™¨æ¥å¯¹ MySQL æ•°æ®åº“è¿è¡Œ SQL æŸ¥è¯¢ã€‚&#xA;æ”¯æŒçš„æ•°æ®åº“ # Connector Database Driver mysql-cdc MySQL: 5.6, 5.7, 8.0.x RDS MySQL: 5.6, 5.7, 8.0.x PolarDB MySQL: 5.6, 5.7, 8.0.x Aurora MySQL: 5.6, 5.7, 8.0.x MariaDB: 10.x PolarDB X: 2.0.1 JDBC Driver: 8.0.27 ä¾èµ– # ä¸ºäº†è®¾ç½® MySQL CDC è¿æ¥å™¨ï¼Œä¸‹è¡¨æä¾›äº†ä½¿ç”¨æ„å»ºè‡ªåŠ¨åŒ–å·¥å…·ï¼ˆå¦‚ Maven æˆ– SBT ï¼‰å’Œå¸¦æœ‰ SQL JAR åŒ…çš„ SQL å®¢æˆ·ç«¯çš„ä¸¤ä¸ªé¡¹ç›®çš„ä¾èµ–å…³ç³»ä¿¡æ¯ã€‚&#xA;Maven dependency # &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-connector-mysql-cdc&amp;lt;/artifactId&amp;gt; &amp;lt;!</description>
    </item>
    <item>
      <title>MySQL</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/mysql/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/mysql/</guid>
      <description>MySQL Connector # MySQL CDC Pipeline è¿æ¥å™¨å…è®¸ä» MySQL æ•°æ®åº“è¯»å–å¿«ç…§æ•°æ®å’Œå¢é‡æ•°æ®ï¼Œå¹¶æä¾›ç«¯åˆ°ç«¯çš„æ•´åº“æ•°æ®åŒæ­¥èƒ½åŠ›ã€‚ æœ¬æ–‡æè¿°äº†å¦‚ä½•è®¾ç½® MySQL CDC Pipeline è¿æ¥å™¨ã€‚&#xA;ä¾èµ–é…ç½® # ç”±äº MySQL Connector é‡‡ç”¨çš„ GPLv2 åè®®ä¸ Flink CDC é¡¹ç›®ä¸å…¼å®¹ï¼Œæˆ‘ä»¬æ— æ³•åœ¨ jar åŒ…ä¸­æä¾› MySQL è¿æ¥å™¨ã€‚ æ‚¨å¯èƒ½éœ€è¦æ‰‹åŠ¨é…ç½®ä»¥ä¸‹ä¾èµ–ï¼Œå¹¶åœ¨æäº¤ YAML ä½œä¸šæ—¶ä½¿ç”¨ Flink CDC CLI çš„ --jar å‚æ•°å°†å…¶ä¼ å…¥ï¼š&#xA;ä¾èµ–åç§° è¯´æ˜ mysql:mysql-connector-java:8.0.27 ç”¨äºè¿æ¥åˆ° MySQL æ•°æ®åº“ã€‚ ç¤ºä¾‹ # ä» MySQL è¯»å–æ•°æ®åŒæ­¥åˆ° Doris çš„ Pipeline å¯ä»¥å®šä¹‰å¦‚ä¸‹ï¼š&#xA;source: type: mysql name: MySQL Source hostname: 127.0.0.1 port: 3306 username: admin password: pass tables: adb.\.*, bdb.</description>
    </item>
    <item>
      <title>MySQL åŒæ­¥åˆ° StarRocks</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/get-started/quickstart/mysql-to-starrocks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/get-started/quickstart/mysql-to-starrocks/</guid>
      <description>Streaming ELT åŒæ­¥ MySQL åˆ° StarRocks # è¿™ç¯‡æ•™ç¨‹å°†å±•ç¤ºå¦‚ä½•åŸºäº Flink CDC å¿«é€Ÿæ„å»º MySQL åˆ° StarRocks çš„ Streaming ELT ä½œä¸šï¼ŒåŒ…å«æ•´åº“åŒæ­¥ã€è¡¨ç»“æ„å˜æ›´åŒæ­¥å’Œåˆ†åº“åˆ†è¡¨åŒæ­¥çš„åŠŸèƒ½ã€‚&#xA;æœ¬æ•™ç¨‹çš„æ¼”ç¤ºéƒ½å°†åœ¨ Flink CDC CLI ä¸­è¿›è¡Œï¼Œæ— éœ€ä¸€è¡Œ Java/Scala ä»£ç ï¼Œä¹Ÿæ— éœ€å®‰è£… IDEã€‚&#xA;å‡†å¤‡é˜¶æ®µ # å‡†å¤‡ä¸€å°å·²ç»å®‰è£…äº† Docker çš„ Linux æˆ–è€… MacOS ç”µè„‘ã€‚&#xA;å‡†å¤‡ Flink Standalone é›†ç¾¤ # ä¸‹è½½ Flink 1.18.0 ï¼Œè§£å‹åå¾—åˆ° flink-1.18.0 ç›®å½•ã€‚&#xA;ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤è·³è½¬è‡³ Flink ç›®å½•ä¸‹ï¼Œå¹¶ä¸”è®¾ç½® FLINK_HOME ä¸º flink-1.18.0 æ‰€åœ¨ç›®å½•ã€‚&#xA;cd flink-1.18.0 é€šè¿‡åœ¨ conf/flink-conf.yaml é…ç½®æ–‡ä»¶è¿½åŠ ä¸‹åˆ—å‚æ•°å¼€å¯ checkpointï¼Œæ¯éš” 3 ç§’åšä¸€æ¬¡ checkpointã€‚&#xA;execution.checkpointing.interval: 3000 ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å¯åŠ¨ Flink é›†ç¾¤ã€‚&#xA;./bin/start-cluster.sh å¯åŠ¨æˆåŠŸçš„è¯ï¼Œå¯ä»¥åœ¨ http://localhost:8081/ è®¿é—®åˆ° Flink Web UIï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š</description>
    </item>
    <item>
      <title>YARN</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/deployment/yarn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/deployment/yarn/</guid>
      <description>Introduction # Apache Hadoop YARN is a resource provider popular with many data processing frameworks. Flink services are submitted to YARN&amp;rsquo;s ResourceManager, which spawns containers on machines managed by YARN NodeManagers. Flink deploys its JobManager and TaskManager instances into such containers.&#xA;Flink can dynamically allocate and de-allocate TaskManager resources depending on the number of processing slots required by the job(s) running on the JobManager.&#xA;Preparation # This Getting Started section assumes a functional YARN environment, starting from version 2.</description>
    </item>
    <item>
      <title>å‘ Flink CDC æäº¤è´¡çŒ®</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/developer-guide/contribute-to-flink-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/developer-guide/contribute-to-flink-cdc/</guid>
      <description>ç¤¾åŒºè´¡çŒ® # Flink CDC ç”±å¼€æ”¾å’Œå‹å¥½çš„ç¤¾åŒºå¼€å‘è€Œæ¥ï¼Œæ¬¢è¿ä»»ä½•æƒ³è¦æä¾›å¸®åŠ©çš„è´¡çŒ®è€…ã€‚æœ‰ä»¥ä¸‹ä¸€äº›æ–¹å¼å¯ä»¥è®©è´¡çŒ®è€…å’Œç¤¾åŒºäº¤æµå’Œåšå‡ºè´¡çŒ®ï¼ŒåŒ…æ‹¬æé—®ï¼Œæäº¤å‘ç°çš„ BugæŠ¥å‘Šï¼Œæè®®æ–°çš„åŠŸèƒ½ï¼ŒåŠ å…¥ç¤¾åŒºé‚®ä»¶åˆ—è¡¨çš„è®¨è®ºï¼Œè´¡çŒ®ä»£ç æˆ–æ–‡æ¡£ï¼Œæ”¹è¿›é¡¹ç›®ç½‘ç«™ï¼Œå‘ç‰ˆå‰æµ‹è¯•å’Œç¼–å†™Blogç­‰ã€‚&#xA;ä½ æƒ³è¦è´¡çŒ®ä»€ä¹ˆï¼Ÿ # Flink CDC ç¤¾åŒºçš„è´¡çŒ®ä¸ä»…é™äºä¸ºé¡¹ç›®è´¡çŒ®ä»£ç ï¼Œä¸‹é¢åˆ—ä¸¾äº†ä¸€äº›å¯ä»¥åœ¨ç¤¾åŒºè´¡çŒ®çš„å†…å®¹ã€‚&#xA;è´¡çŒ®æ–¹å¼ æ›´å¤šä¿¡æ¯ æäº¤BUG ä¸ºäº†æäº¤é—®é¢˜ï¼Œæ‚¨éœ€è¦é¦–å…ˆåœ¨ Flink jira å»ºç«‹å¯¹åº”çš„issueï¼Œå¹¶åœ¨Component/sé€‰æ‹©Flink CDCã€‚ç„¶ååœ¨é—®é¢˜æè¿°ä¸­è¯¦ç»†æè¿°é‡åˆ°çš„é—®é¢˜çš„ä¿¡æ¯ï¼Œå¦‚æœå¯èƒ½çš„è¯ï¼Œæœ€å¥½æä¾›ä¸€ä¸‹èƒ½å¤Ÿå¤ç°é—®é¢˜çš„æ“ä½œæ­¥éª¤ã€‚ è´¡çŒ®ä»£ç  è¯·é˜…è¯» è´¡çŒ®ä»£ç æŒ‡å¯¼ ä»£ç è¯„å®¡ è¯·é˜…è¯» ä»£ç è¯„å®¡æŒ‡å¯¼ ç”¨æˆ·æ”¯æŒ é€šè¿‡ Flink ç”¨æˆ·é‚®ä»¶åˆ—è¡¨ æ¥å¸®åŠ©å›å¤ç”¨æˆ·é—®é¢˜ï¼Œåœ¨ Flink jira å¯ä»¥æŸ¥è¯¢åˆ°æœ€æ–°çš„å·²çŸ¥é—®é¢˜ã€‚ å¦‚æœè¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡ Flink Dev é‚®ä»¶åˆ—è¡¨å¯»æ±‚å¸®åŠ©ã€‚&#xA;è´¡çŒ®ä»£ç æŒ‡å¯¼ Flink CDC é¡¹ç›®é€šè¿‡ä¼—å¤šè´¡çŒ®è€…çš„ä»£ç è´¡çŒ®æ¥ç»´æŠ¤ï¼Œæ”¹è¿›å’Œæ‹“å±•ï¼Œæ¬¢è¿å„ç§å½¢å¼çš„ç¤¾åŒºè´¡çŒ®ã€‚&#xA;åœ¨ Flink CDC ç¤¾åŒºå¯ä»¥è‡ªç”±çš„åœ¨ä»»ä½•æ—¶é—´æå‡ºè‡ªå·±çš„é—®é¢˜ï¼Œé€šè¿‡ç¤¾åŒº Dev é‚®ä»¶åˆ—è¡¨è¿›è¡Œäº¤æµæˆ–åœ¨ä»»ä½•æ„Ÿå…´è¶£çš„ issue ä¸‹è¯„è®ºå’Œè®¨è®ºã€‚&#xA;å¦‚æœæ‚¨æƒ³è¦ä¸º Flink CDC è´¡çŒ®ä»£ç ï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹çš„æ–¹å¼ã€‚&#xA;é¦–å…ˆåœ¨ Flink jira çš„æƒ³è¦è´Ÿè´£çš„ issue ä¸‹è¯„è®ºï¼ˆæœ€å¥½åœ¨è¯„è®ºä¸­è§£é‡Šä¸‹å¯¹äºè¿™ä¸ªé—®é¢˜çš„ç†è§£ï¼Œå’Œåç»­çš„è®¾è®¡ï¼Œå¦‚æœå¯èƒ½çš„è¯ä¹Ÿå¯ä»¥æä¾›ä¸‹ POC çš„ä»£ç ï¼‰ã€‚ åœ¨è¿™ä¸ª issue è¢«åˆ†é…ç»™ä½ åï¼Œå¼€å§‹è¿›è¡Œå¼€å‘å®ç°ï¼ˆæäº¤ä¿¡æ¯è¯·éµå¾ª[FLINK-xxx][xxx] xxxxxxxçš„æ ¼å¼ï¼‰ã€‚ å¼€å‘å®Œæˆåå¯ä»¥å‘ Flink CDC é¡¹ç›®æäº¤ PRï¼ˆè¯·ç¡®ä¿ Clone çš„é¡¹ç›® committer æœ‰æ“ä½œæƒé™ï¼‰ã€‚ æ‰¾åˆ°ä¸€ä¸ªå¼€å‘è€…å¸®å¿™è¯„å®¡ä»£ç ï¼Œè¯„å®¡å‰è¯·ç¡®ä¿ CI é€šè¿‡ã€‚ Flink committer ç¡®è®¤ä»£ç è´¡çŒ®æ»¡è¶³å…¨éƒ¨è¦æ±‚åï¼Œä»£ç ä¼šè¢«åˆå¹¶åˆ°ä»£ç ä»“åº“ã€‚ ä»£ç è¯„å®¡æŒ‡å¯¼ æ¯ä¸€æ¬¡çš„ä»£ç è¯„å®¡éœ€è¦æ£€æŸ¥å¦‚ä¸‹ä¸€äº›æ–¹é¢çš„å†…å®¹ã€‚</description>
    </item>
    <item>
      <title>Data Sink</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/core-concept/data-sink/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/core-concept/data-sink/</guid>
      <description>Definition # Data Sink is used to apply schema changes and write change data to external systems. A Data Sink can write to multiple tables simultaneously.&#xA;Parameters # To describe a data sink, the follows are required:&#xA;parameter meaning optional/required type The type of the sink, such as doris or starrocks. required name The name of the sink, which is user-defined (a default value provided). optional configurations of Data Sink Configurations to build the Data Sink e.</description>
    </item>
    <item>
      <title>Kubernetes</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/deployment/kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/deployment/kubernetes/</guid>
      <description>Introduction # Kubernetes is a popular container-orchestration system for automating computer application deployment, scaling, and management. Flink&amp;rsquo;s native Kubernetes integration allows you to directly deploy Flink on a running Kubernetes cluster. Moreover, Flink is able to dynamically allocate and de-allocate TaskManagers depending on the required resources because it can directly talk to Kubernetes.&#xA;Apache Flink also provides a Kubernetes operator for managing Flink clusters on Kubernetes. It supports both standalone and native deployment mode and greatly simplifies deployment, configuration and the life cycle management of Flink resources on Kubernetes.</description>
    </item>
    <item>
      <title>OceanBase æ•™ç¨‹</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/oceanbase-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/oceanbase-tutorial/</guid>
      <description>æ¼”ç¤º: OceanBase CDC å¯¼å…¥ Elasticsearch # è§†é¢‘æ•™ç¨‹ # YouTube Bilibili å‡†å¤‡æ•™ç¨‹æ‰€éœ€è¦çš„ç»„ä»¶ # é…ç½®å¹¶å¯åŠ¨å®¹å™¨ # é…ç½® docker-compose.ymlã€‚&#xA;version: &amp;#39;2.1&amp;#39; services: observer: image: oceanbase/oceanbase-ce:4.0.0.0 container_name: observer network_mode: &amp;#34;host&amp;#34; oblogproxy: image: whhe/oblogproxy:1.1.0_4x container_name: oblogproxy environment: - &amp;#39;OB_SYS_USERNAME=root&amp;#39; - &amp;#39;OB_SYS_PASSWORD=pswd&amp;#39; network_mode: &amp;#34;host&amp;#34; elasticsearch: image: &amp;#39;elastic/elasticsearch:7.6.0&amp;#39; container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - ES_JAVA_OPTS=-Xms512m -Xmx512m - discovery.type=single-node ports: - &amp;#39;9200:9200&amp;#39; - &amp;#39;9300:9300&amp;#39; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: &amp;#39;elastic/kibana:7.</description>
    </item>
    <item>
      <title>Oracle</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/oracle-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/oracle-cdc/</guid>
      <description>Oracle CDC Connector # The Oracle CDC connector allows for reading snapshot data and incremental data from Oracle database. This document describes how to setup the Oracle CDC connector to run SQL queries against Oracle databases.&#xA;Dependencies # In order to setup the Oracle CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    <item>
      <title>Paimon</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/paimon/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/paimon/</guid>
      <description>Paimon Pipeline è¿æ¥å™¨ # Paimon Pipeline è¿æ¥å™¨å¯ä»¥ç”¨ä½œ Pipeline çš„ Data Sinkï¼Œå°†æ•°æ®å†™å…¥Paimonã€‚ æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•è®¾ç½® Paimon Pipeline è¿æ¥å™¨ã€‚&#xA;è¿æ¥å™¨çš„åŠŸèƒ½ # è‡ªåŠ¨å»ºè¡¨ è¡¨ç»“æ„å˜æ›´åŒæ­¥ æ•°æ®å®æ—¶åŒæ­¥ å¦‚ä½•åˆ›å»º Pipeline # ä» MySQL è¯»å–æ•°æ®åŒæ­¥åˆ° Paimon çš„ Pipeline å¯ä»¥å®šä¹‰å¦‚ä¸‹ï¼š&#xA;source: type: mysql name: MySQL Source hostname: 127.0.0.1 port: 3306 username: admin password: pass tables: adb.\.*, bdb.user_table_[0-9]+, [app|web].order_\.* server-id: 5401-5404 sink: type: paimon name: Paimon Sink catalog.properties.metastore: filesystem catalog.properties.warehouse: /path/warehouse pipeline: name: MySQL to Paimon Pipeline parallelism: 2 Pipeline è¿æ¥å™¨é…ç½®é¡¹ # Option Required Default Type Description type required (none) String æŒ‡å®šè¦ä½¿ç”¨çš„è¿æ¥å™¨, è¿™é‡Œéœ€è¦è®¾ç½®æˆ &#39;paimon&#39;.</description>
    </item>
    <item>
      <title>è®¸å¯è¯</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/developer-guide/licenses/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/developer-guide/licenses/</guid>
      <description> è®¸å¯è¯ # Flink CDC ä½¿ç”¨çš„æ˜¯ Apache License 2.0 è®¸å¯ã€‚&#xA;å¦‚æœå¯¹äº Flink CDC çš„è®¸å¯è¯æœ‰é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡é‚®ä»¶åˆ—è¡¨è”ç³»æˆ‘ä»¬ã€‚&#xA;Apache Software Foundation # æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸€äº›é“¾æ¥äº†è§£æ›´å¤šå…³äº ASF çš„ä¿¡æ¯ã€‚&#xA;Apache Software Foundation License Events Security SponSprShip Thanks Privacy </description>
    </item>
    <item>
      <title>Kafka</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/kafka/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/kafka/</guid>
      <description>Kafka Pipeline è¿æ¥å™¨ # Kafka Pipeline è¿æ¥å™¨å¯ä»¥ç”¨ä½œ Pipeline çš„ Data Sinkï¼Œå°†æ•°æ®å†™å…¥Kafkaã€‚ æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•è®¾ç½® Kafka Pipeline è¿æ¥å™¨ã€‚&#xA;è¿æ¥å™¨çš„åŠŸèƒ½ # è‡ªåŠ¨å»ºè¡¨ è¡¨ç»“æ„å˜æ›´åŒæ­¥ æ•°æ®å®æ—¶åŒæ­¥ å¦‚ä½•åˆ›å»º Pipeline # ä» MySQL è¯»å–æ•°æ®åŒæ­¥åˆ° Kafka çš„ Pipeline å¯ä»¥å®šä¹‰å¦‚ä¸‹ï¼š&#xA;source: type: mysql name: MySQL Source hostname: 127.0.0.1 port: 3306 username: admin password: pass tables: adb.\.*, bdb.user_table_[0-9]+, [app|web].order_\.* server-id: 5401-5404 sink: type: kafka name: Kafka Sink properties.bootstrap.servers: PLAINTEXT://localhost:62510 pipeline: name: MySQL to Kafka Pipeline parallelism: 2 Pipeline è¿æ¥å™¨é…ç½®é¡¹ # Option Required Default Type Description type required (none) String æŒ‡å®šè¦ä½¿ç”¨çš„è¿æ¥å™¨, è¿™é‡Œéœ€è¦è®¾ç½®æˆ &#39;kafka&#39;ã€‚ name optional (none) String Sink çš„åç§°ã€‚ value.</description>
    </item>
    <item>
      <title>Oracle æ•™ç¨‹</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/oracle-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/oracle-tutorial/</guid>
      <description>æ¼”ç¤º: Oracle CDC å¯¼å…¥ Elasticsearch # åˆ›å»ºdocker-compose.ymlæ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹æ‰€ç¤º:&#xA;version: &amp;#39;2.1&amp;#39; services: oracle: image: goodboy008/oracle-19.3.0-ee:non-cdb ports: - &amp;#34;1521:1521&amp;#34; elasticsearch: image: elastic/elasticsearch:7.6.0 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &amp;#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&amp;#34; - discovery.type=single-node ports: - &amp;#34;9200:9200&amp;#34; - &amp;#34;9300:9300&amp;#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 ports: - &amp;#34;5601:5601&amp;#34; volumes: - /var/run/docker.sock:/var/run/docker.sock è¯¥ Docker Compose ä¸­åŒ…å«çš„å®¹å™¨æœ‰:&#xA;Oracle: Oracle 19c æ•°æ®åº“ Elasticsearch: orders è¡¨å°†å’Œ products è¡¨è¿›è¡Œjoinï¼Œjoinçš„ç»“æœå†™å…¥Elasticsearchä¸­ Kibana: å¯è§†åŒ– Elasticsearch ä¸­çš„æ•°æ® åœ¨ docker-compose.</description>
    </item>
    <item>
      <title>SQL Server</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/sqlserver-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/sqlserver-cdc/</guid>
      <description>SQLServer CDC Connector # The SQLServer CDC connector allows for reading snapshot data and incremental data from SQLServer database. This document describes how to setup the SQLServer CDC connector to run SQL queries against SQLServer databases.&#xA;Dependencies # In order to setup the SQLServer CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    <item>
      <title>Table ID</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/core-concept/table-id/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/core-concept/table-id/</guid>
      <description>Definition # When connecting to external systems, it is necessary to establish a mapping relationship with the storage objects of the external system. This is what Table Id refers to.&#xA;Example # To be compatible with most external systems, the Table Id is represented by a 3-tuple : (namespace, schemaName, tableName).&#xA;Connectors should establish the mapping between Table Id and storage objects in external systems.&#xA;The following table lists the parts in table Id of different data systems:</description>
    </item>
    <item>
      <title>Doris</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/doris/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/doris/</guid>
      <description>Doris Connector # æœ¬æ–‡ä»‹ç»äº†Pipeline Doris Connectorçš„ä½¿ç”¨ã€‚&#xA;ç¤ºä¾‹ # source: type: values name: ValuesSource sink: type: doris name: Doris Sink fenodes: 127.0.0.1:8030 username: root password: &amp;#34;&amp;#34; table.create.properties.replication_num: 1 pipeline: parallelism: 1 è¿æ¥å™¨é…ç½®é¡¹ # Option Required Default Type Description type required (none) String æŒ‡å®šè¦ä½¿ç”¨çš„Sink, è¿™é‡Œæ˜¯ &#39;doris&#39;. name optional (none) String PipeLineçš„åç§° fenodes required (none) String Dorisé›†ç¾¤FEçš„Httpåœ°å€, æ¯”å¦‚ 127.0.0.1:8030 benodes optional (none) String Dorisé›†ç¾¤BEçš„Httpåœ°å€, æ¯”å¦‚ 127.0.0.1:8040 jdbc-url optional (none) String Dorisé›†ç¾¤çš„JDBCåœ°å€ï¼Œæ¯”å¦‚ï¼šjdbc:mysql://127.0.0.1:9030/db username required (none) String Dorisé›†ç¾¤çš„ç”¨æˆ·å password optional (none) String Dorisé›†ç¾¤çš„å¯†ç  auto-redirect optional false String æ˜¯å¦é€šè¿‡FEé‡å®šå‘å†™å…¥ï¼Œç›´è¿BEå†™å…¥ sink.</description>
    </item>
    <item>
      <title>PolarDB-X æ•™ç¨‹</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/polardbx-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/polardbx-tutorial/</guid>
      <description>æ¼”ç¤º: PolarDB-X CDC å¯¼å…¥ Elasticsearch # æœ¬ç¤ºä¾‹æˆ‘ä»¬é€šè¿‡æ¼”ç¤º PolarDB-X å€ŸåŠ© Flink-CDC å°†æ•°æ®å¯¼å…¥è‡³ Elasticsearch æ¥ä»‹ç» PolarDB-X çš„å¢é‡è®¢é˜…èƒ½åŠ›ï¼Œä½ å¯ä»¥å‰å¾€:PolarDB-X äº†è§£æ›´å¤šç»†èŠ‚ã€‚&#xA;å‡†å¤‡æ•™ç¨‹æ‰€éœ€è¦çš„ç»„ä»¶ # æˆ‘ä»¬å‡è®¾ä½ è¿è¡Œåœ¨ä¸€å° MacOS æˆ–è€… Linux æœºå™¨ä¸Šï¼Œå¹¶ä¸”å·²ç»å®‰è£… docker.&#xA;é…ç½®å¹¶å¯åŠ¨å®¹å™¨ # é…ç½® docker-compose.ymlã€‚&#xA;version: &amp;#39;2.1&amp;#39; services: polardbx: polardbx: image: polardbx/polardb-x:2.0.1 container_name: polardbx ports: - &amp;#34;8527:8527&amp;#34; elasticsearch: image: &amp;#39;elastic/elasticsearch:7.6.0&amp;#39; container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - ES_JAVA_OPTS=-Xms512m -Xmx512m - discovery.type=single-node ports: - &amp;#39;9200:9200&amp;#39; - &amp;#39;9300:9300&amp;#39; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: &amp;#39;elastic/kibana:7.</description>
    </item>
    <item>
      <title>Postgres</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/postgres-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/postgres-cdc/</guid>
      <description>Postgres CDC Connector # The Postgres CDC connector allows for reading snapshot data and incremental data from PostgreSQL database. This document describes how to setup the Postgres CDC connector to run SQL queries against PostgreSQL databases.&#xA;Dependencies # In order to setup the Postgres CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    <item>
      <title>Transform</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/core-concept/transform/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/core-concept/transform/</guid>
      <description>Definition # Transform module helps users delete and expand data columns based on the data columns in the table. What&amp;rsquo;s more, it also helps users filter some unnecessary data during the synchronization process.&#xA;Parameters # To describe a transform rule, the following parameters can be used:&#xA;Parameter Meaning Optional/Required source-table Source table id, supports regular expressions required projection Projection rule, supports syntax similar to the select clause in SQL optional filter Filter rule, supports syntax similar to the where clause in SQL optional primary-keys Sink table primary keys, separated by commas optional partition-keys Sink table partition keys, separated by commas optional table-options used to the configure table creation statement when automatically creating tables optional description Transform rule description optional Multiple rules can be declared in one single pipeline YAML file.</description>
    </item>
    <item>
      <title>MongoDB</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/mongodb-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/mongodb-cdc/</guid>
      <description>MongoDB CDC è¿æ¥å™¨ # MongoDB CDC è¿æ¥å™¨å…è®¸ä» MongoDB è¯»å–å¿«ç…§æ•°æ®å’Œå¢é‡æ•°æ®ã€‚ æœ¬æ–‡æ¡£æè¿°äº†å¦‚ä½•è®¾ç½® MongoDB CDC è¿æ¥å™¨ä»¥é’ˆå¯¹ MongoDB è¿è¡Œ SQL æŸ¥è¯¢ã€‚&#xA;ä¾èµ– # ä¸ºäº†è®¾ç½® MongoDB CDC è¿æ¥å™¨, ä¸‹è¡¨æä¾›äº†ä½¿ç”¨æ„å»ºè‡ªåŠ¨åŒ–å·¥å…·ï¼ˆå¦‚ Maven æˆ– SBT ï¼‰å’Œå¸¦æœ‰ SQLJar æ†ç»‘åŒ…çš„ SQLClient çš„ä¸¤ä¸ªé¡¹ç›®çš„ä¾èµ–å…³ç³»ä¿¡æ¯ã€‚&#xA;Maven dependency # &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-connector-mongodb-cdc&amp;lt;/artifactId&amp;gt; &amp;lt;!-- è¯·ä½¿ç”¨å·²å‘å¸ƒçš„ç‰ˆæœ¬ä¾èµ–ï¼Œsnapshot ç‰ˆæœ¬çš„ä¾èµ–éœ€è¦æœ¬åœ°è‡ªè¡Œç¼–è¯‘ã€‚ --&amp;gt; &amp;lt;version&amp;gt;3.2-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; SQL Client JAR # ä¸‹è½½é“¾æ¥ä»…é€‚ç”¨äºç¨³å®šç‰ˆæœ¬ã€‚&#xA;ä¸‹è½½ flink-sql-connector-mongodb-cdc-3.0.1.jarï¼Œ æŠŠå®ƒæ”¾åœ¨ &amp;lt;FLINK_HOME&amp;gt;/lib/.&#xA;æ³¨æ„: å‚è€ƒ flink-sql-connector-mongodb-cdcï¼Œ å½“å‰å·²å‘å¸ƒçš„ç‰ˆæœ¬å°†åœ¨ Maven ä¸­å¤®ä»“åº“ä¸­æä¾›ã€‚&#xA;è®¾ç½® MongoDB # å¯ç”¨æ€§ # MongoDB ç‰ˆæœ¬&#xA;MongoDB ç‰ˆæœ¬ &amp;gt;= 3.</description>
    </item>
    <item>
      <title>Route</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/core-concept/route/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/core-concept/route/</guid>
      <description>Definition # Route specifies the rule of matching a list of source-table and mapping to sink-table. The most typical scenario is the merge of sub-databases and sub-tables, routing multiple upstream source tables to the same sink table.&#xA;Parameters # To describe a route, the follows are required:&#xA;parameter meaning optional/required source-table Source table id, supports regular expressions required sink-table Sink table id, supports regular expressions required description Routing rule description(a default value provided) optional A route module can contain a list of source-table/sink-table rules.</description>
    </item>
    <item>
      <title>SqlServer æ•™ç¨‹</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/sqlserver-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/sqlserver-tutorial/</guid>
      <description>æ¼”ç¤º: SqlServer CDC å¯¼å…¥ Elasticsearch # åˆ›å»º docker-compose.yml æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹æ‰€ç¤ºï¼š&#xA;version: &amp;#39;2.1&amp;#39; services: sqlserver: image: mcr.microsoft.com/mssql/server:2019-latest container_name: sqlserver ports: - &amp;#34;1433:1433&amp;#34; environment: - &amp;#34;MSSQL_AGENT_ENABLED=true&amp;#34; - &amp;#34;MSSQL_PID=Standard&amp;#34; - &amp;#34;ACCEPT_EULA=Y&amp;#34; - &amp;#34;SA_PASSWORD=Password!&amp;#34; elasticsearch: image: elastic/elasticsearch:7.6.0 container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &amp;#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&amp;#34; - discovery.type=single-node ports: - &amp;#34;9200:9200&amp;#34; - &amp;#34;9300:9300&amp;#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 container_name: kibana ports: - &amp;#34;5601:5601&amp;#34; volumes: - /var/run/docker.</description>
    </item>
    <item>
      <title>StarRocks</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/starrocks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/pipeline-connectors/starrocks/</guid>
      <description>StarRocks Connector # StarRocks Pipeline è¿æ¥å™¨å¯ä»¥ç”¨ä½œ Pipeline çš„ Data Sinkï¼Œå°†æ•°æ®å†™å…¥StarRocksã€‚ æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•è®¾ç½® StarRocks Pipeline è¿æ¥å™¨ã€‚&#xA;è¿æ¥å™¨çš„åŠŸèƒ½ # è‡ªåŠ¨å»ºè¡¨ è¡¨ç»“æ„å˜æ›´åŒæ­¥ æ•°æ®å®æ—¶åŒæ­¥ ç¤ºä¾‹ # ä» MySQL è¯»å–æ•°æ®åŒæ­¥åˆ° StarRocks çš„ Pipeline å¯ä»¥å®šä¹‰å¦‚ä¸‹ï¼š&#xA;source: type: mysql name: MySQL Source hostname: 127.0.0.1 port: 3306 username: admin password: pass tables: adb.\.*, bdb.user_table_[0-9]+, [app|web].order_\.* server-id: 5401-5404 sink: type: starrocks name: StarRocks Sink jdbc-url: jdbc:mysql://127.0.0.1:9030 load-url: 127.0.0.1:8030 username: root password: pass pipeline: name: MySQL to StarRocks Pipeline parallelism: 2 è¿æ¥å™¨é…ç½®é¡¹ # Option Required Default Type Description type required (none) String æŒ‡å®šè¦ä½¿ç”¨çš„è¿æ¥å™¨, è¿™é‡Œéœ€è¦è®¾ç½®æˆ &#39;starrocks&#39;.</description>
    </item>
    <item>
      <title>Db2</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/db2-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/db2-cdc/</guid>
      <description>Db2 CDC Connector # The Db2 CDC connector allows for reading snapshot data and incremental data from Db2 database. This document describes how to setup the db2 CDC connector to run SQL queries against Db2 databases.&#xA;Supported Databases # Connector Database Driver Db2-cdc Db2: 11.5 Db2 Driver: 11.5.0.0 Dependencies # In order to set up the Db2 CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    <item>
      <title>TiDB æ•™ç¨‹</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/tidb-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/tidb-tutorial/</guid>
      <description>æ¼”ç¤º: TiDB CDC å¯¼å…¥ Elasticsearch # é¦–å…ˆæˆ‘ä»¬å¾—é€šè¿‡ docker æ¥å¯åŠ¨ TiDB é›†ç¾¤ã€‚&#xA;$ git clone https://github.com/pingcap/tidb-docker-compose.git å…¶æ¬¡æ›¿æ¢ç›®å½• tidb-docker-compose é‡Œé¢çš„ docker-compose.yml æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹æ‰€ç¤ºï¼š&#xA;version: &amp;#34;2.1&amp;#34; services: pd: image: pingcap/pd:v5.3.1 ports: - &amp;#34;2379:2379&amp;#34; volumes: - ./config/pd.toml:/pd.toml - ./logs:/logs command: - --client-urls=http://0.0.0.0:2379 - --peer-urls=http://0.0.0.0:2380 - --advertise-client-urls=http://pd:2379 - --advertise-peer-urls=http://pd:2380 - --initial-cluster=pd=http://pd:2380 - --data-dir=/data/pd - --config=/pd.toml - --log-file=/logs/pd.log restart: on-failure tikv: image: pingcap/tikv:v5.3.1 ports: - &amp;#34;20160:20160&amp;#34; volumes: - ./config/tikv.toml:/tikv.toml - ./logs:/logs command: - --addr=0.0.0.0:20160 - --advertise-addr=tikv:20160 - --data-dir=/data/tikv - --pd=pd:2379 - --config=/tikv.</description>
    </item>
    <item>
      <title>TiDB</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tidb-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tidb-cdc/</guid>
      <description>TiDB CDC Connector # The TiDB CDC connector allows for reading snapshot data and incremental data from TiDB database. This document describes how to setup the TiDB CDC connector to run SQL queries against TiDB databases.&#xA;Dependencies # In order to setup the TiDB CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    <item>
      <title>OceanBase</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/oceanbase-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/oceanbase-cdc/</guid>
      <description>OceanBase CDC è¿æ¥å™¨ # OceanBase CDC è¿æ¥å™¨å…è®¸ä» OceanBase è¯»å–å¿«ç…§æ•°æ®å’Œå¢é‡æ•°æ®ã€‚æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•è®¾ç½® OceanBase CDC è¿æ¥å™¨ä»¥å¯¹ OceanBase è¿›è¡Œ SQL æŸ¥è¯¢ã€‚&#xA;OceanBase CDC æ–¹æ¡ˆ # åè¯è§£é‡Š:&#xA;OceanBase CE: OceanBase ç¤¾åŒºç‰ˆã€‚OceanBase çš„å¼€æºç‰ˆæœ¬ï¼Œå…¼å®¹ MySQL https://github.com/oceanbase/oceanbase ã€‚ OceanBase EE: OceanBase ä¼ä¸šç‰ˆã€‚OceanBase çš„å•†ä¸šç‰ˆæœ¬ï¼Œæ”¯æŒ MySQL å’Œ Oracle ä¸¤ç§å…¼å®¹æ¨¡å¼ https://www.oceanbase.com ã€‚ OceanBase Cloud: OceanBase äº‘æ•°æ®åº“ https://www.oceanbase.com/product/cloud ã€‚ Log Proxy CE: OceanBase æ—¥å¿—ä»£ç†æœåŠ¡ç¤¾åŒºç‰ˆã€‚å•ç‹¬ä½¿ç”¨æ—¶æ”¯æŒ CDC æ¨¡å¼ï¼Œæ˜¯ä¸€ä¸ªè·å– OceanBase ç¤¾åŒºç‰ˆäº‹åŠ¡æ—¥å¿—ï¼ˆcommit logï¼‰çš„ä»£ç†æœåŠ¡ https://github.com/oceanbase/oblogproxy ã€‚ Log Proxy EE: OceanBase æ—¥å¿—ä»£ç†æœåŠ¡ä¼ä¸šç‰ˆã€‚å•ç‹¬ä½¿ç”¨æ—¶æ”¯æŒ CDC æ¨¡å¼ï¼Œæ˜¯ä¸€ä¸ªè·å– OceanBase ä¼ä¸šç‰ˆäº‹åŠ¡æ—¥å¿—ï¼ˆcommit logï¼‰çš„ä»£ç†æœåŠ¡ï¼Œç›®å‰ä»…åœ¨ OceanBase Cloud ä¸Šæä¾›æœ‰é™çš„æ”¯æŒ, è¯¦æƒ…è¯·å’¨è¯¢ç›¸å…³æŠ€æœ¯æ”¯æŒã€‚ Binlog Service CE: OceanBase Binlog æœåŠ¡ç¤¾åŒºç‰ˆã€‚OceanBase ç¤¾åŒºç‰ˆçš„ä¸€ä¸ªå…¼å®¹ MySQL å¤åˆ¶åè®®çš„è§£å†³æ–¹æ¡ˆï¼Œè¯¦æƒ…å‚è€ƒ Log Proxy CE Binlog æ¨¡å¼çš„æ–‡æ¡£ã€‚ Binlog Service EE: OceanBase Binlog æœåŠ¡ä¼ä¸šç‰ˆã€‚OceanBase ä¼ä¸šç‰ˆ MySQL æ¨¡å¼çš„ä¸€ä¸ªå…¼å®¹ MySQL å¤åˆ¶åè®®çš„è§£å†³æ–¹æ¡ˆï¼Œä»…å¯åœ¨é˜¿é‡Œäº‘ä½¿ç”¨ï¼Œè¯¦æƒ…è§æ“ä½œæŒ‡å—ã€‚ MySQL Driver: mysql-connector-javaï¼Œå¯ç”¨äº OceanBase ç¤¾åŒºç‰ˆå’Œ OceanBase ä¼ä¸šç‰ˆ MySQL æ¨¡å¼ã€‚ OceanBase Driver: OceanBase JDBC é©±åŠ¨ï¼Œæ”¯æŒæ‰€æœ‰ç‰ˆæœ¬çš„ MySQL å’Œ Oracle å…¼å®¹æ¨¡å¼ https://github.</description>
    </item>
    <item>
      <title>Vitess</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/vitess-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/vitess-cdc/</guid>
      <description>Vitess CDC Connector # The Vitess CDC connector allows for reading of incremental data from Vitess cluster. The connector does not support snapshot feature at the moment. This document describes how to setup the Vitess CDC connector to run SQL queries against Vitess databases. Vitess debezium documentation&#xA;Dependencies # In order to setup the Vitess CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    <item>
      <title>DataStream API æ‰“åŒ…æŒ‡å—</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/datastream-api-package-guidance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/datastream-api-package-guidance/</guid>
      <description>DataStream API Package Guidance # This guide provides a simple pom.xml example for packaging DataStream job JARs with MySQL CDC source.&#xA;Example for pom.xml # &amp;lt;?xml version=&amp;#34;1.0&amp;#34; encoding=&amp;#34;UTF-8&amp;#34;?&amp;gt; &amp;lt;project xmlns=&amp;#34;http://maven.apache.org/POM/4.0.0&amp;#34; xmlns:xsi=&amp;#34;http://www.w3.org/2001/XMLSchema-instance&amp;#34; xsi:schemaLocation=&amp;#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&amp;#34;&amp;gt; &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;FlinkCDCTest&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.0-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;properties&amp;gt; &amp;lt;maven.compiler.source&amp;gt;8&amp;lt;/maven.compiler.source&amp;gt; &amp;lt;maven.compiler.target&amp;gt;8&amp;lt;/maven.compiler.target&amp;gt; &amp;lt;project.build.sourceEncoding&amp;gt;UTF-8&amp;lt;/project.build.sourceEncoding&amp;gt; &amp;lt;java.version&amp;gt;1.8&amp;lt;/java.version&amp;gt; &amp;lt;scala.binary.version&amp;gt;2.12&amp;lt;/scala.binary.version&amp;gt; &amp;lt;maven.compiler.source&amp;gt;${java.version}&amp;lt;/maven.compiler.source&amp;gt; &amp;lt;maven.compiler.target&amp;gt;${java.version}&amp;lt;/maven.compiler.target&amp;gt; &amp;lt;project.build.sourceEncoding&amp;gt;UTF-8&amp;lt;/project.build.sourceEncoding&amp;gt; &amp;lt;!-- Enforce single fork execution due to heavy mini cluster use in the tests --&amp;gt; &amp;lt;flink.forkCount&amp;gt;1&amp;lt;/flink.forkCount&amp;gt; &amp;lt;flink.reuseForks&amp;gt;true&amp;lt;/flink.reuseForks&amp;gt; &amp;lt;!-- dependencies versions --&amp;gt; &amp;lt;flink.version&amp;gt;1.17.2&amp;lt;/flink.version&amp;gt; &amp;lt;slf4j.version&amp;gt;1.7.15&amp;lt;/slf4j.version&amp;gt; &amp;lt;log4j.</description>
    </item>
    <item>
      <title>ä½¿ç”¨ Flink CDC æ„å»ºå®æ—¶æ•°æ®æ¹–</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/build-real-time-data-lake-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/build-real-time-data-lake-tutorial/</guid>
      <description>ä½¿ç”¨ Flink CDC æ„å»ºå®æ—¶æ•°æ®æ¹– # åœ¨ OLTP ç³»ç»Ÿä¸­ï¼Œä¸ºäº†è§£å†³å•è¡¨æ•°æ®é‡å¤§çš„é—®é¢˜ï¼Œé€šå¸¸é‡‡ç”¨åˆ†åº“åˆ†è¡¨çš„æ–¹å¼å°†å•ä¸ªå¤§è¡¨è¿›è¡Œæ‹†åˆ†ä»¥æé«˜ç³»ç»Ÿçš„ååé‡ã€‚ ä½†æ˜¯ä¸ºäº†æ–¹ä¾¿æ•°æ®åˆ†æï¼Œé€šå¸¸éœ€è¦å°†åˆ†åº“åˆ†è¡¨æ‹†åˆ†å‡ºçš„è¡¨åœ¨åŒæ­¥åˆ°æ•°æ®ä»“åº“ã€æ•°æ®æ¹–æ—¶ï¼Œå†åˆå¹¶æˆä¸€ä¸ªå¤§è¡¨ã€‚&#xA;è¿™ç¯‡æ•™ç¨‹å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Flink CDC æ„å»ºå®æ—¶æ•°æ®æ¹–æ¥åº”å¯¹è¿™ç§åœºæ™¯ï¼Œæœ¬æ•™ç¨‹çš„æ¼”ç¤ºåŸºäº Dockerï¼Œåªæ¶‰åŠ SQLï¼Œæ— éœ€ä¸€è¡Œ Java/Scala ä»£ç ï¼Œä¹Ÿæ— éœ€å®‰è£… IDEï¼Œä½ å¯ä»¥å¾ˆæ–¹ä¾¿åœ°åœ¨è‡ªå·±çš„ç”µè„‘ä¸Šå®Œæˆæœ¬æ•™ç¨‹çš„å…¨éƒ¨å†…å®¹ã€‚&#xA;æ¥ä¸‹æ¥å°†ä»¥æ•°æ®ä» MySQL åŒæ­¥åˆ° Iceberg ä¸ºä¾‹å±•ç¤ºæ•´ä¸ªæµç¨‹ï¼Œæ¶æ„å›¾å¦‚ä¸‹æ‰€ç¤ºï¼š&#xA;ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸åŒçš„ source æ¯”å¦‚ Oracle/Postgres å’Œ sink æ¯”å¦‚ Hudi æ¥æ„å»ºè‡ªå·±çš„ ETL æµç¨‹ã€‚&#xA;å‡†å¤‡é˜¶æ®µ # å‡†å¤‡ä¸€å°å·²ç»å®‰è£…äº† Docker çš„ Linux æˆ–è€… MacOS ç”µè„‘ã€‚&#xA;å‡†å¤‡æ•™ç¨‹æ‰€éœ€è¦çš„ç»„ä»¶ # æ¥ä¸‹æ¥çš„æ•™ç¨‹å°†ä»¥ docker-compose çš„æ–¹å¼å‡†å¤‡æ‰€éœ€è¦çš„ç»„ä»¶ã€‚&#xA;ä½¿ç”¨ä¸‹é¢çš„å†…å®¹åˆ›å»ºä¸€ä¸ª docker-compose.yml æ–‡ä»¶ï¼š&#xA;version: &amp;#39;2.1&amp;#39; services: sql-client: user: flink:flink image: yuxialuo/flink-sql-client:1.13.2.v1 depends_on: - jobmanager - mysql environment: FLINK_JOBMANAGER_HOST: jobmanager MYSQL_HOST: mysql volumes: - shared-tmpfs:/tmp/iceberg jobmanager: user: flink:flink image: flink:1.</description>
    </item>
    <item>
      <title>ä½¿ç”¨ Flink CDC æ„å»º Streaming ETL</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/build-streaming-etl-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/docs/connectors/flink-sources/tutorials/build-streaming-etl-tutorial/</guid>
      <description>ä½¿ç”¨ Flink CDC æ„å»º Streaming ETL # è¿™ç¯‡æ•™ç¨‹å°†å±•ç¤ºå¦‚ä½•åŸºäº Flink CDC å¿«é€Ÿæ„å»º MySQL å’Œ Postgres çš„æµå¼ ETLã€‚æœ¬æ•™ç¨‹çš„æ¼”ç¤ºéƒ½å°†åœ¨ Flink SQL CLI ä¸­è¿›è¡Œï¼Œåªæ¶‰åŠ SQLï¼Œæ— éœ€ä¸€è¡Œ Java/Scala ä»£ç ï¼Œä¹Ÿæ— éœ€å®‰è£… IDEã€‚&#xA;å‡è®¾æˆ‘ä»¬æ­£åœ¨ç»è¥ç”µå­å•†åŠ¡ä¸šåŠ¡ï¼Œå•†å“å’Œè®¢å•çš„æ•°æ®å­˜å‚¨åœ¨ MySQL ä¸­ï¼Œè®¢å•å¯¹åº”çš„ç‰©æµä¿¡æ¯å­˜å‚¨åœ¨ Postgres ä¸­ã€‚ å¯¹äºè®¢å•è¡¨ï¼Œä¸ºäº†æ–¹ä¾¿è¿›è¡Œåˆ†æï¼Œæˆ‘ä»¬å¸Œæœ›è®©å®ƒå…³è”ä¸Šå…¶å¯¹åº”çš„å•†å“å’Œç‰©æµä¿¡æ¯ï¼Œæ„æˆä¸€å¼ å®½è¡¨ï¼Œå¹¶ä¸”å®æ—¶æŠŠå®ƒå†™åˆ° ElasticSearch ä¸­ã€‚&#xA;æ¥ä¸‹æ¥çš„å†…å®¹å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨ Flink Mysql/Postgres CDC æ¥å®ç°è¿™ä¸ªéœ€æ±‚ï¼Œç³»ç»Ÿçš„æ•´ä½“æ¶æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š å‡†å¤‡é˜¶æ®µ # å‡†å¤‡ä¸€å°å·²ç»å®‰è£…äº† Docker çš„ Linux æˆ–è€… MacOS ç”µè„‘ã€‚&#xA;å‡†å¤‡æ•™ç¨‹æ‰€éœ€è¦çš„ç»„ä»¶ # æ¥ä¸‹æ¥çš„æ•™ç¨‹å°†ä»¥ docker-compose çš„æ–¹å¼å‡†å¤‡æ‰€éœ€è¦çš„ç»„ä»¶ã€‚&#xA;ä½¿ç”¨ä¸‹é¢çš„å†…å®¹åˆ›å»ºä¸€ä¸ª docker-compose.yml æ–‡ä»¶ï¼š&#xA;version: &amp;#39;2.1&amp;#39; services: postgres: image: debezium/example-postgres:1.1 ports: - &amp;#34;5432:5432&amp;#34; environment: - POSTGRES_DB=postgres - POSTGRES_USER=postgres - POSTGRES_PASSWORD=postgres mysql: image: debezium/example-mysql:1.</description>
    </item>
    <item>
      <title>Versions</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/zh/versions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/zh/versions/</guid>
      <description> Versions # An appendix of hosted documentation for all versions of Apache Flink CDC.&#xA;v3.0 </description>
    </item>
  </channel>
</rss>
