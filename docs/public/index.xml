<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Flink CDC</title>
    <link>//localhost:1313/flink/flink-cdc-docs-master/</link>
    <description>Recent content on Apache Flink CDC</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <atom:link href="//localhost:1313/flink/flink-cdc-docs-master/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Data Pipeline</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/core-concept/data-pipeline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/core-concept/data-pipeline/</guid>
      <description>Definition # Since events in Flink CDC flow from the upstream to the downstream in a pipeline manner, the whole ETL task is referred as a Data Pipeline.&#xA;Parameters # A pipeline corresponds to a chain of operators in Flink.&#xA;To describe a Data Pipeline, the following parts are required:&#xA;source sink pipeline the following parts are optional:&#xA;route transform Example # Only required # We could use following yaml file to define a concise Data Pipeline describing synchronize all tables under MySQL app_db database to Doris :</description>
    </item>
    <item>
      <title>Frequently Asked Questions</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/faq/faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/faq/faq/</guid>
      <description>General FAQ # Q1: Why can&amp;rsquo;t I download Flink-sql-connector-mysql-cdc-2.2-snapshot jar, why doesn&amp;rsquo;t Maven warehouse rely on XXX snapshot? # Like the mainstream Maven project version management, XXX snapshot version is the code corresponding to the development branch. Users need to download the source code and compile the corresponding jar. Users should use the released version, such as flink-sql-connector-mysql-cdc-2.1 0.jar, the released version will be available in the Maven central warehouse.</description>
    </item>
    <item>
      <title>Introduction</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/get-started/introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/get-started/introduction/</guid>
      <description>Welcome to Flink CDC ðŸŽ‰ # Flink CDC is a streaming data integration tool that aims to provide users with a more robust API. It allows users to describe their ETL pipeline logic via YAML elegantly and help users automatically generating customized Flink operators and submitting job. Flink CDC prioritizes optimizing the task submission process and offers enhanced functionalities such as schema evolution, data transformation, full database synchronization and exactly-once semantic.</description>
    </item>
    <item>
      <title>MongoDB Tutorial</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/mongodb-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/mongodb-tutorial/</guid>
      <description>Demo: MongoDB CDC to Elasticsearch # Create docker-compose.yml file using following contents: version: &amp;#39;2.1&amp;#39; services: mongo: image: &amp;#34;mongo:4.0-xenial&amp;#34; command: --replSet rs0 --smallfiles --oplogSize 128 ports: - &amp;#34;27017:27017&amp;#34; environment: - MONGO_INITDB_ROOT_USERNAME=mongouser - MONGO_INITDB_ROOT_PASSWORD=mongopw elasticsearch: image: elastic/elasticsearch:7.6.0 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &amp;#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&amp;#34; - discovery.type=single-node ports: - &amp;#34;9200:9200&amp;#34; - &amp;#34;9300:9300&amp;#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 ports: - &amp;#34;5601:5601&amp;#34; Enter Mongodb&amp;rsquo;s container and initialize replica set and data: docker-compose exec mongo /usr/bin/mongo -u mongouser -p mongopw // 1.</description>
    </item>
    <item>
      <title>MySQL to Doris</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/get-started/quickstart/mysql-to-doris/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/get-started/quickstart/mysql-to-doris/</guid>
      <description>Streaming ELT from MySQL to Doris # This tutorial is to show how to quickly build a Streaming ELT job from MySQL to Doris using Flink CDC, including the feature of sync all table of one database, schema change evolution and sync sharding tables into one table.&#xA;All exercises in this tutorial are performed in the Flink CDC CLI, and the entire process uses standard SQL syntax, without a single line of Java/Scala code or IDE installation.</description>
    </item>
    <item>
      <title>Overview</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/overview/</guid>
      <description>Flink CDC sources # Flink CDC sources is a set of source connectors for Apache FlinkÂ®, ingesting changes from different databases using change data capture (CDC). Some CDC sources integrate Debezium as the engine to capture data changes. So it can fully leverage the ability of Debezium. See more about what is Debezium.&#xA;You can also read tutorials about how to use these sources.&#xA;Supported Connectors # Connector Database Driver mongodb-cdc MongoDB: 3.</description>
    </item>
    <item>
      <title>Overview</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/overview/</guid>
      <description>Connectors # Flink CDC provides several source and sink connectors to interact with external systems. You can use these connectors out-of-box, by adding released JARs to your Flink CDC environment, and specifying the connector in your YAML pipeline definition.&#xA;Supported Connectors # Connector Supported Type External System Apache Doris Sink Apache Doris: 1.2.x, 2.x.x Kafka Sink Kafka MySQL Source MySQL: 5.6, 5.7, 8.0.x RDS MySQL: 5.6, 5.7, 8.0.x PolarDB MySQL: 5.</description>
    </item>
    <item>
      <title>Standalone</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/deployment/standalone/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/deployment/standalone/</guid>
      <description>Introduction # Standalone mode is Flinkâ€™s simplest deployment mode. This short guide will show you how to download the latest stable version of Flink, install, and run it. You will also run an example Flink CDC job and view it in the web UI.&#xA;Preparation # Flink runs on all UNIX-like environments, i.e. Linux, Mac OS X, and Cygwin (for Windows).&#xA;You can refer overview to check supported versions and download the binary release of Flink, then extract the archive:</description>
    </item>
    <item>
      <title>Understand Flink CDC API</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/developer-guide/understand-flink-cdc-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/developer-guide/understand-flink-cdc-api/</guid>
      <description>Understand Flink CDC API # If you are planning to build your own Flink CDC connectors, or considering contributing to Flink CDC, you might want to hava a deeper look at the APIs of Flink CDC. This document will go through some important concepts and interfaces in order to help you with your development.&#xA;Event # An event under the context of Flink CDC is a special kind of record in Flink&amp;rsquo;s data stream.</description>
    </item>
    <item>
      <title>Contribute to Flink CDC</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/developer-guide/contribute-to-flink-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/developer-guide/contribute-to-flink-cdc/</guid>
      <description>Contributing # Flink CDC is developed by an open and friendly community and welcomes anyone who wants to help out in any way. There are several ways to interact with the community and contribute to Flink CDC including asking questions, filing bug reports, proposing new features, joining discussions on the mailing lists, contributing code or documentation, improving website, testing release candidates and writing corresponding blog etc.&#xA;What do you want to do # Contributing to Flink CDC goes beyond writing code for the project.</description>
    </item>
    <item>
      <title>Data Source</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/core-concept/data-source/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/core-concept/data-source/</guid>
      <description>Definition # Data Source is used to access metadata and read the changed data from external systems.&#xA;A Data Source can read data from multiple tables simultaneously.&#xA;Parameters # To describe a data source, the follows are required:&#xA;parameter meaning optional/required type The type of the source, such as mysql. required name The name of the source, which is user-defined (a default value provided). optional configurations of Data Source Configurations to build the Data Source e.</description>
    </item>
    <item>
      <title>Db2 Tutorial</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/db2-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/db2-tutorial/</guid>
      <description>Demo: Db2 CDC to Elasticsearch # 1. Create docker-compose.yml file using following contents:&#xA;version: &amp;#39;2.1&amp;#39; services: db2: image: ruanhang/db2-cdc-demo:v1 privileged: true ports: - 50000:50000 environment: - LICENSE=accept - DB2INSTANCE=db2inst1 - DB2INST1_PASSWORD=admin - DBNAME=testdb - ARCHIVE_LOGS=true elasticsearch: image: elastic/elasticsearch:7.6.0 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &amp;#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&amp;#34; - discovery.type=single-node ports: - &amp;#34;9200:9200&amp;#34; - &amp;#34;9300:9300&amp;#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 ports: - &amp;#34;5601:5601&amp;#34; volumes: - /var/run/docker.</description>
    </item>
    <item>
      <title>MySQL</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/mysql-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/mysql-cdc/</guid>
      <description>MySQL CDC Connector # The MySQL CDC connector allows for reading snapshot data and incremental data from MySQL database. This document describes how to setup the MySQL CDC connector to run SQL queries against MySQL databases.&#xA;Supported Databases # Connector Database Driver mysql-cdc MySQL: 5.6, 5.7, 8.0.x RDS MySQL: 5.6, 5.7, 8.0.x PolarDB MySQL: 5.6, 5.7, 8.0.x Aurora MySQL: 5.6, 5.7, 8.0.x MariaDB: 10.x PolarDB X: 2.0.1 JDBC Driver: 8.</description>
    </item>
    <item>
      <title>MySQL</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/mysql/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/mysql/</guid>
      <description>MySQL Connector # MySQL connector allows reading snapshot data and incremental data from MySQL database and provides end-to-end full-database data synchronization capabilities. This document describes how to setup the MySQL connector.&#xA;Dependencies # Since MySQL Connector&amp;rsquo;s GPLv2 license is incompatible with Flink CDC project, we can&amp;rsquo;t provide MySQL connector in prebuilt connector jar packages. You may need to configure the following dependencies manually, and pass it with --jar argument of Flink CDC CLI when submitting YAML pipeline jobs.</description>
    </item>
    <item>
      <title>MySQL to StarRocks</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/get-started/quickstart/mysql-to-starrocks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/get-started/quickstart/mysql-to-starrocks/</guid>
      <description>Streaming ELT from MySQL to StarRocks # This tutorial is to show how to quickly build a Streaming ELT job from MySQL to StarRocks using Flink CDC, including the feature of sync all table of one database, schema change evolution and sync sharding tables into one table.&#xA;All exercises in this tutorial are performed in the Flink CDC CLI, and the entire process uses standard SQL syntax, without a single line of Java/Scala code or IDE installation.</description>
    </item>
    <item>
      <title>YARN</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/deployment/yarn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/deployment/yarn/</guid>
      <description>Introduction # Apache Hadoop YARN is a resource provider popular with many data processing frameworks. Flink services are submitted to YARN&amp;rsquo;s ResourceManager, which spawns containers on machines managed by YARN NodeManagers. Flink deploys its JobManager and TaskManager instances into such containers.&#xA;Flink can dynamically allocate and de-allocate TaskManager resources depending on the number of processing slots required by the job(s) running on the JobManager.&#xA;Preparation # This Getting Started section assumes a functional YARN environment, starting from version 2.</description>
    </item>
    <item>
      <title>Data Sink</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/core-concept/data-sink/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/core-concept/data-sink/</guid>
      <description>Definition # Data Sink is used to apply schema changes and write change data to external systems. A Data Sink can write to multiple tables simultaneously.&#xA;Parameters # To describe a data sink, the follows are required:&#xA;parameter meaning optional/required type The type of the sink, such as doris or starrocks. required name The name of the sink, which is user-defined (a default value provided). optional configurations of Data Sink Configurations to build the Data Sink e.</description>
    </item>
    <item>
      <title>Kubernetes</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/deployment/kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/deployment/kubernetes/</guid>
      <description>Introduction # Kubernetes is a popular container-orchestration system for automating computer application deployment, scaling, and management. Flink&amp;rsquo;s native Kubernetes integration allows you to directly deploy Flink on a running Kubernetes cluster. Moreover, Flink is able to dynamically allocate and de-allocate TaskManagers depending on the required resources because it can directly talk to Kubernetes.&#xA;Apache Flink also provides a Kubernetes operator for managing Flink clusters on Kubernetes. It supports both standalone and native deployment mode and greatly simplifies deployment, configuration and the life cycle management of Flink resources on Kubernetes.</description>
    </item>
    <item>
      <title>Licenses</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/developer-guide/licenses/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/developer-guide/licenses/</guid>
      <description> Licenses # Flink CDC is licensed under Apache License 2.0.&#xA;If you have any question regarding licenses, just contact us.&#xA;Apache Software Foundation # You could know more about ASF as follows.&#xA;Apache Software Foundation License Events Security SponSprShip Thanks Privacy </description>
    </item>
    <item>
      <title>OceanBase Tutorial</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/oceanbase-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/oceanbase-tutorial/</guid>
      <description>Demo: OceanBase CDC to ElasticSearch # Video tutorial # YouTube Bilibili Preparation # Configure and start the components # Create docker-compose.yml.&#xA;Note: host network mode is required in this demo, so it can only work on Linux, see network-tutorial-host.&#xA;version: &amp;#39;2.1&amp;#39; services: observer: image: oceanbase/oceanbase-ce:4.2.0.0 container_name: observer environment: - &amp;#39;MODE=slim&amp;#39; - &amp;#39;OB_ROOT_PASSWORD=pswd&amp;#39; network_mode: &amp;#34;host&amp;#34; oblogproxy: image: whhe/oblogproxy:1.1.3_4x container_name: oblogproxy environment: - &amp;#39;OB_SYS_USERNAME=root&amp;#39; - &amp;#39;OB_SYS_PASSWORD=pswd&amp;#39; network_mode: &amp;#34;host&amp;#34; elasticsearch: image: &amp;#39;elastic/elasticsearch:7.6.0&amp;#39; container_name: elasticsearch environment: - cluster.</description>
    </item>
    <item>
      <title>Oracle</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/oracle-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/oracle-cdc/</guid>
      <description>Oracle CDC Connector # The Oracle CDC connector allows for reading snapshot data and incremental data from Oracle database. This document describes how to setup the Oracle CDC connector to run SQL queries against Oracle databases.&#xA;Dependencies # In order to setup the Oracle CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    <item>
      <title>Paimon</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/paimon/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/paimon/</guid>
      <description>Paimon Pipeline Connector # The Paimon Pipeline connector can be used as the Data Sink of the pipeline, and write data to Paimon. This document describes how to set up the Paimon Pipeline connector.&#xA;What can the connector do? # Create table automatically if not exist Schema change synchronization Data synchronization How to create Pipeline # The pipeline for reading data from MySQL and sink to Paimon can be defined as follows:</description>
    </item>
    <item>
      <title>Kafka</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/kafka/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/kafka/</guid>
      <description>Kafka Pipeline Connector # The Kafka Pipeline connector can be used as the Data Sink of the pipeline, and write data to Kafka. This document describes how to set up the Kafka Pipeline connector.&#xA;What can the connector do? # Data synchronization How to create Pipeline # The pipeline for reading data from MySQL and sink to Kafka can be defined as follows:&#xA;source: type: mysql name: MySQL Source hostname: 127.</description>
    </item>
    <item>
      <title>Oracle Tutorial</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/oracle-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/oracle-tutorial/</guid>
      <description>Demo: Oracle CDC to Elasticsearch # Create docker-compose.yml file using following contents:&#xA;version: &amp;#39;2.1&amp;#39; services: oracle: image: goodboy008/oracle-19.3.0-ee:non-cdb ports: - &amp;#34;1521:1521&amp;#34; elasticsearch: image: elastic/elasticsearch:7.6.0 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &amp;#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&amp;#34; - discovery.type=single-node ports: - &amp;#34;9200:9200&amp;#34; - &amp;#34;9300:9300&amp;#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 ports: - &amp;#34;5601:5601&amp;#34; volumes: - /var/run/docker.sock:/var/run/docker.sock The Docker Compose environment consists of the following containers:&#xA;Oracle: Oracle 19c database.</description>
    </item>
    <item>
      <title>SQL Server</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/sqlserver-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/sqlserver-cdc/</guid>
      <description>SQLServer CDC Connector # The SQLServer CDC connector allows for reading snapshot data and incremental data from SQLServer database. This document describes how to setup the SQLServer CDC connector to run SQL queries against SQLServer databases.&#xA;Dependencies # In order to setup the SQLServer CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    <item>
      <title>Table ID</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/core-concept/table-id/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/core-concept/table-id/</guid>
      <description>Definition # When connecting to external systems, it is necessary to establish a mapping relationship with the storage objects of the external system. This is what Table Id refers to.&#xA;Example # To be compatible with most external systems, the Table Id is represented by a 3-tuple : (namespace, schemaName, tableName).&#xA;Connectors should establish the mapping between Table Id and storage objects in external systems.&#xA;The following table lists the parts in table Id of different data systems:</description>
    </item>
    <item>
      <title>Doris</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/doris/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/doris/</guid>
      <description>Doris Connector # This article introduces of Doris Connector&#xA;Example # source: type: values name: ValuesSource sink: type: doris name: Doris Sink fenodes: 127.0.0.1:8030 username: root password: &amp;#34;&amp;#34; table.create.properties.replication_num: 1 pipeline: parallelism: 1 Connector Options # Option Required Default Type Description type required (none) String Specify the Sink to use, here is &#39;doris&#39;. name optional (none) String Name of PipeLine fenodes required (none) String Http address of Doris cluster FE, such as 127.</description>
    </item>
    <item>
      <title>PolarDB-X Tutorial</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/polardbx-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/polardbx-tutorial/</guid>
      <description>Demo: PolarDB-X CDC to Elasticsearch # This tutorial is to show how to quickly build streaming ETL for PolarDB-X with Flink CDC.&#xA;Assuming we are running an e-commerce business. The product and order data stored in PolarDB-X. We want to enrich the orders using the product table, and then load the enriched orders to ElasticSearch in real time.&#xA;In the following sections, we will describe how to use Flink PolarDB-X CDC to implement it.</description>
    </item>
    <item>
      <title>Postgres</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/postgres-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/postgres-cdc/</guid>
      <description>Postgres CDC Connector # The Postgres CDC connector allows for reading snapshot data and incremental data from PostgreSQL database. This document describes how to setup the Postgres CDC connector to run SQL queries against PostgreSQL databases.&#xA;Dependencies # In order to setup the Postgres CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    <item>
      <title>Transform</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/core-concept/transform/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/core-concept/transform/</guid>
      <description>Definition # Transform module helps users delete and expand data columns based on the data columns in the table. What&amp;rsquo;s more, it also helps users filter some unnecessary data during the synchronization process.&#xA;Parameters # To describe a transform rule, the following parameters can be used:&#xA;Parameter Meaning Optional/Required source-table Source table id, supports regular expressions required projection Projection rule, supports syntax similar to the select clause in SQL optional filter Filter rule, supports syntax similar to the where clause in SQL optional primary-keys Sink table primary keys, separated by commas optional partition-keys Sink table partition keys, separated by commas optional table-options used to the configure table creation statement when automatically creating tables optional description Transform rule description optional Multiple rules can be declared in one single pipeline YAML file.</description>
    </item>
    <item>
      <title>MongoDB</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/mongodb-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/mongodb-cdc/</guid>
      <description>MongoDB CDC Connector # The MongoDB CDC connector allows for reading snapshot data and incremental data from MongoDB. This document describes how to setup the MongoDB CDC connector to run SQL queries against MongoDB.&#xA;Dependencies # In order to setup the MongoDB CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    <item>
      <title>Route</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/core-concept/route/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/core-concept/route/</guid>
      <description>Definition # Route specifies the rule of matching a list of source-table and mapping to sink-table. The most typical scenario is the merge of sub-databases and sub-tables, routing multiple upstream source tables to the same sink table.&#xA;Parameters # To describe a route, the follows are required:&#xA;parameter meaning optional/required source-table Source table id, supports regular expressions required sink-table Sink table id, supports regular expressions required description Routing rule description(a default value provided) optional A route module can contain a list of source-table/sink-table rules.</description>
    </item>
    <item>
      <title>SqlServer Tutorial</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/sqlserver-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/sqlserver-tutorial/</guid>
      <description>Demo: SqlServer CDC to Elasticsearch # Create docker-compose.yml file using following contents:&#xA;version: &amp;#39;2.1&amp;#39; services: sqlserver: image: mcr.microsoft.com/mssql/server:2019-latest container_name: sqlserver ports: - &amp;#34;1433:1433&amp;#34; environment: - &amp;#34;MSSQL_AGENT_ENABLED=true&amp;#34; - &amp;#34;MSSQL_PID=Standard&amp;#34; - &amp;#34;ACCEPT_EULA=Y&amp;#34; - &amp;#34;SA_PASSWORD=Password!&amp;#34; elasticsearch: image: elastic/elasticsearch:7.6.0 container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &amp;#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&amp;#34; - discovery.type=single-node ports: - &amp;#34;9200:9200&amp;#34; - &amp;#34;9300:9300&amp;#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 container_name: kibana ports: - &amp;#34;5601:5601&amp;#34; volumes: - /var/run/docker.</description>
    </item>
    <item>
      <title>StarRocks</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/starrocks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/starrocks/</guid>
      <description>StarRocks Connector # StarRocks connector can be used as the Data Sink of the pipeline, and write data to StarRocks. This document describes how to set up the StarRocks connector.&#xA;What can the connector do? # Create table automatically if not exist Schema change synchronization Data synchronization Example # The pipeline for reading data from MySQL and sink to StarRocks can be defined as follows:&#xA;source: type: mysql name: MySQL Source hostname: 127.</description>
    </item>
    <item>
      <title>Db2</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/db2-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/db2-cdc/</guid>
      <description>Db2 CDC Connector # The Db2 CDC connector allows for reading snapshot data and incremental data from Db2 database. This document describes how to setup the db2 CDC connector to run SQL queries against Db2 databases.&#xA;Supported Databases # Connector Database Driver Db2-cdc Db2: 11.5 Db2 Driver: 11.5.0.0 Dependencies # In order to set up the Db2 CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    <item>
      <title>TiDB Tutorial</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/tidb-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/tidb-tutorial/</guid>
      <description>Demo: TiDB CDC to Elasticsearch # First,we will start TiDB cluster with docker.&#xA;$ git clone https://github.com/pingcap/tidb-docker-compose.git Next,replace docker-compose.yml file using following contents in directory tidb-docker-compose:&#xA;version: &amp;#34;2.1&amp;#34; services: pd: image: pingcap/pd:v5.3.1 ports: - &amp;#34;2379:2379&amp;#34; volumes: - ./config/pd.toml:/pd.toml - ./logs:/logs command: - --client-urls=http://0.0.0.0:2379 - --peer-urls=http://0.0.0.0:2380 - --advertise-client-urls=http://pd:2379 - --advertise-peer-urls=http://pd:2380 - --initial-cluster=pd=http://pd:2380 - --data-dir=/data/pd - --config=/pd.toml - --log-file=/logs/pd.log restart: on-failure tikv: image: pingcap/tikv:v5.3.1 ports: - &amp;#34;20160:20160&amp;#34; volumes: - ./config/tikv.toml:/tikv.toml - .</description>
    </item>
    <item>
      <title>TiDB</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tidb-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tidb-cdc/</guid>
      <description>TiDB CDC Connector # The TiDB CDC connector allows for reading snapshot data and incremental data from TiDB database. This document describes how to setup the TiDB CDC connector to run SQL queries against TiDB databases.&#xA;Dependencies # In order to setup the TiDB CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    <item>
      <title>OceanBase</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/oceanbase-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/oceanbase-cdc/</guid>
      <description>OceanBase CDC Connector # The OceanBase CDC connector allows for reading snapshot data and incremental data from OceanBase. This document describes how to set up the OceanBase CDC connector to run SQL queries against OceanBase.&#xA;OceanBase CDC Solutions # Glossary:&#xA;OceanBase CE: OceanBase Community Edition. It&amp;rsquo;s compatible with MySQL and has been open sourced at https://github.com/oceanbase/oceanbase. OceanBase EE: OceanBase Enterprise Edition. It supports two compatibility modes: MySQL and Oracle. See https://en.</description>
    </item>
    <item>
      <title>Vitess</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/vitess-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/vitess-cdc/</guid>
      <description>Vitess CDC Connector # The Vitess CDC connector allows for reading of incremental data from Vitess cluster. The connector does not support snapshot feature at the moment. This document describes how to setup the Vitess CDC connector to run SQL queries against Vitess databases. Vitess debezium documentation&#xA;Dependencies # In order to setup the Vitess CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    <item>
      <title>Building a Real-time Data Lake with Flink CDC</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/build-real-time-data-lake-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/build-real-time-data-lake-tutorial/</guid>
      <description>Building a Real-time Data Lake with Flink CDC # For OLTP databases, to deal with a huge number of data in a single table, we usually do database and table sharding to get better throughput. But sometimes, for convenient analysis, we need to merge them into one table when loading them to data warehouse or data lake.&#xA;This tutorial will show how to use Flink CDC to build a real-time data lake for such a scenario.</description>
    </item>
    <item>
      <title>DataStream API Package Guidance</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/datastream-api-package-guidance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/datastream-api-package-guidance/</guid>
      <description>DataStream API Package Guidance # This guide provides a simple pom.xml example for packaging DataStream job JARs with MySQL CDC source.&#xA;Example for pom.xml # &amp;lt;?xml version=&amp;#34;1.0&amp;#34; encoding=&amp;#34;UTF-8&amp;#34;?&amp;gt; &amp;lt;project xmlns=&amp;#34;http://maven.apache.org/POM/4.0.0&amp;#34; xmlns:xsi=&amp;#34;http://www.w3.org/2001/XMLSchema-instance&amp;#34; xsi:schemaLocation=&amp;#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&amp;#34;&amp;gt; &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;FlinkCDCTest&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.0-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;properties&amp;gt; &amp;lt;maven.compiler.source&amp;gt;8&amp;lt;/maven.compiler.source&amp;gt; &amp;lt;maven.compiler.target&amp;gt;8&amp;lt;/maven.compiler.target&amp;gt; &amp;lt;project.build.sourceEncoding&amp;gt;UTF-8&amp;lt;/project.build.sourceEncoding&amp;gt; &amp;lt;java.version&amp;gt;1.8&amp;lt;/java.version&amp;gt; &amp;lt;scala.binary.version&amp;gt;2.12&amp;lt;/scala.binary.version&amp;gt; &amp;lt;maven.compiler.source&amp;gt;${java.version}&amp;lt;/maven.compiler.source&amp;gt; &amp;lt;maven.compiler.target&amp;gt;${java.version}&amp;lt;/maven.compiler.target&amp;gt; &amp;lt;project.build.sourceEncoding&amp;gt;UTF-8&amp;lt;/project.build.sourceEncoding&amp;gt; &amp;lt;!-- Enforce single fork execution due to heavy mini cluster use in the tests --&amp;gt; &amp;lt;flink.forkCount&amp;gt;1&amp;lt;/flink.forkCount&amp;gt; &amp;lt;flink.reuseForks&amp;gt;true&amp;lt;/flink.reuseForks&amp;gt; &amp;lt;!-- dependencies versions --&amp;gt; &amp;lt;flink.version&amp;gt;1.17.2&amp;lt;/flink.version&amp;gt; &amp;lt;slf4j.version&amp;gt;1.7.15&amp;lt;/slf4j.version&amp;gt; &amp;lt;log4j.</description>
    </item>
    <item>
      <title>Building a Streaming ETL with Flink CDC</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/build-streaming-etl-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/build-streaming-etl-tutorial/</guid>
      <description>Building a Streaming ETL with Flink CDC # This tutorial is to show how to quickly build streaming ETL for MySQL and Postgres with Flink CDC.&#xA;Assuming we are running an e-commerce business. The product and order data stored in MySQL, the shipment data related to the order is stored in Postgres. We want to enrich the orders using the product and shipment table, and then load the enriched orders to ElasticSearch in real time.</description>
    </item>
    <item>
      <title>Versions</title>
      <link>//localhost:1313/flink/flink-cdc-docs-master/versions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/flink/flink-cdc-docs-master/versions/</guid>
      <description> Versions # An appendix of hosted documentation for all versions of Apache Flink CDC.&#xA;v3.0 </description>
    </item>
  </channel>
</rss>
