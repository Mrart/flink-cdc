"use strict";(function(){const t={cache:!0};t.doc={id:"id",field:["title","content"],store:["title","href","section"]};const e=FlexSearch.create("balance",t);window.bookSearchIndex=e,e.add({id:0,href:"/flink/flink-cdc-docs-master/docs/core-concept/data-pipeline/",title:"Data Pipeline",section:"Core Concept",content:` Definition # Since events in Flink CDC flow from the upstream to the downstream in a pipeline manner, the whole ETL task is referred as a Data Pipeline.
Parameters # A pipeline corresponds to a chain of operators in Flink.
To describe a Data Pipeline, the following parts are required:
source sink pipeline the following parts are optional:
route transform Example # Only required # We could use following yaml file to define a concise Data Pipeline describing synchronize all tables under MySQL app_db database to Doris :
source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* sink: type: doris fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; pipeline: name: Sync MySQL Database to Doris parallelism: 2 With optional # We could use following yaml file to define a complicated Data Pipeline describing synchronize all tables under MySQL app_db database to Doris and give specific target database name ods_db and specific target table name prefix ods_ :
source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* sink: type: doris fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; route: - source-table: app_db.orders sink-table: ods_db.ods_orders - source-table: app_db.shipments sink-table: ods_db.ods_shipments - source-table: app_db.products sink-table: ods_db.ods_products pipeline: name: Sync MySQL Database to Doris parallelism: 2 Pipeline Configurations # The following config options of Data Pipeline level are supported:
parameter meaning optional/required name The name of the pipeline, which will be submitted to the Flink cluster as the job name. optional parallelism The global parallelism of the pipeline. required local-time-zone The local time zone defines current session time zone id. optional `}),e.add({id:1,href:"/flink/flink-cdc-docs-master/docs/faq/faq/",title:"Frequently Asked Questions",section:"FAQ",content:` General FAQ # Q1: Why can&rsquo;t I download Flink-sql-connector-mysql-cdc-2.2-snapshot jar, why doesn&rsquo;t Maven warehouse rely on XXX snapshot? # Like the mainstream Maven project version management, XXX snapshot version is the code corresponding to the development branch. Users need to download the source code and compile the corresponding jar. Users should use the released version, such as flink-sql-connector-mysql-cdc-2.1 0.jar, the released version will be available in the Maven central warehouse.
Q2: When should I use Flink SQL connector XXX Jar? When should I Flink connector XXX jar? What&rsquo;s the difference between the two? # The dependency management of each connector in Flink CDC project is consistent with that in Flink project. Flink SQL connector XX is a fat jar. In addition to the code of connector, it also enters all the third-party packages that connector depends on into the shade and provides them to SQL jobs. Users only need to add the fat jar in the flink/lib directory. The Flink connector XX has only the code of the connector and does not contain the required dependencies. It is used by DataStream jobs. Users need to manage the required three-party package dependencies. Conflicting dependencies need to be excluded and shaded by themselves.
Q3: Why change the package name from com.alibaba.ververica changed to org.apache.flink? Why can&rsquo;t the 2. X version be found in Maven warehouse? # Flink CDC project changes the group ID from com.alibaba.ververica changed to org.apache.flink since 2.0.0 version, this is to make the project more community neutral and more convenient for developers of various companies to build. So look for 2.x in Maven warehouse package, the path is /com/ververica, while the path of 3.1+ is /org/apache/flink.
MySQL CDC FAQ # Q1: I use CDC 2.x version , only full data can be read, but binlog data cannot be read. What&rsquo;s the matter? # CDC 2.0 supports lock free algorithm and concurrent reading. In order to ensure the order of full data + incremental data, it relies on Flink&rsquo;s checkpoint mechanism, so the job needs to be configured with checkpoint.
Configuration method in SQL job:
Flink SQL&gt; SET &#39;execution.checkpointing.interval&#39; = &#39;3s&#39;; DataStream job configuration mode:
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(3000); Q2: Using MySQL CDC DataStream API, the timestamp field read in the incremental phase has a time zone difference of 8 hours. What&rsquo;s the matter? # When parsing the timestamp field in binlog data, CDC will use the server time zone information configured in the job, that is, the time zone of the MySQL server. If this time zone is not consistent with the time zone of your MySQL server, this problem will occur.
In addition, if the serializer is customized in the DataStream job.
such as MyDeserializer implements DebeziumDeserializationSchema, when the customized serializer parses the timestamp type data, it needs to refer to the analysis of the timestamp type in RowDataDebeziumDeserializeSchema and use the given time zone information.
private TimestampData convertToTimestamp(Object dbzObj, Schema schema) { if (dbzObj instanceof Long) { switch (schema.name()) { case Timestamp.SCHEMA_NAME: return TimestampData.fromEpochMillis((Long) dbzObj); case MicroTimestamp.SCHEMA_NAME: long micro = (long) dbzObj; return TimestampData.fromEpochMillis(micro / 1000, (int) (micro % 1000 * 1000)); case NanoTimestamp.SCHEMA_NAME: long nano = (long) dbzObj; return TimestampData.fromEpochMillis(nano / 1000_000, (int) (nano % 1000_000)); } } LocalDateTime localDateTime = TemporalConversions.toLocalDateTime(dbzObj, serverTimeZone); return TimestampData.fromLocalDateTime(localDateTime); } Q3: Does MySQL CDC support listening to slave database? How to configure slave database? # Yes, the slave database needs to be configured with log slave updates = 1, so that the slave instance can also write the data synchronized from the master instance to the binlog file of the slave database. If the master database has enabled gtid mode, the slave database also needs to be enabled.
log-slave-updates = 1 gtid_mode = on enforce_gtid_consistency = on Q4: I want to synchronize sub databases and sub tables. How should I configure them? # In the with parameter of MySQL CDC table, both table name and database name support regular configuration, such as &rsquo;table name &rsquo; = &lsquo;user_ &lsquo;.&rsquo; Can match table name &lsquo;user_ 1, user_ 2,user_ A &rsquo; table.
Note that any regular matching character is&rsquo;. &rsquo; Instead of &lsquo;*&rsquo;, where the dot represents any character, the asterisk represents 0 or more, and so does database name, that the shared table should be in the same schema.
Q5: I want to skip the stock reading phase and only read binlog data. How to configure it? # In the with parameter of MySQL CDC table
&#39;scan.startup.mode&#39; = &#39;latest-offset&#39;. Q6: I want to get DDL events in the database. What should I do? Is there a demo? # Flink CDC provides DataStream API MysqlSource since version 2.1. Users can configure includeschemachanges to indicate whether DDL events are required. After obtaining DDL events, they can write code for next processing.
public void consumingAllEvents() throws Exception { inventoryDatabase.createAndInitialize(); MySqlSource&lt;String&gt; mySqlSource = MySqlSource.&lt;String&gt;builder() .hostname(MYSQL_CONTAINER.getHost()) .port(MYSQL_CONTAINER.getDatabasePort()) .databaseList(inventoryDatabase.getDatabaseName()) .tableList(inventoryDatabase.getDatabaseName() + &#34;.products&#34;) .username(inventoryDatabase.getUsername()) .password(inventoryDatabase.getPassword()) .serverId(&#34;5401-5404&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) .includeSchemaChanges(true) // Configure here and output DDL events .build(); ... // Other processing logic } Q7: How to synchronize the whole MySQL database? Does Flink CDC support it? # The DataStream API provided in Q6 has enabled users to obtain DDL change events and data change events. On this basis, users need to develop DataStream jobs according to their own business logic and downstream storage. Flink CDC provides the pipeline to synchronize the whole MySQL database Since version 3.0. Q8: In the same MySQL instance, the table of one database cannot synchronize incremental data, but other databases works fine. Why? # Users can check Binlog_Ignore_DB and Binlog_Do_DB through the show master status command
mysql&gt; show master status; +------------------+----------+--------------+------------------+----------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+----------------------+ | mysql-bin.000006 | 4594 | | | xxx:1-15 | +------------------+----------+--------------+------------------+----------------------+ Q9: The job reports an error the connector is trying to read binlog starting at GTIDs xxx and binlog file &lsquo;binlog.000064&rsquo;, pos=89887992, skipping 4 events plus 1 rows, but this is no longer available on the server. Reconfigure the connector to use a snapshot when needed, What should I do? # This error occurs because the binlog file being read by the job has been cleaned up on the MySQL server. Generally, the expiration time of the binlog file retained on the MySQL server is too short. You can set this value higher, such as 7 days.
mysql&gt; show variables like &#39;expire_logs_days&#39;; mysql&gt; set global expire_logs_days=7; In another case, the binlog consumption of the Flink CDC job is too slow. Generally, sufficient resources can be allocated.
Q10: The job reports an error ConnectException: A slave with the same server_uuid/server_id as this slave has connected to the master,What should I do? # This error occurs because the server ID used in the job conflicts with the server ID used by other jobs or other synchronization tools. The server ID needs to be globally unique. The server ID is an int type integer. In CDC 2.x In version, each concurrency of the source requires a server ID. it is recommended to reasonably plan the server ID. for example, if the source of the job is set to four concurrency, you can configure &lsquo;server ID&rsquo; = &lsquo;5001-5004&rsquo;, so that each source task will not conflict.
Q11: The job reports an error ConnectException: Received DML ‘…’ for processing, binlog probably contains events generated with statement or mixed based replication format,What should I do? # This error occurs because the MySQL server is not configured correctly. You need to check the binlog is format row? You can view it through the following command
mysql&gt; show variables like &#39;%binlog_format%&#39;; Q12: The job reports an error Mysql8.0 Public Key Retrieval is not allowed,What should I do? # This is because the MySQL user configured by the user uses sha256 password authentication and requires TLS and other protocols to transmit passwords. A simple method is to allow MySQL users to support original password access.
mysql&gt; ALTER USER &#39;username&#39;@&#39;localhost&#39; IDENTIFIED WITH mysql_native_password BY &#39;password&#39;; mysql&gt; FLUSH PRIVILEGES; Q13: The job reports an error EventDataDeserializationException: Failed to deserialize data of EventHeaderV4 &hellip;. Caused by: java.net.SocketException: Connection reset,What should I do? # This problem is generally caused by the network. First, check the network between the Flink cluster and the database, and then increase the network parameters of the MySQL server.
mysql&gt; set global slave_net_timeout = 120; mysql&gt; set global thread_pool_idle_timeout = 120; Or try to use the flink configuration as follows.
execution.checkpointing.interval=10min execution.checkpointing.tolerable-failed-checkpoints=100 restart-strategy=fixed-delay restart-strategy.fixed-delay.attempts=2147483647 restart-strategy.fixed-delay.delay= 30s If there is bad back pressure in the job, this problem may happen too. Then you need to handle the back pressure in the job first.
Q14: The job reports an error The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires,What should I do? # The reason for this problem is that the reading of the full volume phase of the job is too slow. After reading the full volume phase, the previously recorded gtid site at the beginning of the full volume phase has been cleared by mysql. This can increase the save time of binlog files on the MySQL server, or increase the concurrency of source to make the full volume phase read faster.
Q15: How to config tableList option when build MySQL CDC source in DataStream API? # The tableList option requires table name with database name rather than table name in DataStream API. For MySQL CDC source, the tableList option value should like ‘my_db.my_table’.
Postgres CDC FAQ # Q1: It is found that the disk utilization rate of PG server is high. What is the reason why wal is not released? # Flink Postgres CDC will only update the LSN in the Postgres slot when the checkpoint is completed. Therefore, if you find that the disk utilization is high, please first confirm whether the checkpoint is turned on.
Q2: Flink Postgres CDC returns null for decimal types exceeding the maximum precision (38, 18) in synchronous Postgres # In Flink, if the precision of the received data is greater than the precision of the type declared in Flink, the data will be processed as null. You can configure the corresponding &lsquo;debezium decimal. handling. Mode &lsquo;=&lsquo;string&rsquo; process the read data with string type
Q3: Flink Postgres CDC prompts that toast data is not transmitted. What is the reason? # Please ensure that the replica identity is full first. The toast data is relatively large. In order to save the size of wal, if the toast data is not changed, the wal2json plugin will not bring toast data to the updated data. To avoid this problem, you can use &lsquo;debezium schema. refresh. mode&rsquo;=&lsquo;columns_ diff_ exclude_ unchanged_ Toast &rsquo;to solve.
Q4: The job reports an error replication slot &ldquo;XXXX&rdquo; is active. What should I do? # Currently, Flink Postgres CDC does not release the slot manually after the job exits.
Go to Postgres and manually execute the following command.
select pg_drop_replication_slot(&#39;rep_slot&#39;); ERROR: replication slot &#34;rep_slot&#34; is active for PID 162564 select pg_terminate_backend(162564); select pg_drop_replication_slot(&#39;rep_slot&#39;); Q5: Jobs have dirty data, such as illegal dates. Are there parameters that can be configured and filtered? # Yes, you can add configure. In the with parameter of the Flink CDC table &lsquo;debezium.event.deserialization.failure.handling.mode&rsquo;=&lsquo;warn&rsquo; parameter, skip dirty data and print dirty data to warn log. You can also configure &lsquo;debezium.event.deserialization.failure.handling.mode&rsquo;=&lsquo;ignore&rsquo;, skip dirty data directly and do not print dirty data to the log.
Q6: How to config tableList option when build Postgres CDC source in DataStream API? # The tableList option requires table name with schema name rather than table name in DataStream API. For Postgres CDC source, the tableList option value should like ‘my_schema.my_table’.
MongoDB CDC FAQ # Q1: Does MongoDB CDC support full + incremental read and read-only incremental? # Yes, the default is full + incremental reading; Using &lsquo;scan.startup.mode&rsquo; = &rsquo;latest-offset&rsquo; parameter can set to read-only incremental.
Q2: Does MongoDB CDC support recovery from checkpoint? What is the principle? # Yes, the checkpoint will record the resumeToken of the changeStream. During recovery, the changeStream can be restored through the resumeToken. Where resumeToken corresponds to oplog.rs (Change log collection in MongoDB), oplog.rs is a fixed capacity collection. When the corresponding record of resumeToken does not exist in oplog.rs, an Invalid resumeToken Exception may occur. In this case, you can set the appropriate size of oplog.rs to avoid retention time of oplog.rs is too short, you can refer to https://docs.mongodb.com/manual/tutorial/change-oplog-size/. In addition, the resumeToken can be refreshed through the newly arrived change record and heartbeat record.
Q3: Does MongoDB CDC support outputting - U (update_before) messages? # In MongoDB versions &gt;= 6.0, if MongoDB enable document preimages, setting &lsquo;scan.full-changelog&rsquo; = &rsquo;true&rsquo; in Flink SQL can make source output -U messages, so ChangelogNormalize operator can be removed.
In MongoDB versions &lt; 6.0, the original oplog.rs in MongoDB only has operation types including insert, update, replace and delete. It does not save the information before update, so it cannot output - U messages. It can only realize the UPSERT semantics in Flink. When using MongoDBTableSource, Flink planner will automatically perform ChangelogNormalize optimization, fill in the missing - U messages, and output complete + I, - U, + U, and - D messages. The cost of ChangelogNormalize optimization is that the operator will save the states of all previous keys. Therefore, if the DataStream job directly uses MongoDBSource, without the optimization of Flink planner, ChangelogNormalize will not be performed automatically, so - U messages cannot be obtained directly. To obtain the pre update image value, you need to manage the status yourself. If you don&rsquo;t want to manage the status yourself, you can convert MongodbTableSource to changelogstream or retractstream and supplement the pre update image value with the optimization ability of Flink planner. An example is as follows:
tEnv.executeSql(&#34;CREATE TABLE orders ( ... ) WITH ( &#39;connector&#39;=&#39;mongodb-cdc&#39;,... )&#34;); Table table = tEnv.from(&#34;orders&#34;) .select($(&#34;*&#34;)); tEnv.toChangelogStream(table) .print() .setParallelism(1); env.execute(); Q4: Does MongoDB CDC support subscribing multiple collections? # All collections in database can be subscribed. For example, if database is configured as &rsquo; mgdb&rsquo; and collection is configured as an empty string, all collections under &lsquo;mgdb&rsquo; database will be subscribed.
It also supports subscribing collections using regular expressions. If the name of the collections to be monitored contains special characters used in regular expressions, then the collection parameter must be configured as a fully qualified namespace (&ldquo;database-name.collection-name&rdquo;), otherwise the changes to the corresponding collections cannot be captured.
Q5: Which versions of MongoDB are supported by MongoDB CDC? # MongoDB CDC is implemented based on the ChangeStream feature, which is a new feature introduced in MongoDB 3.6. Mongodb CDC theoretically supports versions &gt;= 3.6. It is recommended to run on version &gt;= 4.0. When executed on versions &lt; 3.6, an error will occur: Unrecognized pipeline stage name: &lsquo;$changeStream&rsquo;.
Q6: Which operational modes of MongoDB are supported by MongoDB CDC? # ChangeStream requires MongoDB to run in replica set or sharded cluster mode. For local test, a single-node replica set can be initialized with rs.initiate(). An error will occur in standalone mode: The $changeStream stage is only supported on replica sets.
Q7: MongoDB CDC reports an error. The username and password are incorrect, but other components can connect normally with this username and password. What is the reason? # If the user is not created in the default admin database, you need to add parameter &lsquo;connection.options&rsquo; = &lsquo;authSource={{ database where the user is created }}&rsquo;.
Q8: Does MongoDB CDC support debezium related parameters? # It is not supported, because MongoDB CDC connector is developed independently in the Flink CDC project and does not rely on the debezium project.
Oracle CDC FAQ # Q1: Oracle CDC&rsquo;s archive logs grow rapidly and read logs slowly? # The online mining mode can be used without writing the data dictionary to the redo log, but it cannot process DDL statements. The default policy of the production environment reads the log slowly, and the default policy will write the data dictionary information to the redo log, resulting in a large increase in the log volume. You can add the following debezium configuration items. &quot; log. mining. strategy&rsquo; = &lsquo;online_ catalog&rsquo;,&rsquo;log. mining. continuous. mine&rsquo; = &rsquo;true&rsquo;。 If you use SQL, you need to prefix the configuration item with &lsquo;debezium.&rsquo;, Namely:
&#39;debezium.log.mining.strategy&#39; = &#39;online_catalog&#39;, &#39;debezium.log.mining.continuous.mine&#39; = &#39;true&#39; Q2: Operation error caused by: io debezium. DebeziumException: Supplemental logging not configured for table xxx. Use command: alter table XXX add supplementary log data (all) columns? # For Oracle version 11, debezium will set tableidcasesensitive to true by default, resulting in the table name being updated to lowercase. Therefore, the table completion log setting cannot be queried in Oracle, resulting in the false alarm of &ldquo;supplementary logging not configured for table error&rdquo;.
If it is the DataStream API, add the configuration item of debezium &lsquo;database.tablename.case.insensitive&rsquo; = &lsquo;false&rsquo;. If the SQL API is used, add the configuration item &lsquo;debezium.database.tablename.case.insensitive&rsquo; = &lsquo;false&rsquo; in the option of the table
Q3: How does Oracle CDC switch to XStream? # Add configuration item &lsquo;database.connection.adpter&rsquo; = &lsquo;xstream&rsquo;, please use the configuration item &lsquo;debezium.database.connection.adpter&rsquo; = &lsquo;xstream&rsquo; if you&rsquo;re using SQL API.
Q4: What are the database name and schema name of Oracle CDC? # Database name is the name of the database example, that is, the SID of Oracle. Schema name is the schema corresponding to the table. Generally speaking, a user corresponds to a schema. The schema name of the user is equal to the user name and is used as the default schema of the user. Therefore, schema name is generally the user name for creating the table, but if a schema is specified when creating the table, the specified schema is schema name. For example, use create table AAAA If TestTable (XXXX) is successfully created, AAAA is schema name.
`}),e.add({id:2,href:"/flink/flink-cdc-docs-master/docs/get-started/",title:"Get Started",section:"Docs",content:" "}),e.add({id:3,href:"/flink/flink-cdc-docs-master/docs/get-started/introduction/",title:"Introduction",section:"Get Started",content:` Welcome to Flink CDC 🎉 # Flink CDC is a streaming data integration tool that aims to provide users with a more robust API. It allows users to describe their ETL pipeline logic via YAML elegantly and help users automatically generating customized Flink operators and submitting job. Flink CDC prioritizes optimizing the task submission process and offers enhanced functionalities such as schema evolution, data transformation, full database synchronization and exactly-once semantic.
Deeply integrated with and powered by Apache Flink, Flink CDC provides:
✅ End-to-end data integration framework ✅ API for data integration users to build jobs easily ✅ Multi-table support in Source / Sink ✅ Synchronization of entire databases ✅ Schema evolution capability How to Use Flink CDC # Flink CDC provides an YAML-formatted user API that more suitable for data integration scenarios. Here&rsquo;s an example YAML file defining a data pipeline that ingests real-time changes from MySQL, and synchronize them to Apache Doris:
source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: doris fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; table.create.properties.light_schema_change: true table.create.properties.replication_num: 1 pipeline: name: Sync MySQL Database to Doris parallelism: 2 By submitting the YAML file with flink-cdc.sh, a Flink job will be compiled and deployed to a designated Flink cluster. Please refer to Core Concept to get full documentation of all supported functionalities of a pipeline.
Write Your First Flink CDC Pipeline # Explore Flink CDC document to get hands on your first real-time data integration pipeline:
Quickstart # Check out the quickstart guide to learn how to establish a Flink CDC pipeline:
MySQL to Apache Doris MySQL to StarRocks Understand Core Concepts # Get familiar with core concepts we introduced in Flink CDC and try to build more complex pipelines:
Data Pipeline Data Source Data Sink Table ID Transform Route Submit Pipeline to Flink Cluster # Learn how to submit the pipeline to Flink cluster running on different deployment mode:
standalone Kubernetes YARN Development and Contribution # If you want to connect Flink CDC to your customized external system, or contributing to the framework itself, these sections could be helpful:
Understand Flink CDC APIs to develop your own Flink CDC connector Learn about how to contributing to Flink CDC Check out licenses used by Flink CDC `}),e.add({id:4,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/mongodb-tutorial/",title:"MongoDB Tutorial",section:"Tutorials",content:` Demo: MongoDB CDC to Elasticsearch # Create docker-compose.yml file using following contents: version: &#39;2.1&#39; services: mongo: image: &#34;mongo:4.0-xenial&#34; command: --replSet rs0 --smallfiles --oplogSize 128 ports: - &#34;27017:27017&#34; environment: - MONGO_INITDB_ROOT_USERNAME=mongouser - MONGO_INITDB_ROOT_PASSWORD=mongopw elasticsearch: image: elastic/elasticsearch:7.6.0 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34; - discovery.type=single-node ports: - &#34;9200:9200&#34; - &#34;9300:9300&#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 ports: - &#34;5601:5601&#34; Enter Mongodb&rsquo;s container and initialize replica set and data: docker-compose exec mongo /usr/bin/mongo -u mongouser -p mongopw // 1. initialize replica set rs.initiate(); rs.status(); // 2. switch database use mgdb; // 3. initialize data db.orders.insertMany([ { order_id: 101, order_date: ISODate(&#34;2020-07-30T10:08:22.001Z&#34;), customer_id: 1001, price: NumberDecimal(&#34;50.50&#34;), product: { name: &#39;scooter&#39;, description: &#39;Small 2-wheel scooter&#39; }, order_status: false }, { order_id: 102, order_date: ISODate(&#34;2020-07-30T10:11:09.001Z&#34;), customer_id: 1002, price: NumberDecimal(&#34;15.00&#34;), product: { name: &#39;car battery&#39;, description: &#39;12V car battery&#39; }, order_status: false }, { order_id: 103, order_date: ISODate(&#34;2020-07-30T12:00:30.001Z&#34;), customer_id: 1003, price: NumberDecimal(&#34;25.25&#34;), product: { name: &#39;hammer&#39;, description: &#39;16oz carpenter hammer&#39; }, order_status: false } ]); db.customers.insertMany([ { customer_id: 1001, name: &#39;Jark&#39;, address: &#39;Hangzhou&#39; }, { customer_id: 1002, name: &#39;Sally&#39;, address: &#39;Beijing&#39; }, { customer_id: 1003, name: &#39;Edward&#39;, address: &#39;Shanghai&#39; } ]); Download following JAR package to &lt;FLINK_HOME&gt;/lib/: Download links are available only for stable releases, SNAPSHOT dependencies need to be built based on master or release branches by yourself. flink-sql-connector-elasticsearch7-3.0.1-1.17.jar flink-sql-connector-mongodb-cdc-3.0-SNAPSHOT.jar Launch a Flink cluster, then start a Flink SQL CLI and execute following SQL statements inside: -- Flink SQL -- checkpoint every 3000 milliseconds Flink SQL&gt; SET execution.checkpointing.interval = 3s; -- set local time zone as Asia/Shanghai Flink SQL&gt; SET table.local-time-zone = Asia/Shanghai; Flink SQL&gt; CREATE TABLE orders ( _id STRING, order_id INT, order_date TIMESTAMP_LTZ(3), customer_id INT, price DECIMAL(10, 5), product ROW&lt;name STRING, description STRING&gt;, order_status BOOLEAN, PRIMARY KEY (_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mongodb-cdc&#39;, &#39;hosts&#39; = &#39;localhost:27017&#39;, &#39;username&#39; = &#39;mongouser&#39;, &#39;password&#39; = &#39;mongopw&#39;, &#39;database&#39; = &#39;mgdb&#39;, &#39;collection&#39; = &#39;orders&#39; ); Flink SQL&gt; CREATE TABLE customers ( _id STRING, customer_id INT, name STRING, address STRING, PRIMARY KEY (_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mongodb-cdc&#39;, &#39;hosts&#39; = &#39;localhost:27017&#39;, &#39;username&#39; = &#39;mongouser&#39;, &#39;password&#39; = &#39;mongopw&#39;, &#39;database&#39; = &#39;mgdb&#39;, &#39;collection&#39; = &#39;customers&#39; ); Flink SQL&gt; CREATE TABLE enriched_orders ( order_id INT, order_date TIMESTAMP_LTZ(3), customer_id INT, price DECIMAL(10, 5), product ROW&lt;name STRING, description STRING&gt;, order_status BOOLEAN, customer_name STRING, customer_address STRING, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;elasticsearch-7&#39;, &#39;hosts&#39; = &#39;http://localhost:9200&#39;, &#39;index&#39; = &#39;enriched_orders&#39; ); Flink SQL&gt; INSERT INTO enriched_orders SELECT o.order_id, o.order_date, o.customer_id, o.price, o.product, o.order_status, c.name, c. address FROM orders AS o LEFT JOIN customers AS c ON o.customer_id = c.customer_id; Make some changes in MongoDB, then check the result in Elasticsearch: db.orders.insert({ order_id: 104, order_date: ISODate(&#34;2020-07-30T12:00:30.001Z&#34;), customer_id: 1004, price: NumberDecimal(&#34;25.25&#34;), product: { name: &#39;rocks&#39;, description: &#39;box of assorted rocks&#39; }, order_status: false }); db.customers.insert({ customer_id: 1004, name: &#39;Jacob&#39;, address: &#39;Shanghai&#39; }); db.orders.updateOne( { order_id: 104 }, { $set: { order_status: true } } ); db.orders.deleteOne( { order_id : 104 } ); Back to top
`}),e.add({id:5,href:"/flink/flink-cdc-docs-master/docs/get-started/quickstart/mysql-to-doris/",title:"MySQL to Doris",section:"Quickstart",content:" Streaming ELT from MySQL to Doris # This tutorial is to show how to quickly build a Streaming ELT job from MySQL to Doris using Flink CDC, including the feature of sync all table of one database, schema change evolution and sync sharding tables into one table.\nAll exercises in this tutorial are performed in the Flink CDC CLI, and the entire process uses standard SQL syntax, without a single line of Java/Scala code or IDE installation.\nPreparation # Prepare a Linux or MacOS computer with Docker installed.\nPrepare Flink Standalone cluster # Download Flink 1.18.0 ，unzip and get flink-1.18.0 directory.\nUse the following command to navigate to the Flink directory and set FLINK_HOME to the directory where flink-1.18.0 is located.\ncd flink-1.18.0 Enable checkpointing by appending the following parameters to the conf/flink-conf.yaml configuration file to perform a checkpoint every 3 seconds.\nexecution.checkpointing.interval: 3000 Start the Flink cluster using the following command.\n./bin/start-cluster.sh If successfully started, you can access the Flink Web UI at http://localhost:8081/, as shown below.\nExecuting start-cluster.sh multiple times can start multiple TaskManagers.\nPrepare docker compose # The following tutorial will prepare the required components using docker-compose.\nHost Machine Configuration\nSince Doris requires memory mapping support for operation, execute the following command on the host machine:\nsysctl -w vm.max_map_count=2000000 Due to the different ways of implementing containers internally on MacOS, it may not be possible to directly modify the value of max_map_count on the host during deployment. You need to create the following containers first:\ndocker run -it --privileged --pid=host --name=change_count debian nsenter -t 1 -m -u -n -i sh The container was created successfully executing the following command:\nsysctl -w vm.max_map_count=2000000 Then exit exits and creates the Doris Docker cluster.\nStart docker compose Create a docker-compose.yml file using the content provided below:\nversion: &#39;2.1&#39; services: doris: image: yagagagaga/doris-standalone ports: - &#34;8030:8030&#34; - &#34;8040:8040&#34; - &#34;9030:9030&#34; mysql: image: debezium/example-mysql:1.1 ports: - &#34;3306:3306&#34; environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_USER=mysqluser - MYSQL_PASSWORD=mysqlpw The Docker Compose should include the following services (containers):\nMySQL: include a database named app_db Doris: to store tables from MySQL To start all containers, run the following command in the directory that contains the docker-compose.yml file.\ndocker-compose up -d This command automatically starts all the containers defined in the Docker Compose configuration in a detached mode. Run docker ps to check whether these containers are running properly. You can also visit http://localhost:8030/ to check whether Doris is running.\nPrepare records for MySQL # Enter MySQL container\ndocker-compose exec mysql mysql -uroot -p123456 create app_db database and orders,products,shipments tables, then insert records\n-- create database CREATE DATABASE app_db; USE app_db; -- create orders table CREATE TABLE `orders` ( `id` INT NOT NULL, `price` DECIMAL(10,2) NOT NULL, PRIMARY KEY (`id`) ); -- insert records INSERT INTO `orders` (`id`, `price`) VALUES (1, 4.00); INSERT INTO `orders` (`id`, `price`) VALUES (2, 100.00); -- create shipments table CREATE TABLE `shipments` ( `id` INT NOT NULL, `city` VARCHAR(255) NOT NULL, PRIMARY KEY (`id`) ); -- insert records INSERT INTO `shipments` (`id`, `city`) VALUES (1, &#39;beijing&#39;); INSERT INTO `shipments` (`id`, `city`) VALUES (2, &#39;xian&#39;); -- create products table CREATE TABLE `products` ( `id` INT NOT NULL, `product` VARCHAR(255) NOT NULL, PRIMARY KEY (`id`) ); -- insert records INSERT INTO `products` (`id`, `product`) VALUES (1, &#39;Beer&#39;); INSERT INTO `products` (`id`, `product`) VALUES (2, &#39;Cap&#39;); INSERT INTO `products` (`id`, `product`) VALUES (3, &#39;Peanut&#39;); Create database in Doris # Doris connector currently does not support automatic database creation and needs to first create a database corresponding to the write table.\nEnter Doris Web UI。\nhttp://localhost:8030/\nThe default username is root, and the default password is empty.\nCreate app_db database through Web UI.\ncreate database app_db; Submit job using FlinkCDC cli # Download the binary compressed packages listed below and extract them to the directory flink cdc-3.0.0 '： flink-cdc-3.0.0-bin.tar.gz flink-cdc-3.0.0 directory will contain four directory bin,lib,log,conf.\nDownload the connector package listed below and move it to the lib directory\nDownload links are available only for stable releases, SNAPSHOT dependencies need to be built based on master or release branches by yourself.\nMySQL pipeline connector 3.0.0 Apache Doris pipeline connector 3.0.0 MySQL Connector Java Write task configuration yaml file Here is an example file for synchronizing the entire database mysql-to-doris.yaml：\n################################################################################ # Description: Sync MySQL all tables to Doris ################################################################################ source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: doris fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; table.create.properties.light_schema_change: true table.create.properties.replication_num: 1 pipeline: name: Sync MySQL Database to Doris parallelism: 2 Notice that:\ntables: app_db.\\.* in source synchronize all tables in app_db through Regular Matching.\ntable.create.properties.replication_num in sink is because there is only one Doris BE node in the Docker image.\nFinally, submit job to Flink Standalone cluster using Cli. bash bin/flink-cdc.sh mysql-to-doris.yaml --jar lib/mysql-connector-java-8.0.27.jar After successful submission, the return information is as follows：\nPipeline has been submitted to cluster. Job ID: ae30f4580f1918bebf16752d4963dc54 Job Description: Sync MySQL Database to Doris We can find a job named Sync MySQL Database to Doris is running through Flink Web UI.\nWe can find that tables are created and inserted through Doris Web UI.\nSynchronize Schema and Data changes # Enter MySQL container\ndocker-compose exec mysql mysql -uroot -p123456 Then, modify schema and record in MySQL, and the tables of Doris will change the same in real time：\ninsert one record in orders from MySQL:\nINSERT INTO app_db.orders (id, price) VALUES (3, 100.00); add one column in orders from MySQL:\nALTER TABLE app_db.orders ADD amount varchar(100) NULL; update one record in orders from MySQL:\nUPDATE app_db.orders SET price=100.00, amount=100.00 WHERE id=1; delete one record in orders from MySQL:\nDELETE FROM app_db.orders WHERE id=2; Refresh the Doris Web UI every time you execute a step, and you can see that the orders table displayed in Doris will be updated in real-time, like the following：\nSimilarly, by modifying the &lsquo;shipments&rsquo; and&rsquo; products&rsquo; tables, you can also see the results of synchronized changes in real-time in Doris.\nRoute the changes # Flink CDC provides the configuration to route the table structure/data of the source table to other table names.\nWith this ability, we can achieve functions such as table name, database name replacement, and whole database synchronization. Here is an example file for using route feature:\n################################################################################ # Description: Sync MySQL all tables to Doris ################################################################################ source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: doris fenodes: 127.0.0.1:8030 benodes: 127.0.0.1:8040 username: root password: &#34;&#34; table.create.properties.light_schema_change: true table.create.properties.replication_num: 1 route: - source-table: app_db.orders sink-table: ods_db.ods_orders - source-table: app_db.shipments sink-table: ods_db.ods_shipments - source-table: app_db.products sink-table: ods_db.ods_products pipeline: name: Sync MySQL Database to Doris parallelism: 2 Using the upper route configuration, we can synchronize the table schema and data of app_db.orders to ods_db.ods_orders, thus achieving the function of database migration.\nSpecifically, source-table support regular expression matching with multiple tables to synchronize sharding databases and tables. like the following：\nroute: - source-table: app_db.order\\.* sink-table: ods_db.ods_orders In this way, we can synchronize sharding tables like app_db.order01、app_db.order02、app_db.order03 into one ods_db.ods_orders tables. Warning that there is currently no support for scenarios where the same primary key data exists in multiple tables, which will be supported in future versions.\nClean up # After finishing the tutorial, run the following command to stop all containers in the directory of docker-compose.yml:\ndocker-compose down Run the following command to stop the Flink cluster in the directory of Flink flink-1.18.0:\n./bin/stop-cluster.sh Back to top\n"}),e.add({id:6,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/overview/",title:"Overview",section:"Flink Sources",content:` Flink CDC sources # Flink CDC sources is a set of source connectors for Apache Flink®, ingesting changes from different databases using change data capture (CDC). Some CDC sources integrate Debezium as the engine to capture data changes. So it can fully leverage the ability of Debezium. See more about what is Debezium.
You can also read tutorials about how to use these sources.
Supported Connectors # Connector Database Driver mongodb-cdc MongoDB: 3.6, 4.x, 5.0 MongoDB Driver: 4.3.4 mysql-cdc MySQL: 5.6, 5.7, 8.0.x RDS MySQL: 5.6, 5.7, 8.0.x PolarDB MySQL: 5.6, 5.7, 8.0.x Aurora MySQL: 5.6, 5.7, 8.0.x MariaDB: 10.x PolarDB X: 2.0.1 JDBC Driver: 8.0.28 oceanbase-cdc OceanBase CE: 3.1.x, 4.x OceanBase EE: 2.x, 3.x, 4.x OceanBase Driver: 2.4.x oracle-cdc Oracle: 11, 12, 19, 21 Oracle Driver: 19.3.0.0 postgres-cdc PostgreSQL: 9.6, 10, 11, 12, 13, 14 JDBC Driver: 42.5.1 sqlserver-cdc Sqlserver: 2012, 2014, 2016, 2017, 2019 JDBC Driver: 9.4.1.jre8 tidb-cdc TiDB: 5.1.x, 5.2.x, 5.3.x, 5.4.x, 6.0.0 JDBC Driver: 8.0.27 db2-cdc Db2: 11.5 Db2 Driver: 11.5.0.0 vitess-cdc Vitess: 8.0.x, 9.0.x MySql JDBC Driver: 8.0.26 Supported Flink Versions # The following table shows the version mapping between Flink® CDC Connectors and Flink®:
Flink® CDC Version Flink® Version 1.0.0 1.11.* 1.1.0 1.11.* 1.2.0 1.12.* 1.3.0 1.12.* 1.4.0 1.13.* 2.0.* 1.13.* 2.1.* 1.13.* 2.2.* 1.13.*, 1.14.* 2.3.* 1.13.*, 1.14.*, 1.15.*, 1.16.* 2.4.* 1.13.*, 1.14.*, 1.15.*, 1.16.*, 1.17.* 3.0.* 1.14.*, 1.15.*, 1.16.*, 1.17.*, 1.18.* Features # Supports reading database snapshot and continues to read binlogs with exactly-once processing even failures happen. CDC connectors for DataStream API, users can consume changes on multiple databases and tables in a single job without Debezium and Kafka deployed. CDC connectors for Table/SQL API, users can use SQL DDL to create a CDC source to monitor changes on a single table. The following table shows the current features of the connector:
Connector No-lock Read Parallel Read Exactly-once Read Incremental Snapshot Read mongodb-cdc ✅ ✅ ✅ ✅ mysql-cdc ✅ ✅ ✅ ✅ oracle-cdc ✅ ✅ ✅ ✅ postgres-cdc ✅ ✅ ✅ ✅ sqlserver-cdc ✅ ✅ ✅ ✅ oceanbase-cdc ❌ ❌ ❌ ❌ tidb-cdc ✅ ❌ ✅ ❌ db2-cdc ✅ ✅ ✅ ✅ vitess-cdc ✅ ❌ ✅ ❌ Usage for Table/SQL API # We need several steps to setup a Flink cluster with the provided connector.
Setup a Flink cluster with version 1.12+ and Java 8+ installed. Download the connector SQL jars from the Downloads page (or build yourself). Put the downloaded jars under FLINK_HOME/lib/. Restart the Flink cluster. The example shows how to create a MySQL CDC source in Flink SQL Client and execute queries on it.
-- creates a mysql cdc table source CREATE TABLE mysql_binlog ( id INT NOT NULL, name STRING, description STRING, weight DECIMAL(10,3), PRIMARY KEY(id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;username&#39; = &#39;flinkuser&#39;, &#39;password&#39; = &#39;flinkpw&#39;, &#39;database-name&#39; = &#39;inventory&#39;, &#39;table-name&#39; = &#39;products&#39; ); -- read snapshot and binlog data from mysql, and do some transformation, and show on the client SELECT id, UPPER(name), description, weight FROM mysql_binlog; Usage for DataStream API # Include following Maven dependency (available through Maven Central):
&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;!-- add the dependency matching your database --&gt; &lt;artifactId&gt;flink-connector-mysql-cdc&lt;/artifactId&gt; &lt;!-- The dependency is available only for stable releases, SNAPSHOT dependencies need to be built based on master or release branches by yourself. --&gt; &lt;version&gt;3.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.cdc.connectors.mysql.source.MySqlSource; public class MySqlBinlogSourceExample { public static void main(String[] args) throws Exception { MySqlSource&lt;String&gt; mySqlSource = MySqlSource.&lt;String&gt;builder() .hostname(&#34;yourHostname&#34;) .port(yourPort) .databaseList(&#34;yourDatabaseName&#34;) // set captured database .tableList(&#34;yourDatabaseName.yourTableName&#34;) // set captured table .username(&#34;yourUsername&#34;) .password(&#34;yourPassword&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000); env .fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), &#34;MySQL Source&#34;) // set 4 parallel source tasks .setParallelism(4) .print().setParallelism(1); // use parallelism 1 for sink to keep message ordering env.execute(&#34;Print MySQL Snapshot + Binlog&#34;); } } Deserialization # The following JSON data show the change event in JSON format.
{ &#34;before&#34;: { &#34;id&#34;: 111, &#34;name&#34;: &#34;scooter&#34;, &#34;description&#34;: &#34;Big 2-wheel scooter&#34;, &#34;weight&#34;: 5.18 }, &#34;after&#34;: { &#34;id&#34;: 111, &#34;name&#34;: &#34;scooter&#34;, &#34;description&#34;: &#34;Big 2-wheel scooter&#34;, &#34;weight&#34;: 5.15 }, &#34;source&#34;: {...}, &#34;op&#34;: &#34;u&#34;, // the operation type, &#34;u&#34; means this this is an update event &#34;ts_ms&#34;: 1589362330904, // the time at which the connector processed the event &#34;transaction&#34;: null } Note: Please refer Debezium documentation to know the meaning of each field.
In some cases, users can use the JsonDebeziumDeserializationSchema(true) Constructor to enabled include schema in the message. Then the Debezium JSON message may look like this:
{ &#34;schema&#34;: { &#34;type&#34;: &#34;struct&#34;, &#34;fields&#34;: [ { &#34;type&#34;: &#34;struct&#34;, &#34;fields&#34;: [ { &#34;type&#34;: &#34;int32&#34;, &#34;optional&#34;: false, &#34;field&#34;: &#34;id&#34; }, { &#34;type&#34;: &#34;string&#34;, &#34;optional&#34;: false, &#34;default&#34;: &#34;flink&#34;, &#34;field&#34;: &#34;name&#34; }, { &#34;type&#34;: &#34;string&#34;, &#34;optional&#34;: true, &#34;field&#34;: &#34;description&#34; }, { &#34;type&#34;: &#34;double&#34;, &#34;optional&#34;: true, &#34;field&#34;: &#34;weight&#34; } ], &#34;optional&#34;: true, &#34;name&#34;: &#34;mysql_binlog_source.inventory_1pzxhca.products.Value&#34;, &#34;field&#34;: &#34;before&#34; }, { &#34;type&#34;: &#34;struct&#34;, &#34;fields&#34;: [ { &#34;type&#34;: &#34;int32&#34;, &#34;optional&#34;: false, &#34;field&#34;: &#34;id&#34; }, { &#34;type&#34;: &#34;string&#34;, &#34;optional&#34;: false, &#34;default&#34;: &#34;flink&#34;, &#34;field&#34;: &#34;name&#34; }, { &#34;type&#34;: &#34;string&#34;, &#34;optional&#34;: true, &#34;field&#34;: &#34;description&#34; }, { &#34;type&#34;: &#34;double&#34;, &#34;optional&#34;: true, &#34;field&#34;: &#34;weight&#34; } ], &#34;optional&#34;: true, &#34;name&#34;: &#34;mysql_binlog_source.inventory_1pzxhca.products.Value&#34;, &#34;field&#34;: &#34;after&#34; }, { &#34;type&#34;: &#34;struct&#34;, &#34;fields&#34;: {...}, &#34;optional&#34;: false, &#34;name&#34;: &#34;io.debezium.connector.mysql.Source&#34;, &#34;field&#34;: &#34;source&#34; }, { &#34;type&#34;: &#34;string&#34;, &#34;optional&#34;: false, &#34;field&#34;: &#34;op&#34; }, { &#34;type&#34;: &#34;int64&#34;, &#34;optional&#34;: true, &#34;field&#34;: &#34;ts_ms&#34; } ], &#34;optional&#34;: false, &#34;name&#34;: &#34;mysql_binlog_source.inventory_1pzxhca.products.Envelope&#34; }, &#34;payload&#34;: { &#34;before&#34;: { &#34;id&#34;: 111, &#34;name&#34;: &#34;scooter&#34;, &#34;description&#34;: &#34;Big 2-wheel scooter&#34;, &#34;weight&#34;: 5.18 }, &#34;after&#34;: { &#34;id&#34;: 111, &#34;name&#34;: &#34;scooter&#34;, &#34;description&#34;: &#34;Big 2-wheel scooter&#34;, &#34;weight&#34;: 5.15 }, &#34;source&#34;: {...}, &#34;op&#34;: &#34;u&#34;, // the operation type, &#34;u&#34; means this this is an update event &#34;ts_ms&#34;: 1589362330904, // the time at which the connector processed the event &#34;transaction&#34;: null } } Usually, it is recommended to exclude schema because schema fields makes the messages very verbose which reduces parsing performance.
The JsonDebeziumDeserializationSchema can also accept custom configuration of JsonConverter, for example if you want to obtain numeric output for decimal data, you can construct JsonDebeziumDeserializationSchema as following:
Map&lt;String, Object&gt; customConverterConfigs = new HashMap&lt;&gt;(); customConverterConfigs.put(JsonConverterConfig.DECIMAL_FORMAT_CONFIG, &#34;numeric&#34;); JsonDebeziumDeserializationSchema schema = new JsonDebeziumDeserializationSchema(true, customConverterConfigs); Building from source # Prerequisites:
git Maven At least Java 8 git clone https://github.com/apache/flink-cdc.git cd flink-cdc mvn clean install -DskipTests The dependencies are now available in your local .m2 repository.
Back to top
`}),e.add({id:7,href:"/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/overview/",title:"Overview",section:"Pipeline Connectors",content:` Connectors # Flink CDC provides several source and sink connectors to interact with external systems. You can use these connectors out-of-box, by adding released JARs to your Flink CDC environment, and specifying the connector in your YAML pipeline definition.
Supported Connectors # Connector Supported Type External System Apache Doris Sink Apache Doris: 1.2.x, 2.x.x Kafka Sink Kafka MySQL Source MySQL: 5.6, 5.7, 8.0.x RDS MySQL: 5.6, 5.7, 8.0.x PolarDB MySQL: 5.6, 5.7, 8.0.x Aurora MySQL: 5.6, 5.7, 8.0.x MariaDB: 10.x PolarDB X: 2.0.1 Paimon Sink Paimon: 0.6, 0.7, 0.8 StarRocks Sink StarRocks: 2.x, 3.x Develop Your Own Connector # If provided connectors cannot fulfill your requirement, you can always develop your own connector to get your external system involved in Flink CDC pipelines. Check out Flink CDC APIs to learn how to develop your own connectors.
Back to top
`}),e.add({id:8,href:"/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/",title:"Pipeline Connectors",section:"Connectors",content:" "}),e.add({id:9,href:"/flink/flink-cdc-docs-master/docs/deployment/standalone/",title:"Standalone",section:"Deployment",content:` Introduction # Standalone mode is Flink’s simplest deployment mode. This short guide will show you how to download the latest stable version of Flink, install, and run it. You will also run an example Flink CDC job and view it in the web UI.
Preparation # Flink runs on all UNIX-like environments, i.e. Linux, Mac OS X, and Cygwin (for Windows).
You can refer overview to check supported versions and download the binary release of Flink, then extract the archive:
tar -xzf flink-*.tgz You should set FLINK_HOME environment variables like:
export FLINK_HOME=/path/flink-* Start and stop a local cluster # To start a local cluster, run the bash script that comes with Flink:
cd /path/flink-* ./bin/start-cluster.sh Flink is now running as a background process. You can check its status with the following command:
ps aux | grep flink You should be able to navigate to the web UI at localhost:8081 to view the Flink dashboard and see that the cluster is up and running.
To quickly stop the cluster and all running components, you can use the provided script:
./bin/stop-cluster.sh Set up Flink CDC # Download the tar file of Flink CDC from release page, then extract the archive:
tar -xzf flink-cdc-*.tar.gz Extracted flink-cdc contains four directories: bin,lib,log and conf.
Download the connector jars from release page, and move it to the lib directory. Download links are available only for stable releases, SNAPSHOT dependencies need to be built based on specific branch by yourself.
Submit a Flink CDC Job # Here is an example file for synchronizing the entire database mysql-to-doris.yaml：
################################################################################ # Description: Sync MySQL all tables to Doris ################################################################################ source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: doris fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; pipeline: name: Sync MySQL Database to Doris parallelism: 2 You need to modify the configuration file according to your needs, refer to connectors more information.
MySQL pipeline connector Apache Doris pipeline connector Finally, submit job to Flink Standalone cluster using Cli.
cd /path/flink-cdc-* ./bin/flink-cdc.sh mysql-to-doris.yaml After successful submission, the return information is as follows：
Pipeline has been submitted to cluster. Job ID: ae30f4580f1918bebf16752d4963dc54 Job Description: Sync MySQL Database to Doris Then you can find a job named Sync MySQL Database to Doris running through Flink Web UI.
`}),e.add({id:10,href:"/flink/flink-cdc-docs-master/docs/developer-guide/understand-flink-cdc-api/",title:"Understand Flink CDC API",section:"Developer Guide",content:` Understand Flink CDC API # If you are planning to build your own Flink CDC connectors, or considering contributing to Flink CDC, you might want to hava a deeper look at the APIs of Flink CDC. This document will go through some important concepts and interfaces in order to help you with your development.
Event # An event under the context of Flink CDC is a special kind of record in Flink&rsquo;s data stream. It describes the captured changes in the external system on source side, gets processed and transformed by internal operators built by Flink CDC, and finally passed to data sink then write or applied to the external system on sink side.
Each change event contains the table ID it belongs to, and the payload that the event carries. Based on the type of payload, we categorize events into these kinds:
DataChangeEvent # DataChangeEvent describes data changes in the source. It consists of 5 fields
Table ID: table ID it belongs to Before: pre-image of the data After: post-image of the data Operation type: type of the change operation Meta: metadata of the change For the operation type field, we pre-define 4 operation types:
Insert: new data entry, with before = null and after = new data Delete: removal of data, with before = removed data and after = null Update: update of existed data, with before = data before change and after = data after change Replace: SchemaChangeEvent # SchemaChangeEvent describes schema changes in the source. Compared to DataChangeEvent, the payload of SchemaChangeEvent describes changes in the table structure in the external system, including:
AddColumnEvent: new column in the table AlterColumnTypeEvent: type change of a column CreateTableEvent: creation of a new table. Also used to describe the schema of a pre-emitted DataChangeEvent DropColumnEvent: removal of a column RenameColumnEvent: name change of a column Flow of Events # As you may have noticed, data change event doesn&rsquo;t have its schema bound with it. This reduces the size of data change event and the overhead of serialization, but makes it not self-descriptive Then how does the framework know how to interpret the data change event?
To resolve the problem, the framework adds a requirement to the flow of events: a CreateTableEvent must be emitted before any DataChangeEvent if a table is new to the framework, and SchemaChangeEvent must be emitted before any DataChangeEvent if the schema of a table is changed. This requirement makes sure that the framework has been aware of the schema before processing any data changes.
Data Source # Data source works as a factory of EventSource and MetadataAccessor, constructing runtime implementations of source that captures changes from external system and provides metadata.
EventSource is a Flink source that reads changes, converts them to events , then emits to downstream Flink operators. You can refer to Flink documentation to learn internals and how to implement a Flink source.
MetadataAccessor serves as the metadata reader of the external system, by listing namespaces, schemas and tables, and provide the table schema (table structure) of the given table ID.
Data Sink # Symmetrical with data source, data sink consists of EventSink and MetadataApplier, which writes data change events and apply schema changes (metadata changes) to external system.
EventSink is a Flink sink that receives change event from upstream operator, and apply them to the external system. Currently we only support Flink&rsquo;s Sink V2 API.
MetadataApplier will be used to handle schema changes. When the framework receives schema change event from source, after making some internal synchronizations and flushes, it will apply the schema change to external system via this applier.
`}),e.add({id:11,href:"/flink/flink-cdc-docs-master/docs/developer-guide/contribute-to-flink-cdc/",title:"Contribute to Flink CDC",section:"Developer Guide",content:` Contributing # Flink CDC is developed by an open and friendly community and welcomes anyone who wants to help out in any way. There are several ways to interact with the community and contribute to Flink CDC including asking questions, filing bug reports, proposing new features, joining discussions on the mailing lists, contributing code or documentation, improving website, testing release candidates and writing corresponding blog etc.
What do you want to do # Contributing to Flink CDC goes beyond writing code for the project. Here are different opportunities to help the project as follows.
Area Further information Report Bug To report a problem, open an issue in Flink jira and select Flink CDC in Component/s. Please give detailed information about the problem you encountered and, if possible, add a description that helps to reproduce the problem. Contribute Code Read the Code Contribution Guide Code Reviews Read the Code Review Guide Support Users Reply to questions on the flink user mailing list, check the latest issues in Flink jira for tickets which are actually user questions. Any other question? Reach out to the Dev mail list to get help!
Code Contribution Guide Flink CDC is maintained, improved, and extended by code contributions of volunteers. We welcome contributions.
Please feel free to ask questions at any time. Either send a mail to the Dev mailing list or comment on the issue you are working on.
If you would like to contribute to Flink CDC, you could raise it as follows.
Left comment under the issue that you want to take. (It&rsquo;s better to explain your understanding of the issue, and your design, and if possible, you need to provide your POC code) Start to implement it after this issue is assigned to you. (Commit message should follow the format [FLINK-xxx][xxx] xxxxxxx) Create a PR to Flink CDC. (Please enable the actions of your own clone project) Find a reviewer to review your PR and make sure the CI passed A committer of Flink CDC checks if the contribution fulfills the requirements and merges the code to the codebase. Code Review Guide Every review needs to check the following aspects.
Is This Pull Request Well-Described? Check whether this pull request is sufficiently well-described to support a good review. Trivial changes and fixes do not need a long description.
How Is the Overall Code Quality, Meeting Standard We Want to Maintain? Does the code follow the right software engineering practices? Is the code correct, robust, maintainable, testable? Are the changes performance aware, when changing a performance sensitive part? Are the changes sufficiently covered by tests? Are the tests executing fast? If dependencies have been changed, were the NOTICE files updated? Does the commit message follow the required format? Are the Documentation Updated? If the pull request introduces a new feature, the feature should be documented.
`}),e.add({id:12,href:"/flink/flink-cdc-docs-master/docs/core-concept/",title:"Core Concept",section:"Docs",content:" "}),e.add({id:13,href:"/flink/flink-cdc-docs-master/docs/core-concept/data-source/",title:"Data Source",section:"Core Concept",content:` Definition # Data Source is used to access metadata and read the changed data from external systems.
A Data Source can read data from multiple tables simultaneously.
Parameters # To describe a data source, the follows are required:
parameter meaning optional/required type The type of the source, such as mysql. required name The name of the source, which is user-defined (a default value provided). optional configurations of Data Source Configurations to build the Data Source e.g. connection configurations and source table properties. optional Example # We could use yaml files to define a mysql source:
source: type: mysql name: mysql-source #optional，description information host: localhost port: 3306 username: admin password: pass tables: adb.*, bdb.user_table_[0-9]+, [app|web]_order_\\.* `}),e.add({id:14,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/db2-tutorial/",title:"Db2 Tutorial",section:"Tutorials",content:` Demo: Db2 CDC to Elasticsearch # 1. Create docker-compose.yml file using following contents:
version: &#39;2.1&#39; services: db2: image: ruanhang/db2-cdc-demo:v1 privileged: true ports: - 50000:50000 environment: - LICENSE=accept - DB2INSTANCE=db2inst1 - DB2INST1_PASSWORD=admin - DBNAME=testdb - ARCHIVE_LOGS=true elasticsearch: image: elastic/elasticsearch:7.6.0 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34; - discovery.type=single-node ports: - &#34;9200:9200&#34; - &#34;9300:9300&#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 ports: - &#34;5601:5601&#34; volumes: - /var/run/docker.sock:/var/run/docker.sock The Docker Compose environment consists of the following containers:
Db2: db2 server and a pre-populated products table in the database testdb. Elasticsearch: store the result of the products table. Kibana: mainly used to visualize the data in Elasticsearch To start all containers, run the following command in the directory that contains the docker-compose.yml file.
docker-compose up -d This command automatically starts all the containers defined in the Docker Compose configuration in a detached mode. Run docker ps to check whether these containers are running properly. You can also visit http://localhost:5601/ to see if Kibana is running normally.
Don’t forget to run the following command to stop all containers after you finished the tutorial:
docker-compose down 2. Download following JAR package to &lt;FLINK_HOME&gt;/lib
Download links are available only for stable releases, SNAPSHOT dependencies need to be built based on master or release branches by yourself.
flink-sql-connector-elasticsearch7-3.0.1-1.17.jar flink-sql-connector-db2-cdc-3.0-SNAPSHOT.jar 3. Launch a Flink cluster and start a Flink SQL CLI
Execute following SQL statements in the Flink SQL CLI:
-- Flink SQL -- checkpoint every 3000 milliseconds Flink SQL&gt; SET execution.checkpointing.interval = 3s; Flink SQL&gt; CREATE TABLE products ( ID INT NOT NULL, NAME STRING, DESCRIPTION STRING, WEIGHT DECIMAL(10,3), PRIMARY KEY (ID) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;db2-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;50000&#39;, &#39;username&#39; = &#39;db2inst1&#39;, &#39;password&#39; = &#39;admin&#39;, &#39;database-name&#39; = &#39;TESTDB&#39;, &#39;table-name&#39; = &#39;DB2INST1.PRODUCTS&#39; ); Flink SQL&gt; CREATE TABLE es_products ( ID INT NOT NULL, NAME STRING, DESCRIPTION STRING, WEIGHT DECIMAL(10,3), PRIMARY KEY (ID) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;elasticsearch-7&#39;, &#39;hosts&#39; = &#39;http://localhost:9200&#39;, &#39;index&#39; = &#39;enriched_products_1&#39; ); Flink SQL&gt; INSERT INTO es_products SELECT * FROM products; 4. Check result in Elasticsearch
Check the data has been written to Elasticsearch successfully, you can visit Kibana to see the data.
5. Make changes in Db2 and watch result in Elasticsearch
Enter Db2&rsquo;s container to make some changes in Db2, then you can see the result in Elasticsearch will change after executing every SQL statement:
docker exec -it \${containerId} /bin/bash su db2inst1 db2 connect to testdb # enter db2 and execute sqls db2 UPDATE DB2INST1.PRODUCTS SET DESCRIPTION=&#39;18oz carpenter hammer&#39; WHERE ID=106; INSERT INTO DB2INST1.PRODUCTS VALUES (default,&#39;jacket&#39;,&#39;water resistent white wind breaker&#39;,0.2); INSERT INTO DB2INST1.PRODUCTS VALUES (default,&#39;scooter&#39;,&#39;Big 2-wheel scooter &#39;,5.18); DELETE FROM DB2INST1.PRODUCTS WHERE ID=111; Back to top
`}),e.add({id:15,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/",title:"Flink Sources",section:"Connectors",content:" "}),e.add({id:16,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/mysql-cdc/",title:"MySQL",section:"Flink Sources",content:` MySQL CDC Connector # The MySQL CDC connector allows for reading snapshot data and incremental data from MySQL database. This document describes how to setup the MySQL CDC connector to run SQL queries against MySQL databases.
Supported Databases # Connector Database Driver mysql-cdc MySQL: 5.6, 5.7, 8.0.x RDS MySQL: 5.6, 5.7, 8.0.x PolarDB MySQL: 5.6, 5.7, 8.0.x Aurora MySQL: 5.6, 5.7, 8.0.x MariaDB: 10.x PolarDB X: 2.0.1 JDBC Driver: 8.0.27 Dependencies # In order to setup the MySQL CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency # &ltdependency&gt &ltgroupId&gtorg.apache.flink&lt/groupId&gt &ltartifactId&gtflink-connector-mysql-cdc&lt/artifactId&gt &ltversion&gt3.2-SNAPSHOT&lt/version&gt &lt/dependency&gt Copied to clipboard! SQL Client JAR # Download link is available only for stable releases.
Download flink-sql-connector-mysql-cdc-3.0.1.jar and put it under &lt;FLINK_HOME&gt;/lib/.
Note: Refer to flink-sql-connector-mysql-cdc, more released versions will be available in the Maven central warehouse.
Since MySQL Connector&rsquo;s GPLv2 license is incompatible with Flink CDC project, we can&rsquo;t provide MySQL connector in prebuilt connector jar packages. You may need to configure the following dependencies manually.
Dependency Item Description mysql:mysql-connector-java:8.0.27 Used for connecting to MySQL database. Setup MySQL server # You have to define a MySQL user with appropriate permissions on all databases that the Debezium MySQL connector monitors.
Create the MySQL user: mysql&gt; CREATE USER &#39;user&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;password&#39;; Grant the required permissions to the user: mysql&gt; GRANT SELECT, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO &#39;user&#39; IDENTIFIED BY &#39;password&#39;; Note: The RELOAD permissions is not required any more when scan.incremental.snapshot.enabled is enabled (enabled by default).
Finalize the user’s permissions: mysql&gt; FLUSH PRIVILEGES; See more about the permission explanation.
Notes # Set a different SERVER ID for each reader # Every MySQL database client for reading binlog should have a unique id, called server id. MySQL server will use this id to maintain network connection and the binlog position. Therefore, if different jobs share a same server id, it may result to read from wrong binlog position. Thus, it is recommended to set different server id for each reader via the SQL Hints, e.g. assuming the source parallelism is 4, then we can use SELECT * FROM source_table /*+ OPTIONS('server-id'='5401-5404') */ ; to assign unique server id for each of the 4 source readers.
Setting up MySQL session timeouts # When an initial consistent snapshot is made for large databases, your established connection could timeout while the tables are being read. You can prevent this behavior by configuring interactive_timeout and wait_timeout in your MySQL configuration file.
interactive_timeout: The number of seconds the server waits for activity on an interactive connection before closing it. See MySQL documentations. wait_timeout: The number of seconds the server waits for activity on a noninteractive connection before closing it. See MySQL documentations. How to create a MySQL CDC table # The MySQL CDC table can be defined as following:
-- checkpoint every 3000 milliseconds Flink SQL&gt; SET &#39;execution.checkpointing.interval&#39; = &#39;3s&#39;; -- register a MySQL table &#39;orders&#39; in Flink SQL Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY(order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;username&#39; = &#39;root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;orders&#39;); -- read snapshot and binlogs from orders table Flink SQL&gt; SELECT * FROM orders; Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'mysql-cdc'. hostname required (none) String IP address or hostname of the MySQL database server. username required (none) String Name of the MySQL database to use when connecting to the MySQL database server. password required (none) String Password to use when connecting to the MySQL database server. database-name required (none) String Database name of the MySQL server to monitor. The database-name also supports regular expressions to monitor multiple tables matches the regular expression. table-name required (none) String Table name of the MySQL database to monitor. The table-name also supports regular expressions to monitor multiple tables that satisfy the regular expressions. Note: When the MySQL CDC connector regularly matches the table name, it will concat the database-name and table-name filled in by the user through the string \`\\\\.\` to form a full-path regular expression, and then use the regular expression to match the fully qualified name of the table in the MySQL database. port optional 3306 Integer Integer port number of the MySQL database server. server-id optional (none) String A numeric ID or a numeric ID range of this database client, The numeric ID syntax is like '5400', the numeric ID range syntax is like '5400-5408', The numeric ID range syntax is recommended when 'scan.incremental.snapshot.enabled' enabled. Every ID must be unique across all currently-running database processes in the MySQL cluster. This connector joins the MySQL cluster as another server (with this unique ID) so it can read the binlog. By default, a random number is generated between 5400 and 6400, though we recommend setting an explicit value. scan.incremental.snapshot.enabled optional true Boolean Incremental snapshot is a new mechanism to read snapshot of a table. Compared to the old snapshot mechanism, the incremental snapshot has many advantages, including: (1) source can be parallel during snapshot reading, (2) source can perform checkpoints in the chunk granularity during snapshot reading, (3) source doesn't need to acquire global read lock (FLUSH TABLES WITH READ LOCK) before snapshot reading. If you would like the source run in parallel, each parallel reader should have an unique server id, so the 'server-id' must be a range like '5400-6400', and the range must be larger than the parallelism. Please see Incremental Snapshot Readingsection for more detailed information. scan.incremental.snapshot.chunk.size optional 8096 Integer The chunk size (number of rows) of table snapshot, captured tables are split into multiple chunks when read the snapshot of table. scan.snapshot.fetch.size optional 1024 Integer The maximum fetch size for per poll when read table snapshot. scan.startup.mode optional initial String Optional startup mode for MySQL CDC consumer, valid enumerations are "initial", "earliest-offset", "latest-offset", "specific-offset" and "timestamp". Please see Startup Reading Position section for more detailed information. scan.startup.specific-offset.file optional (none) String Optional binlog file name used in case of "specific-offset" startup mode scan.startup.specific-offset.pos optional (none) Long Optional binlog file position used in case of "specific-offset" startup mode scan.startup.specific-offset.gtid-set optional (none) String Optional GTID set used in case of "specific-offset" startup mode scan.startup.specific-offset.skip-events optional (none) Long Optional number of events to skip after the specific starting offset scan.startup.specific-offset.skip-rows optional (none) Long Optional number of rows to skip after the specific starting offset server-time-zone optional (none) String The session time zone in database server, e.g. "Asia/Shanghai". It controls how the TIMESTAMP type in MYSQL converted to STRING. See more here. If not set, then ZoneId.systemDefault() is used to determine the server time zone. debezium.min.row. count.to.stream.result optional 1000 Integer During a snapshot operation, the connector will query each included table to produce a read event for all rows in that table. This parameter determines whether the MySQL connection will pull all results for a table into memory (which is fast but requires large amounts of memory), or whether the results will instead be streamed (can be slower, but will work for very large tables). The value specifies the minimum number of rows a table must contain before the connector will stream results, and defaults to 1,000. Set this parameter to '0' to skip all table size checks and always stream all results during a snapshot. connect.timeout optional 30s Duration The maximum time that the connector should wait after trying to connect to the MySQL database server before timing out. connect.max-retries optional 3 Integer The max retry times that the connector should retry to build MySQL database server connection. connection.pool.size optional 20 Integer The connection pool size. jdbc.properties.* optional 20 String Option to pass custom JDBC URL properties. User can pass custom properties like 'jdbc.properties.useSSL' = 'false'. heartbeat.interval optional 30s Duration The interval of sending heartbeat event for tracing the latest available binlog offsets. debezium.* optional (none) String Pass-through Debezium's properties to Debezium Embedded Engine which is used to capture data changes from MySQL server. For example: 'debezium.snapshot.mode' = 'never'. See more about the Debezium's MySQL Connector properties scan.incremental.close-idle-reader.enabled optional false Boolean Whether to close idle readers at the end of the snapshot phase. The flink version is required to be greater than or equal to 1.14 when 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' is set to true.
If the flink version is greater than or equal to 1.15, the default value of 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' has been changed to true, so it does not need to be explicitly configured 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' = 'true' debezium.binary.handling.mode optional (none) String debezium.binary.handling.mode can be set to one of the following values: none: No processing is performed, and the binary data type is transmitted as a byte array (byte array). base64: The binary data type is converted to a Base64-encoded string and transmitted. hex: The binary data type is converted to a hexadecimal string and transmitted. The default value is none. Depending on your requirements and data types, you can choose the appropriate processing mode. If your database contains a large number of binary data types, it is recommended to use base64 or hex mode to make it easier to handle during transmission. Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Key DataType Description table_name STRING NOT NULL Name of the table that contain the row. database_name STRING NOT NULL Name of the database that contain the row. op_ts TIMESTAMP_LTZ(3) NOT NULL It indicates the time that the change was made in the database. If the record is read from snapshot of the table instead of the binlog, the value is always 0. row_kind STRING NOT NULL It indicates the row kind of the changelog,Note: The downstream SQL operator may fail to compare due to this new added column when processing the row retraction if the source operator chooses to output the 'row_kind' column for each record. It is recommended to use this metadata column only in simple synchronization jobs. '+I' means INSERT message, '-D' means DELETE message, '-U' means UPDATE_BEFORE message and '+U' means UPDATE_AFTER message. The extended CREATE TABLE example demonstrates the syntax for exposing these metadata fields:
CREATE TABLE products ( db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, table_name STRING METADATA FROM &#39;table_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, operation STRING METADATA FROM &#39;row_kind&#39; VIRTUAL, order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;username&#39; = &#39;root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;orders&#39; ); The extended CREATE TABLE example demonstrates the usage of regex to match multi-tables:
CREATE TABLE products ( db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, table_name STRING METADATA FROM &#39;table_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, operation STRING METADATA FROM &#39;row_kind&#39; VIRTUAL, order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;username&#39; = &#39;root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;(^(test).*|^(tpc).*|txc|.*[p$]|t{2})&#39;, &#39;table-name&#39; = &#39;(t[5-8]|tt)&#39; ); example expression description prefix match ^(test).* This matches the database name or table name starts with prefix of test, e.g test1、test2. suffix match .*[p$] This matches the database name or table name ends with suffix of p, e.g cdcp、edcp. specific match txc This matches the database name or table name according to a specific name, e.g txc. It will use database-name\\\\.table-name as a pattern to match tables, as above examples using pattern (^(test).*|^(tpc).*|txc|.*[p$]|t{2})\\\\.(t[5-8]|tt) matches txc.tt、test2.test5.
Features # Incremental Snapshot Reading # Incremental snapshot reading is a new mechanism to read snapshot of a table. Compared to the old snapshot mechanism, the incremental snapshot has many advantages, including:
(1) MySQL CDC Source can be parallel during snapshot reading (2) MySQL CDC Source can perform checkpoints in the chunk granularity during snapshot reading (3) MySQL CDC Source doesn&rsquo;t need to acquire global read lock (FLUSH TABLES WITH READ LOCK) before snapshot reading If you would like the source run in parallel, each parallel reader should have an unique server id, so the &lsquo;server-id&rsquo; must be a range like &lsquo;5400-6400&rsquo;, and the range must be larger than the parallelism.
During the incremental snapshot reading, the MySQL CDC Source firstly splits snapshot chunks (splits) by primary key of table, and then MySQL CDC Source assigns the chunks to multiple readers to read the data of snapshot chunk.
Controlling Parallelism # Incremental snapshot reading provides the ability to read snapshot data parallelly. You can control the source parallelism by setting the job parallelism parallelism.default. For example, in SQL CLI:
Flink SQL&gt; SET &#39;parallelism.default&#39; = 8; Checkpoint # Incremental snapshot reading provides the ability to perform checkpoint in chunk level. It resolves the checkpoint timeout problem in previous version with old snapshot reading mechanism.
Lock-free # The MySQL CDC source use incremental snapshot algorithm, which avoid acquiring global read lock (FLUSH TABLES WITH READ LOCK) and thus doesn&rsquo;t need RELOAD permission.
MySQL High Availability Support # The mysql-cdc connector offers high availability of MySQL high available cluster by using the GTID information. To obtain the high availability, the MySQL cluster need enable the GTID mode, the GTID mode in your mysql config file should contain following settings:
gtid_mode = on enforce_gtid_consistency = on If the monitored MySQL server address contains slave instance, you need set following settings to the MySQL conf file. The setting log-slave-updates = 1 enables the slave instance to also write the data that synchronized from master to its binlog, this makes sure that the mysql-cdc connector can consume entire data from the slave instance.
gtid_mode = on enforce_gtid_consistency = on log-slave-updates = 1 After the server you monitored fails in MySQL cluster, you only need to change the monitored server address to other available server and then restart the job from the latest checkpoint/savepoint, the job will restore from the checkpoint/savepoint and won&rsquo;t miss any records.
It&rsquo;s recommended to configure a DNS(Domain Name Service) or VIP(Virtual IP Address) for your MySQL cluster, using the DNS or VIP address for mysql-cdc connector, the DNS or VIP would automatically route the network request to the active MySQL server. In this way, you don&rsquo;t need to modify the address and restart your pipeline anymore.
MySQL Heartbeat Event Support # If the table updates infrequently, the binlog file or GTID set may have been cleaned in its last committed binlog position. The CDC job may restart fails in this case. So the heartbeat event will help update binlog position. By default heartbeat event is enabled in MySQL CDC source and the interval is set to 30 seconds. You can specify the interval by using table option heartbeat.interval, or set the option to 0s to disable heartbeat events.
How Incremental Snapshot Reading works # When the MySQL CDC source is started, it reads snapshot of table parallelly and then reads binlog of table with single parallelism.
In snapshot phase, the snapshot is cut into multiple snapshot chunks according to primary key of table and the size of table rows. Snapshot chunks is assigned to multiple snapshot readers. Each snapshot reader reads its received chunks with chunk reading algorithm and send the read data to downstream. The source manages the process status (finished or not) of chunks, thus the source of snapshot phase can support checkpoint in chunk level. If a failure happens, the source can be restored and continue to read chunks from last finished chunks.
After all snapshot chunks finished, the source will continue to read binlog in a single task. In order to guarantee the global data order of snapshot records and binlog records, binlog reader will start to read data until there is a complete checkpoint after snapshot chunks finished to make sure all snapshot data has been consumed by downstream. The binlog reader tracks the consumed binlog position in state, thus source of binlog phase can support checkpoint in row level.
Flink performs checkpoints for the source periodically, in case of failover, the job will restart and restore from the last successful checkpoint state and guarantees the exactly once semantic.
Snapshot Chunk Splitting # When performing incremental snapshot reading, MySQL CDC source need a criterion which used to split the table. MySQL CDC Source use a splitting column to split the table to multiple splits (chunks). By default, MySQL CDC source will identify the primary key column of the table and use the first column in primary key as the splitting column. If there is no primary key in the table, incremental snapshot reading will fail and you can disable scan.incremental.snapshot.enabled to fallback to old snapshot reading mechanism.
For numeric and auto incremental splitting column, MySQL CDC Source efficiently splits chunks by fixed step length. For example, if you had a table with a primary key column of id which is auto-incremental BIGINT type, the minimum value was 0 and maximum value was 100, and the table option scan.incremental.snapshot.chunk.size value is 25, the table would be split into following chunks:
(-∞, 25), [25, 50), [50, 75), [75, 100), [100, +∞) For other primary key column type, MySQL CDC Source executes the statement in the form of SELECT MAX(STR_ID) AS chunk_high FROM (SELECT * FROM TestTable WHERE STR_ID &gt; 'uuid-001' limit 25) to get the low and high value for each chunk, the splitting chunks set would be like:
(-∞, &#39;uuid-001&#39;), [&#39;uuid-001&#39;, &#39;uuid-009&#39;), [&#39;uuid-009&#39;, &#39;uuid-abc&#39;), [&#39;uuid-abc&#39;, &#39;uuid-def&#39;), [uuid-def, +∞). Chunk Reading Algorithm # For above example MyTable, if the MySQL CDC Source parallelism was set to 4, MySQL CDC Source would run 4 readers which each executes Offset Signal Algorithm to get a final consistent output of the snapshot chunk. The Offset Signal Algorithm simply describes as following:
(1) Record current binlog position as LOW offset (2) Read and buffer the snapshot chunk records by executing statement SELECT * FROM MyTable WHERE id &gt; chunk_low AND id &lt;= chunk_high (3) Record current binlog position as HIGH offset (4) Read the binlog records that belong to the snapshot chunk from LOW offset to HIGH offset (5) Upsert the read binlog records into the buffered chunk records, and emit all records in the buffer as final output (all as INSERT records) of the snapshot chunk (6) Continue to read and emit binlog records belong to the chunk after the HIGH offset in single binlog reader. The algorithm is inspired by DBLog Paper, please refer it for more detail.
Note: If the actual values for the primary key are not uniformly distributed across its range, this may lead to unbalanced tasks when incremental snapshot read.
Exactly-Once Processing # The MySQL CDC connector is a Flink Source connector which will read table snapshot chunks first and then continues to read binlog, both snapshot phase and binlog phase, MySQL CDC connector read with exactly-once processing even failures happen.
Startup Reading Position # The config option scan.startup.mode specifies the startup mode for MySQL CDC consumer. The valid enumerations are:
initial (default): Performs an initial snapshot on the monitored database tables upon first startup, and continue to read the latest binlog. earliest-offset: Skip snapshot phase and start reading binlog events from the earliest accessible binlog offset. latest-offset: Never to perform snapshot on the monitored database tables upon first startup, just read from the end of the binlog which means only have the changes since the connector was started. specific-offset: Skip snapshot phase and start reading binlog events from a specific offset. The offset could be specified with binlog filename and position, or a GTID set if GTID is enabled on server. timestamp: Skip snapshot phase and start reading binlog events from a specific timestamp. For example in DataStream API:
MySQLSource.builder() .startupOptions(StartupOptions.earliest()) // Start from earliest offset .startupOptions(StartupOptions.latest()) // Start from latest offset .startupOptions(StartupOptions.specificOffset(&#34;mysql-bin.000003&#34;, 4L) // Start from binlog file and offset .startupOptions(StartupOptions.specificOffset(&#34;24DA167-0C0C-11E8-8442-00059A3C7B00:1-19&#34;)) // Start from GTID set .startupOptions(StartupOptions.timestamp(1667232000000L) // Start from timestamp ... .build() and with SQL:
CREATE TABLE mysql_source (...) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;scan.startup.mode&#39; = &#39;earliest-offset&#39;, -- Start from earliest offset &#39;scan.startup.mode&#39; = &#39;latest-offset&#39;, -- Start from latest offset &#39;scan.startup.mode&#39; = &#39;specific-offset&#39;, -- Start from specific offset &#39;scan.startup.mode&#39; = &#39;timestamp&#39;, -- Start from timestamp &#39;scan.startup.specific-offset.file&#39; = &#39;mysql-bin.000003&#39;, -- Binlog filename under specific offset startup mode &#39;scan.startup.specific-offset.pos&#39; = &#39;4&#39;, -- Binlog position under specific offset mode &#39;scan.startup.specific-offset.gtid-set&#39; = &#39;24DA167-0C0C-11E8-8442-00059A3C7B00:1-19&#39;, -- GTID set under specific offset startup mode &#39;scan.startup.timestamp-millis&#39; = &#39;1667232000000&#39; -- Timestamp under timestamp startup mode ... ) Notes:
MySQL source will print the current binlog position into logs with INFO level on checkpoint, with the prefix &ldquo;Binlog offset on checkpoint {checkpoint-id}&rdquo;. It could be useful if you want to restart the job from a specific checkpointed position. If schema of capturing tables was changed previously, starting with earliest offset, specific offset or timestamp could fail as the Debezium reader keeps the current latest table schema internally and earlier records with unmatched schema cannot be correctly parsed. DataStream Source # import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.cdc.connectors.mysql.source.MySqlSource; public class MySqlSourceExample { public static void main(String[] args) throws Exception { MySqlSource&lt;String&gt; mySqlSource = MySqlSource.&lt;String&gt;builder() .hostname(&#34;yourHostname&#34;) .port(yourPort) .databaseList(&#34;yourDatabaseName&#34;) // set captured database, If you need to synchronize the whole database, Please set tableList to &#34;.*&#34;. .tableList(&#34;yourDatabaseName.yourTableName&#34;) // set captured table .username(&#34;yourUsername&#34;) .password(&#34;yourPassword&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000); env .fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), &#34;MySQL Source&#34;) // set 4 parallel source tasks .setParallelism(4) .print().setParallelism(1); // use parallelism 1 for sink to keep message ordering env.execute(&#34;Print MySQL Snapshot + Binlog&#34;); } } Scan Newly Added Tables # Scan Newly Added Tables feature enables you add new tables to monitor for existing running pipeline, the newly added tables will read theirs snapshot data firstly and then read their changelog automatically.
Imaging this scenario: At the beginning, a Flink job monitor tables [product, user, address], but after some days we would like the job can also monitor tables [order, custom] which contains history data, and we need the job can still reuse existing state of the job, this feature can resolve this case gracefully.
The following operations show how to enable this feature to resolve above scenario. An existing Flink job which uses CDC Source like:
MySqlSource&lt;String&gt; mySqlSource = MySqlSource.&lt;String&gt;builder() .hostname(&#34;yourHostname&#34;) .port(yourPort) .scanNewlyAddedTableEnabled(true) // enable scan the newly added tables feature .databaseList(&#34;db&#34;) // set captured database .tableList(&#34;db.product, db.user, db.address&#34;) // set captured tables [product, user, address] .username(&#34;yourUsername&#34;) .password(&#34;yourPassword&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String .build(); // your business code If we would like to add new tables [order, custom] to an existing Flink job，just need to update the tableList() value of the job to include [order, custom] and restore the job from previous savepoint.
Step 1: Stop the existing Flink job with savepoint.
$ ./bin/flink stop $Existing_Flink_JOB_ID Suspending job &#34;cca7bc1061d61cf15238e92312c2fc20&#34; with a savepoint. Savepoint completed. Path: file:/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab Step 2: Update the table list option for the existing Flink job .
update tableList() value. build the jar of updated job. MySqlSource&lt;String&gt; mySqlSource = MySqlSource.&lt;String&gt;builder() .hostname(&#34;yourHostname&#34;) .port(yourPort) .scanNewlyAddedTableEnabled(true) .databaseList(&#34;db&#34;) .tableList(&#34;db.product, db.user, db.address, db.order, db.custom&#34;) // set captured tables [product, user, address ,order, custom] .username(&#34;yourUsername&#34;) .password(&#34;yourPassword&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String .build(); // your business code Step 3: Restore the updated Flink job from savepoint.
$ ./bin/flink run \\ --detached \\ --from-savepoint /tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab \\ ./FlinkCDCExample.jar Note: Please refer the doc Restore the job from previous savepoint for more details.
Tables Without primary keys # Starting from version 2.4.0, MySQL CDC support tables that do not have a primary key. To use a table without primary keys, you must configure the scan.incremental.snapshot.chunk.key-column option and specify one non-null field.
There are two places that need to be taken care of.
If there is an index in the table, try to use a column which is contained in the index in scan.incremental.snapshot.chunk.key-column. This will increase the speed of select statement. The processing semantics of a MySQL CDC table without primary keys is determined based on the behavior of the column that are specified by the scan.incremental.snapshot.chunk.key-column. If no update operation is performed on the specified column, the exactly-once semantics is ensured. If the update operation is performed on the specified column, only the at-least-once semantics is ensured. However, you can specify primary keys at downstream and perform the idempotence operation to ensure data correctness. About converting binary type data to base64 encoded data # CREATE TABLE products ( db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, table_name STRING METADATA FROM &#39;table_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, binary_data STRING, PRIMARY KEY(order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;username&#39; = &#39;root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;test_db&#39;, &#39;table-name&#39; = &#39;test_tb&#39;, &#39;debezium.binary.handling.mode&#39; = &#39;base64&#39; ); binary_data field in the database is of type VARBINARY(N). In some scenarios, we need to convert binary data to base64 encoded string data. This feature can be enabled by adding the parameter &lsquo;debezium.binary.handling.mode&rsquo;=&lsquo;base64&rsquo;, By adding this parameter, we can map the binary field type to &lsquo;STRING&rsquo; in Flink SQL, thereby obtaining base64 encoded string data.
Data Type Mapping # MySQL type Flink SQL type NOTE TINYINT TINYINT SMALLINT
TINYINT UNSIGNED
TINYINT UNSIGNED ZEROFILL SMALLINT INT
MEDIUMINT
SMALLINT UNSIGNED
SMALLINT UNSIGNED ZEROFILL INT BIGINT
INT UNSIGNED
INT UNSIGNED ZEROFILL
MEDIUMINT UNSIGNED
MEDIUMINT UNSIGNED ZEROFILL BIGINT BIGINT UNSIGNED
BIGINT UNSIGNED ZEROFILL
SERIAL DECIMAL(20, 0) FLOAT
FLOAT UNSIGNED
FLOAT UNSIGNED ZEROFILL FLOAT REAL
REAL UNSIGNED
REAL UNSIGNED ZEROFILL
DOUBLE
DOUBLE UNSIGNED
DOUBLE UNSIGNED ZEROFILL
DOUBLE PRECISION
DOUBLE PRECISION UNSIGNED
DOUBLE PRECISION UNSIGNED ZEROFILL DOUBLE NUMERIC(p, s)
NUMERIC(p, s) UNSIGNED
NUMERIC(p, s) UNSIGNED ZEROFILL
DECIMAL(p, s)
DECIMAL(p, s) UNSIGNED
DECIMAL(p, s) UNSIGNED ZEROFILL
FIXED(p, s)
FIXED(p, s) UNSIGNED
FIXED(p, s) UNSIGNED ZEROFILL
where p <= 38
DECIMAL(p, s) NUMERIC(p, s)
NUMERIC(p, s) UNSIGNED
NUMERIC(p, s) UNSIGNED ZEROFILL
DECIMAL(p, s)
DECIMAL(p, s) UNSIGNED
DECIMAL(p, s) UNSIGNED ZEROFILL
FIXED(p, s)
FIXED(p, s) UNSIGNED
FIXED(p, s) UNSIGNED ZEROFILL
where 38 < p <= 65
STRING The precision for DECIMAL data type is up to 65 in MySQL, but the precision for DECIMAL is limited to 38 in Flink. So if you define a decimal column whose precision is greater than 38, you should map it to STRING to avoid precision loss. BOOLEAN
TINYINT(1)
BIT(1) BOOLEAN DATE DATE TIME [(p)] TIME [(p)] TIMESTAMP [(p)]
DATETIME [(p)] TIMESTAMP [(p)] CHAR(n) CHAR(n) VARCHAR(n) VARCHAR(n) BIT(n) BINARY(⌈(n + 7) / 8⌉) BINARY(n) BINARY(n) VARBINARY(N) VARBINARY(N) TINYTEXT
TEXT
MEDIUMTEXT
LONGTEXT
STRING TINYBLOB
BLOB
MEDIUMBLOB
LONGBLOB
BYTES Currently, for BLOB data type in MySQL, only the blob whose length isn't greater than 2,147,483,647(2 ** 31 - 1) is supported. YEAR INT ENUM STRING JSON STRING The JSON data type will be converted into STRING with JSON format in Flink. SET ARRAY&lt;STRING&gt; As the SET data type in MySQL is a string object that can have zero or more values, it should always be mapped to an array of string GEOMETRY
POINT
LINESTRING
POLYGON
MULTIPOINT
MULTILINESTRING
MULTIPOLYGON
GEOMETRYCOLLECTION
STRING The spatial data types in MySQL will be converted into STRING with a fixed Json format. Please see MySQL Spatial Data Types Mapping section for more detailed information. MySQL Spatial Data Types Mapping # The spatial data types except for GEOMETRYCOLLECTION in MySQL will be converted into Json String with a fixed format like:
{&#34;srid&#34;: 0 , &#34;type&#34;: &#34;xxx&#34;, &#34;coordinates&#34;: [0, 0]} The field srid identifies the SRS in which the geometry is defined, SRID 0 is the default for new geometry values if no SRID is specified. As only MySQL 8+ support to specific SRID when define spatial data type, the field srid will always be 0 in MySQL with a lower version.
The field type identifies the spatial data type, such as POINT/LINESTRING/POLYGON.
The field coordinates represents the coordinates of the spatial data.
For GEOMETRYCOLLECTION, it will be converted into Json String with a fixed format like:
{&#34;srid&#34;: 0 , &#34;type&#34;: &#34;GeometryCollection&#34;, &#34;geometries&#34;: [{&#34;type&#34;:&#34;Point&#34;,&#34;coordinates&#34;:[10,10]}]} The field geometries is an array contains all spatial data.
The example for different spatial data types mapping is as follows:
Spatial data in MySQL Json String converted in Flink POINT(1 1) {"coordinates":[1,1],"type":"Point","srid":0} LINESTRING(3 0, 3 3, 3 5) {"coordinates":[[3,0],[3,3],[3,5]],"type":"LineString","srid":0} POLYGON((1 1, 2 1, 2 2, 1 2, 1 1)) {"coordinates":[[[1,1],[2,1],[2,2],[1,2],[1,1]]],"type":"Polygon","srid":0} MULTIPOINT((1 1),(2 2)) {"coordinates":[[1,1],[2,2]],"type":"MultiPoint","srid":0} MultiLineString((1 1,2 2,3 3),(4 4,5 5)) {"coordinates":[[[1,1],[2,2],[3,3]],[[4,4],[5,5]]],"type":"MultiLineString","srid":0} MULTIPOLYGON(((0 0, 10 0, 10 10, 0 10, 0 0)), ((5 5, 7 5, 7 7, 5 7, 5 5))) {"coordinates":[[[[0,0],[10,0],[10,10],[0,10],[0,0]]],[[[5,5],[7,5],[7,7],[5,7],[5,5]]]],"type":"MultiPolygon","srid":0} GEOMETRYCOLLECTION(POINT(10 10), POINT(30 30), LINESTRING(15 15, 20 20)) {"geometries":[{"type":"Point","coordinates":[10,10]},{"type":"Point","coordinates":[30,30]},{"type":"LineString","coordinates":[[15,15],[20,20]]}],"type":"GeometryCollection","srid":0} Back to top
`}),e.add({id:17,href:"/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/mysql/",title:"MySQL",section:"Pipeline Connectors",content:` MySQL Connector # MySQL connector allows reading snapshot data and incremental data from MySQL database and provides end-to-end full-database data synchronization capabilities. This document describes how to setup the MySQL connector.
Dependencies # Since MySQL Connector&rsquo;s GPLv2 license is incompatible with Flink CDC project, we can&rsquo;t provide MySQL connector in prebuilt connector jar packages. You may need to configure the following dependencies manually, and pass it with --jar argument of Flink CDC CLI when submitting YAML pipeline jobs.
Dependency Item Description mysql:mysql-connector-java:8.0.27 Used for connecting to MySQL database. Example # An example of the pipeline for reading data from MySQL and sink to Doris can be defined as follows:
source: type: mysql name: MySQL Source hostname: 127.0.0.1 port: 3306 username: admin password: pass tables: adb.\\.*, bdb.user_table_[0-9]+, [app|web].order_\\.* server-id: 5401-5404 sink: type: doris name: Doris Sink fenodes: 127.0.0.1:8030 username: root password: pass pipeline: name: MySQL to Doris Pipeline parallelism: 4 Connector Options # Option Required Default Type Description hostname required (none) String IP address or hostname of the MySQL database server. port optional 3306 Integer Integer port number of the MySQL database server. username required (none) String Name of the MySQL database to use when connecting to the MySQL database server. password required (none) String Password to use when connecting to the MySQL database server. tables required (none) String Table name of the MySQL database to monitor. The table-name also supports regular expressions to monitor multiple tables that satisfy the regular expressions. It is important to note that the dot (.) is treated as a delimiter for database and table names. If there is a need to use a dot (.) in a regular expression to match any character, it is necessary to escape the dot with a backslash.
eg. db0.\\.*, db1.user_table_[0-9]+, db[1-2].[app|web]order_\\.* tables.exclude optional (none) String Table name of the MySQL database to exclude, parameter will have an exclusion effect after the tables parameter. The table-name also supports regular expressions to exclude multiple tables that satisfy the regular expressions. The usage is the same as the tables parameter schema-change.enabled optional true Boolean Whether to send schema change events, so that downstream sinks can respond to schema changes and achieve table structure synchronization. server-id optional (none) String A numeric ID or a numeric ID range of this database client, The numeric ID syntax is like '5400', the numeric ID range syntax is like '5400-5408', The numeric ID range syntax is recommended when 'scan.incremental.snapshot.enabled' enabled. Every ID must be unique across all currently-running database processes in the MySQL cluster. This connector joins the MySQL cluster as another server (with this unique ID) so it can read the binlog. By default, a random number is generated between 5400 and 6400, though we recommend setting an explicit value. scan.incremental.snapshot.chunk.size optional 8096 Integer The chunk size (number of rows) of table snapshot, captured tables are split into multiple chunks when read the snapshot of table. scan.snapshot.fetch.size optional 1024 Integer The maximum fetch size for per poll when read table snapshot. scan.startup.mode optional initial String Optional startup mode for MySQL CDC consumer, valid enumerations are "initial", "earliest-offset", "latest-offset", "specific-offset" and "timestamp". Please see Startup Reading Position section for more detailed information. scan.startup.specific-offset.file optional (none) String Optional binlog file name used in case of "specific-offset" startup mode scan.startup.specific-offset.pos optional (none) Long Optional binlog file position used in case of "specific-offset" startup mode scan.startup.specific-offset.gtid-set optional (none) String Optional GTID set used in case of "specific-offset" startup mode scan.startup.specific-offset.skip-events optional (none) Long Optional number of events to skip after the specific starting offset scan.startup.specific-offset.skip-rows optional (none) Long Optional number of rows to skip after the specific starting offset connect.timeout optional 30s Duration The maximum time that the connector should wait after trying to connect to the MySQL database server before timing out. connect.max-retries optional 3 Integer The max retry times that the connector should retry to build MySQL database server connection. connection.pool.size optional 20 Integer The connection pool size. jdbc.properties.* optional 20 String Option to pass custom JDBC URL properties. User can pass custom properties like 'jdbc.properties.useSSL' = 'false'. heartbeat.interval optional 30s Duration The interval of sending heartbeat event for tracing the latest available binlog offsets. debezium.* optional (none) String Pass-through Debezium's properties to Debezium Embedded Engine which is used to capture data changes from MySQL server. For example: 'debezium.snapshot.mode' = 'never'. See more about the Debezium's MySQL Connector properties scan.incremental.close-idle-reader.enabled optional false Boolean Whether to close idle readers at the end of the snapshot phase. The flink version is required to be greater than or equal to 1.14 when 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' is set to true.
If the flink version is greater than or equal to 1.15, the default value of 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' has been changed to true, so it does not need to be explicitly configured 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' = 'true' Startup Reading Position # The config option scan.startup.mode specifies the startup mode for MySQL CDC consumer. The valid enumerations are:
initial (default): Performs an initial snapshot on the monitored database tables upon first startup, and continue to read the latest binlog. earliest-offset: Skip snapshot phase and start reading binlog events from the earliest accessible binlog offset. latest-offset: Never to perform snapshot on the monitored database tables upon first startup, just read from the end of the binlog which means only have the changes since the connector was started. specific-offset: Skip snapshot phase and start reading binlog events from a specific offset. The offset could be specified with binlog filename and position, or a GTID set if GTID is enabled on server. timestamp: Skip snapshot phase and start reading binlog events from a specific timestamp. Data Type Mapping # MySQL type Flink CDC type Note TINYINT(n) TINYINT SMALLINT
TINYINT UNSIGNED
TINYINT UNSIGNED ZEROFILL SMALLINT INT
YEAR
MEDIUMINT
MEDIUMINT UNSIGNED
MEDIUMINT UNSIGNED ZEROFILL
SMALLINT UNSIGNED
SMALLINT UNSIGNED ZEROFILL INT BIGINT
INT UNSIGNED
INT UNSIGNED ZEROFILL BIGINT BIGINT UNSIGNED
BIGINT UNSIGNED ZEROFILL
SERIAL DECIMAL(20, 0) FLOAT
FLOAT UNSIGNED
FLOAT UNSIGNED ZEROFILL FLOAT REAL
REAL UNSIGNED
REAL UNSIGNED ZEROFILL
DOUBLE
DOUBLE UNSIGNED
DOUBLE UNSIGNED ZEROFILL
DOUBLE PRECISION
DOUBLE PRECISION UNSIGNED
DOUBLE PRECISION UNSIGNED ZEROFILL DOUBLE NUMERIC(p, s)
NUMERIC(p, s) UNSIGNED
NUMERIC(p, s) UNSIGNED ZEROFILL
DECIMAL(p, s)
DECIMAL(p, s) UNSIGNED
DECIMAL(p, s) UNSIGNED ZEROFILL
FIXED(p, s)
FIXED(p, s) UNSIGNED
FIXED(p, s) UNSIGNED ZEROFILL
where p <= 38
DECIMAL(p, s) NUMERIC(p, s)
NUMERIC(p, s) UNSIGNED
NUMERIC(p, s) UNSIGNED ZEROFILL
DECIMAL(p, s)
DECIMAL(p, s) UNSIGNED
DECIMAL(p, s) UNSIGNED ZEROFILL
FIXED(p, s)
FIXED(p, s) UNSIGNED
FIXED(p, s) UNSIGNED ZEROFILL
where 38 < p <= 65
STRING The precision for DECIMAL data type is up to 65 in MySQL, but the precision for DECIMAL is limited to 38 in Flink. So if you define a decimal column whose precision is greater than 38, you should map it to STRING to avoid precision loss. BOOLEAN
TINYINT(1)
BIT(1) BOOLEAN DATE DATE TIME [(p)] TIME [(p)] TIMESTAMP [(p)] TIMESTAMP_LTZ [(p)] DATETIME [(p)] TIMESTAMP [(p)] CHAR(n) CHAR(n) VARCHAR(n) VARCHAR(n) BIT(n) BINARY(⌈(n + 7) / 8⌉) BINARY(n) BINARY(n) VARBINARY(N) VARBINARY(N) TINYTEXT
TEXT
MEDIUMTEXT
LONGTEXT
STRING TINYBLOB
BLOB
MEDIUMBLOB
LONGBLOB
BYTES Currently, for BLOB data type in MySQL, only the blob whose length isn't greater than 2,147,483,647(2 ** 31 - 1) is supported. ENUM STRING JSON STRING The JSON data type will be converted into STRING with JSON format in Flink. SET - Not supported yet. GEOMETRY
POINT
LINESTRING
POLYGON
MULTIPOINT
MULTILINESTRING
MULTIPOLYGON
GEOMETRYCOLLECTION
STRING The spatial data types in MySQL will be converted into STRING with a fixed Json format. Please see MySQL Spatial Data Types Mapping section for more detailed information. MySQL Spatial Data Types Mapping # The spatial data types except for GEOMETRYCOLLECTION in MySQL will be converted into Json String with a fixed format like:
{&#34;srid&#34;: 0 , &#34;type&#34;: &#34;xxx&#34;, &#34;coordinates&#34;: [0, 0]} The field srid identifies the SRS in which the geometry is defined, SRID 0 is the default for new geometry values if no SRID is specified. As only MySQL 8+ support to specific SRID when define spatial data type, the field srid will always be 0 in MySQL with a lower version.
The field type identifies the spatial data type, such as POINT/LINESTRING/POLYGON.
The field coordinates represents the coordinates of the spatial data.
For GEOMETRYCOLLECTION, it will be converted into Json String with a fixed format like:
{&#34;srid&#34;: 0 , &#34;type&#34;: &#34;GeometryCollection&#34;, &#34;geometries&#34;: [{&#34;type&#34;:&#34;Point&#34;,&#34;coordinates&#34;:[10,10]}]} The field geometries is an array contains all spatial data.
The example for different spatial data types mapping is as follows:
Spatial data in MySQL Json String converted in Flink POINT(1 1) {"coordinates":[1,1],"type":"Point","srid":0} LINESTRING(3 0, 3 3, 3 5) {"coordinates":[[3,0],[3,3],[3,5]],"type":"LineString","srid":0} POLYGON((1 1, 2 1, 2 2, 1 2, 1 1)) {"coordinates":[[[1,1],[2,1],[2,2],[1,2],[1,1]]],"type":"Polygon","srid":0} MULTIPOINT((1 1),(2 2)) {"coordinates":[[1,1],[2,2]],"type":"MultiPoint","srid":0} MultiLineString((1 1,2 2,3 3),(4 4,5 5)) {"coordinates":[[[1,1],[2,2],[3,3]],[[4,4],[5,5]]],"type":"MultiLineString","srid":0} MULTIPOLYGON(((0 0, 10 0, 10 10, 0 10, 0 0)), ((5 5, 7 5, 7 7, 5 7, 5 5))) {"coordinates":[[[[0,0],[10,0],[10,10],[0,10],[0,0]]],[[[5,5],[7,5],[7,7],[5,7],[5,5]]]],"type":"MultiPolygon","srid":0} GEOMETRYCOLLECTION(POINT(10 10), POINT(30 30), LINESTRING(15 15, 20 20)) {"geometries":[{"type":"Point","coordinates":[10,10]},{"type":"Point","coordinates":[30,30]},{"type":"LineString","coordinates":[[15,15],[20,20]]}],"type":"GeometryCollection","srid":0} Back to top
`}),e.add({id:18,href:"/flink/flink-cdc-docs-master/docs/get-started/quickstart/mysql-to-starrocks/",title:"MySQL to StarRocks",section:"Quickstart",content:" Streaming ELT from MySQL to StarRocks # This tutorial is to show how to quickly build a Streaming ELT job from MySQL to StarRocks using Flink CDC, including the feature of sync all table of one database, schema change evolution and sync sharding tables into one table.\nAll exercises in this tutorial are performed in the Flink CDC CLI, and the entire process uses standard SQL syntax, without a single line of Java/Scala code or IDE installation.\nPreparation # Prepare a Linux or MacOS computer with Docker installed.\nPrepare Flink Standalone cluster # Download Flink 1.18.0 ，unzip and get flink-1.18.0 directory.\nUse the following command to navigate to the Flink directory and set FLINK_HOME to the directory where flink-1.18.0 is located.\ncd flink-1.18.0 Enable checkpointing by appending the following parameters to the conf/flink-conf.yaml configuration file to perform a checkpoint every 3 seconds.\nexecution.checkpointing.interval: 3000 Start the Flink cluster using the following command.\n./bin/start-cluster.sh If successfully started, you can access the Flink Web UI at http://localhost:8081/, as shown below.\nExecuting start-cluster.sh multiple times can start multiple TaskManagers.\nPrepare docker compose # The following tutorial will prepare the required components using docker-compose. Create a docker-compose.yml file using the content provided below:\nversion: &#39;2.1&#39; services: StarRocks: image: starrocks/allin1-ubuntu:3.2.6 ports: - &#34;8080:8080&#34; - &#34;9030:9030&#34; MySQL: image: debezium/example-mysql:1.1 ports: - &#34;3306:3306&#34; environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_USER=mysqluser - MYSQL_PASSWORD=mysqlpw The Docker Compose should include the following services (containers):\nMySQL: include a database named app_db StarRocks: to store tables from MySQL To start all containers, run the following command in the directory that contains the docker-compose.yml file.\ndocker-compose up -d This command automatically starts all the containers defined in the Docker Compose configuration in a detached mode. Run docker ps to check whether these containers are running properly. You can also visit http://localhost:8030/ to check whether StarRocks is running.\nPrepare records for MySQL # Enter MySQL container\ndocker-compose exec MySQL mysql -uroot -p123456 create app_db database and orders,products,shipments tables, then insert records\n-- create database CREATE DATABASE app_db; USE app_db; -- create orders table CREATE TABLE `orders` ( `id` INT NOT NULL, `price` DECIMAL(10,2) NOT NULL, PRIMARY KEY (`id`) ); -- insert records INSERT INTO `orders` (`id`, `price`) VALUES (1, 4.00); INSERT INTO `orders` (`id`, `price`) VALUES (2, 100.00); -- create shipments table CREATE TABLE `shipments` ( `id` INT NOT NULL, `city` VARCHAR(255) NOT NULL, PRIMARY KEY (`id`) ); -- insert records INSERT INTO `shipments` (`id`, `city`) VALUES (1, &#39;beijing&#39;); INSERT INTO `shipments` (`id`, `city`) VALUES (2, &#39;xian&#39;); -- create products table CREATE TABLE `products` ( `id` INT NOT NULL, `product` VARCHAR(255) NOT NULL, PRIMARY KEY (`id`) ); -- insert records INSERT INTO `products` (`id`, `product`) VALUES (1, &#39;Beer&#39;); INSERT INTO `products` (`id`, `product`) VALUES (2, &#39;Cap&#39;); INSERT INTO `products` (`id`, `product`) VALUES (3, &#39;Peanut&#39;); Submit job using FlinkCDC cli # Download the binary compressed packages listed below and extract them to the directory flink cdc-3.0.0 '： flink-cdc-3.0.0-bin.tar.gz flink-cdc-3.0.0 directory will contain four directory bin,lib,log,conf.\nDownload the connector package listed below and move it to the lib directory\nDownload links are available only for stable releases, SNAPSHOT dependencies need to be built based on master or release branches by yourself.\nMySQL pipeline connector 3.0.0 StarRocks pipeline connector 3.0.0 MySQL Connector Java Write task configuration yaml file. Here is an example file for synchronizing the entire database mysql-to-starrocks.yaml：\n################################################################################ # Description: Sync MySQL all tables to StarRocks ################################################################################ source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: starrocks name: StarRocks Sink jdbc-url: jdbc:mysql://127.0.0.1:9030 load-url: 127.0.0.1:8080 username: root password: &#34;&#34; table.create.properties.replication_num: 1 pipeline: name: Sync MySQL Database to StarRocks parallelism: 2 Notice that:\ntables: app_db.\\.* in source synchronize all tables in app_db through Regular Matching. table.create.properties.replication_num in sink is because there is only one StarRocks BE node in the Docker image. Finally, submit job to Flink Standalone cluster using Cli.\nbash bin/flink-cdc.sh mysql-to-starrocks.yaml --jar lib/mysql-connector-java-8.0.27.jar After successful submission, the return information is as follows：\nPipeline has been submitted to cluster. Job ID: 02a31c92f0e7bc9a1f4c0051980088a0 Job Description: Sync MySQL Database to StarRocks We can find a job named Sync MySQL Database to StarRocks is running through Flink Web UI.\nConnect to jdbc through database connection tools such as Dbeaver using mysql://127.0.0.1:9030. You can view the data written to three tables in StarRocks.\nSynchronize Schema and Data changes # Enter MySQL container\ndocker-compose exec mysql mysql -uroot -p123456 Then, modify schema and record in MySQL, and the tables of StarRocks will change the same in real time：\ninsert one record in orders from MySQL:\nINSERT INTO app_db.orders (id, price) VALUES (3, 100.00); add one column in orders from MySQL:\nALTER TABLE app_db.orders ADD amount varchar(100) NULL; update one record in orders from MySQL:\nUPDATE app_db.orders SET price=100.00, amount=100.00 WHERE id=1; delete one record in orders from MySQL:\nDELETE FROM app_db.orders WHERE id=2; Refresh the Dbeaver every time you execute a step, and you can see that the orders table displayed in StarRocks will be updated in real-time, like the following：\nSimilarly, by modifying the shipments and products tables, you can also see the results of synchronized changes in real-time in StarRocks.\nRoute the changes # Flink CDC provides the configuration to route the table structure/data of the source table to other table names.\nWith this ability, we can achieve functions such as table name, database name replacement, and whole database synchronization. Here is an example file for using route feature:\n################################################################################ # Description: Sync MySQL all tables to StarRocks ################################################################################ source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: starrocks jdbc-url: jdbc:mysql://127.0.0.1:9030 load-url: 127.0.0.1:8030 username: root password: &#34;&#34; table.create.properties.replication_num: 1 route: - source-table: app_db.orders sink-table: ods_db.ods_orders - source-table: app_db.shipments sink-table: ods_db.ods_shipments - source-table: app_db.products sink-table: ods_db.ods_products pipeline: name: Sync MySQL Database to StarRocks parallelism: 2 Using the upper route configuration, we can synchronize the table schema and data of app_db.orders to ods_db.ods_orders, thus achieving the function of database migration.\nSpecifically, source-table support regular expression matching with multiple tables to synchronize sharding databases and tables. like the following：\nroute: - source-table: app_db.order\\.* sink-table: ods_db.ods_orders In this way, we can synchronize sharding tables like app_db.order01、app_db.order02、app_db.order03 into one ods_db.ods_orders tables. Warning that there is currently no support for scenarios where the same primary key data exists in multiple tables, which will be supported in future versions.\nClean up # After finishing the tutorial, run the following command to stop all containers in the directory of docker-compose.yml:\ndocker-compose down Run the following command to stop the Flink cluster in the directory of Flink flink-1.18.0:\n./bin/stop-cluster.sh Back to top\n"}),e.add({id:19,href:"/flink/flink-cdc-docs-master/docs/get-started/quickstart/",title:"Quickstart",section:"Get Started",content:" "}),e.add({id:20,href:"/flink/flink-cdc-docs-master/docs/deployment/yarn/",title:"YARN",section:"Deployment",content:` Introduction # Apache Hadoop YARN is a resource provider popular with many data processing frameworks. Flink services are submitted to YARN&rsquo;s ResourceManager, which spawns containers on machines managed by YARN NodeManagers. Flink deploys its JobManager and TaskManager instances into such containers.
Flink can dynamically allocate and de-allocate TaskManager resources depending on the number of processing slots required by the job(s) running on the JobManager.
Preparation # This Getting Started section assumes a functional YARN environment, starting from version 2.10.2. YARN environments are provided most conveniently through services such as Amazon EMR, Google Cloud DataProc or products like Cloudera. Manually setting up a YARN environment locally or on a cluster is not recommended for following through this Getting Started tutorial.
Make sure your YARN cluster is ready for accepting Flink applications by running yarn top. It should show no error messages. Download a recent Flink distribution from the download page and unpack it. Important Make sure that the HADOOP_CLASSPATH environment variable is set up (it can be checked by running echo $HADOOP_CLASSPATH). If not, set it up using export HADOOP_CLASSPATH=\`hadoop classpath\` Session Mode # Flink runs on all UNIX-like environments, i.e. Linux, Mac OS X, and Cygwin (for Windows).
You can refer overview to check supported versions and download the binary release of Flink, then extract the archive:
tar -xzf flink-*.tgz You should set FLINK_HOME environment variables like:
export FLINK_HOME=/path/flink-* Starting a Flink Session on YARN # Once you&rsquo;ve made sure that the HADOOP_CLASSPATH environment variable is set, you can launch a Flink on YARN session:
# we assume to be in the root directory of # the unzipped Flink distribution # export HADOOP_CLASSPATH export HADOOP_CLASSPATH=\`hadoop classpath\` # Start YARN session ./bin/yarn-session.sh --detached # Stop YARN session (replace the application id based # on the output of the yarn-session.sh command) echo &#34;stop&#34; | ./bin/yarn-session.sh -id application_XXXXX_XXX After starting YARN session, you can now access the Flink Web UI through the URL printed in the last lines of the command output, or through the YARN ResourceManager web UI.
Then, you need to add some configs to your flink-conf.yaml:
rest.bind-port: {{REST_PORT}} rest.address: {{NODE_IP}} execution.target: yarn-session yarn.application.id: {{YARN_APPLICATION_ID}} {{REST_PORT}} and {{NODE_IP}} should be replaced by the actual values of your JobManager Web Interface, and {{YARN_APPLICATION_ID}} should be replaced by the actual YARN application ID of Flink.
Set up Flink CDC # Download the tar file of Flink CDC from release page, then extract the archive:
tar -xzf flink-cdc-*.tar.gz Extracted flink-cdc contains four directories: bin,lib,log and conf.
Download the connector jars from release page, and move it to the lib directory. Download links are available only for stable releases, SNAPSHOT dependencies need to be built based on specific branch by yourself.
Submit a Flink CDC Job # Here is an example file for synchronizing the entire database mysql-to-doris.yaml:
################################################################################ # Description: Sync MySQL all tables to Doris ################################################################################ source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: doris fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; pipeline: name: Sync MySQL Database to Doris parallelism: 2 You need to modify the configuration file according to your needs. Finally, submit job to Flink Standalone cluster using Cli.
cd /path/flink-cdc-* ./bin/flink-cdc.sh mysql-to-doris.yaml After successful submission, the return information is as follows:
Pipeline has been submitted to cluster. Job ID: ae30f4580f1918bebf16752d4963dc54 Job Description: Sync MySQL Database to Doris You can find a job named Sync MySQL Database to Doris running through Flink Web UI.
Please note that submitting to application mode cluster and per-job mode cluster are not supported for now.
`}),e.add({id:21,href:"/flink/flink-cdc-docs-master/docs/connectors/",title:"Connectors",section:"Docs",content:" "}),e.add({id:22,href:"/flink/flink-cdc-docs-master/docs/core-concept/data-sink/",title:"Data Sink",section:"Core Concept",content:` Definition # Data Sink is used to apply schema changes and write change data to external systems. A Data Sink can write to multiple tables simultaneously.
Parameters # To describe a data sink, the follows are required:
parameter meaning optional/required type The type of the sink, such as doris or starrocks. required name The name of the sink, which is user-defined (a default value provided). optional configurations of Data Sink Configurations to build the Data Sink e.g. connection configurations and sink table properties. optional Example # We could use this yaml file to define a doris sink:
sink: type: doris name: doris-sink # Optional parameter for description purpose fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; table.create.properties.replication_num: 1 # Optional parameter for advanced functionalities `}),e.add({id:23,href:"/flink/flink-cdc-docs-master/docs/deployment/kubernetes/",title:"Kubernetes",section:"Deployment",content:` Introduction # Kubernetes is a popular container-orchestration system for automating computer application deployment, scaling, and management. Flink&rsquo;s native Kubernetes integration allows you to directly deploy Flink on a running Kubernetes cluster. Moreover, Flink is able to dynamically allocate and de-allocate TaskManagers depending on the required resources because it can directly talk to Kubernetes.
Apache Flink also provides a Kubernetes operator for managing Flink clusters on Kubernetes. It supports both standalone and native deployment mode and greatly simplifies deployment, configuration and the life cycle management of Flink resources on Kubernetes.
For more information, please refer to the Flink Kubernetes Operator documentation.
Preparation # The doc assumes a running Kubernetes cluster fulfilling the following requirements:
Kubernetes &gt;= 1.9. KubeConfig, which has access to list, create, delete pods and services, configurable via ~/.kube/config. You can verify permissions by running kubectl auth can-i &lt;list|create|edit|delete&gt; pods. Enabled Kubernetes DNS. default service account with RBAC permissions to create, delete pods. If you have problems setting up a Kubernetes cluster, please take a look at how to setup a Kubernetes cluster.
Session Mode # Flink runs on all UNIX-like environments, i.e. Linux, Mac OS X, and Cygwin (for Windows).
You can refer overview to check supported versions and download the binary release of Flink, then extract the archive:
tar -xzf flink-*.tgz You should set FLINK_HOME environment variables like:
export FLINK_HOME=/path/flink-* Start a session cluster # To start a session cluster on k8s, run the bash script that comes with Flink:
cd /path/flink-* ./bin/kubernetes-session.sh -Dkubernetes.cluster-id=my-first-flink-cluster After successful startup, the return information is as follows：
org.apache.flink.kubernetes.utils.KubernetesUtils [] - Kubernetes deployment requires a fixed port. Configuration blob.server.port will be set to 6124 org.apache.flink.kubernetes.utils.KubernetesUtils [] - Kubernetes deployment requires a fixed port. Configuration taskmanager.rpc.port will be set to 6122 org.apache.flink.kubernetes.KubernetesClusterDescriptor [] - Please note that Flink client operations(e.g. cancel, list, stop, savepoint, etc.) won&#39;t work from outside the Kubernetes cluster since &#39;kubernetes.rest-service.exposed.type&#39; has been set to ClusterIP. org.apache.flink.kubernetes.KubernetesClusterDescriptor [] - Create flink session cluster my-first-flink-cluster successfully, JobManager Web Interface: http://my-first-flink-cluster-rest.default:8081 please refer to Flink documentation to expose Flink’s Web UI and REST endpoint.
You should ensure that REST endpoint can be accessed by the node of your submission. Then, you need to add these two config to your flink-conf.yaml:
rest.bind-port: {{REST_PORT}} rest.address: {{NODE_IP}} {{REST_PORT}} and {{NODE_IP}} should be replaced by the actual values of your JobManager Web Interface.
Set up Flink CDC # Download the tar file of Flink CDC from release page, then extract the archive:
tar -xzf flink-cdc-*.tar.gz Extracted flink-cdc contains four directories: bin,lib,log and conf.
Download the connector jars from release page, and move it to the lib directory. Download links are available only for stable releases, SNAPSHOT dependencies need to be built based on specific branch by yourself.
Submit a Flink CDC Job # Here is an example file for synchronizing the entire database mysql-to-doris.yaml：
################################################################################ # Description: Sync MySQL all tables to Doris ################################################################################ source: type: mysql hostname: localhost port: 3306 username: root password: 123456 tables: app_db.\\.* server-id: 5400-5404 server-time-zone: UTC sink: type: doris fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; pipeline: name: Sync MySQL Database to Doris parallelism: 2 You need to modify the configuration file according to your needs, refer to connectors more information.
MySQL pipeline connector Apache Doris pipeline connector Finally, submit job to Flink Standalone cluster using Cli.
cd /path/flink-cdc-* ./bin/flink-cdc.sh mysql-to-doris.yaml After successful submission, the return information is as follows：
Pipeline has been submitted to cluster. Job ID: ae30f4580f1918bebf16752d4963dc54 Job Description: Sync MySQL Database to Doris Then you can find a job named Sync MySQL Database to Doris running through Flink Web UI.
Please note that submitting with native application mode and Flink Kubernetes operator are not supported for now. `}),e.add({id:24,href:"/flink/flink-cdc-docs-master/docs/developer-guide/licenses/",title:"Licenses",section:"Developer Guide",content:` Licenses # Flink CDC is licensed under Apache License 2.0.
If you have any question regarding licenses, just contact us.
Apache Software Foundation # You could know more about ASF as follows.
Apache Software Foundation License Events Security SponSprShip Thanks Privacy `}),e.add({id:25,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/oceanbase-tutorial/",title:"OceanBase Tutorial",section:"Tutorials",content:` Demo: OceanBase CDC to ElasticSearch # Video tutorial # YouTube Bilibili Preparation # Configure and start the components # Create docker-compose.yml.
Note: host network mode is required in this demo, so it can only work on Linux, see network-tutorial-host.
version: &#39;2.1&#39; services: observer: image: oceanbase/oceanbase-ce:4.2.0.0 container_name: observer environment: - &#39;MODE=slim&#39; - &#39;OB_ROOT_PASSWORD=pswd&#39; network_mode: &#34;host&#34; oblogproxy: image: whhe/oblogproxy:1.1.3_4x container_name: oblogproxy environment: - &#39;OB_SYS_USERNAME=root&#39; - &#39;OB_SYS_PASSWORD=pswd&#39; network_mode: &#34;host&#34; elasticsearch: image: &#39;elastic/elasticsearch:7.6.0&#39; container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - ES_JAVA_OPTS=-Xms512m -Xmx512m - discovery.type=single-node ports: - &#39;9200:9200&#39; - &#39;9300:9300&#39; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: &#39;elastic/kibana:7.6.0&#39; container_name: kibana ports: - &#39;5601:5601&#39; volumes: - &#39;/var/run/docker.sock:/var/run/docker.sock&#39; Execute the following command in the directory where docker-compose.yml is located.
docker-compose up -d Set password # From OceanBase 4.0.0.0 CE, we can only fetch the commit log of non-sys tenant.
Here we use the &rsquo;test&rsquo; tenant for example.
Login with &lsquo;root&rsquo; user of &rsquo;test&rsquo; tenant:
docker-compose exec observer obclient -h127.0.0.1 -P2881 -uroot@test Set a password:
ALTER USER root IDENTIFIED BY &#39;test&#39;; Create data for reading snapshot # Login &lsquo;root&rsquo; user of &rsquo;test&rsquo; tenant.
docker-compose exec observer obclient -h127.0.0.1 -P2881 -uroot@test -ptest Insert data:
CREATE DATABASE ob; USE ob; CREATE TABLE products ( id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(512) ); ALTER TABLE products AUTO_INCREMENT = 101; INSERT INTO products VALUES (default,&#34;scooter&#34;,&#34;Small 2-wheel scooter&#34;), (default,&#34;car battery&#34;,&#34;12V car battery&#34;), (default,&#34;12-pack drill bits&#34;,&#34;12-pack of drill bits with sizes ranging from #40 to #3&#34;), (default,&#34;hammer&#34;,&#34;12oz carpenter&#39;s hammer&#34;), (default,&#34;hammer&#34;,&#34;14oz carpenter&#39;s hammer&#34;), (default,&#34;hammer&#34;,&#34;16oz carpenter&#39;s hammer&#34;), (default,&#34;rocks&#34;,&#34;box of assorted rocks&#34;), (default,&#34;jacket&#34;,&#34;water resistent black wind breaker&#34;), (default,&#34;spare tire&#34;,&#34;24 inch spare tire&#34;); CREATE TABLE orders ( order_id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, order_date DATETIME NOT NULL, customer_name VARCHAR(255) NOT NULL, price DECIMAL(10, 5) NOT NULL, product_id INTEGER NOT NULL, order_status BOOLEAN NOT NULL -- Whether order has been placed ) AUTO_INCREMENT = 10001; INSERT INTO orders VALUES (default, &#39;2020-07-30 10:08:22&#39;, &#39;Jark&#39;, 50.50, 102, false), (default, &#39;2020-07-30 10:11:09&#39;, &#39;Sally&#39;, 15.00, 105, false), (default, &#39;2020-07-30 12:00:30&#39;, &#39;Edward&#39;, 25.25, 106, false); Download the libraries required # Download links are only available for stable releases.
flink-sql-connector-elasticsearch7-3.0.1-1.17.jar flink-sql-connector-oceanbase-cdc-3.0-SNAPSHOT.jar Use Flink DDL to create dynamic table in Flink SQL CLI # -- checkpoint every 3000 milliseconds Flink SQL&gt; SET execution.checkpointing.interval = 3s; -- set local time zone as Asia/Shanghai Flink SQL&gt; SET table.local-time-zone = Asia/Shanghai; -- create orders table Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;oceanbase-cdc&#39;, &#39;scan.startup.mode&#39; = &#39;initial&#39;, &#39;username&#39; = &#39;root@test&#39;, &#39;password&#39; = &#39;test&#39;, &#39;tenant-name&#39; = &#39;test&#39;, &#39;database-name&#39; = &#39;^ob$&#39;, &#39;table-name&#39; = &#39;^orders$&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;2881&#39;, &#39;rootserver-list&#39; = &#39;127.0.0.1:2882:2881&#39;, &#39;logproxy.host&#39; = &#39;localhost&#39;, &#39;logproxy.port&#39; = &#39;2983&#39;, &#39;working-mode&#39; = &#39;memory&#39; ); -- create products table Flink SQL&gt; CREATE TABLE products ( id INT, name STRING, description STRING, PRIMARY KEY (id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;oceanbase-cdc&#39;, &#39;scan.startup.mode&#39; = &#39;initial&#39;, &#39;username&#39; = &#39;root@test&#39;, &#39;password&#39; = &#39;test&#39;, &#39;tenant-name&#39; = &#39;test&#39;, &#39;database-name&#39; = &#39;^ob$&#39;, &#39;table-name&#39; = &#39;^products$&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;2881&#39;, &#39;rootserver-list&#39; = &#39;127.0.0.1:2882:2881&#39;, &#39;logproxy.host&#39; = &#39;localhost&#39;, &#39;logproxy.port&#39; = &#39;2983&#39;, &#39;working-mode&#39; = &#39;memory&#39; ); -- create flat table enriched_orders Flink SQL&gt; CREATE TABLE enriched_orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, product_name STRING, product_description STRING, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;elasticsearch-7&#39;, &#39;hosts&#39; = &#39;http://localhost:9200&#39;, &#39;index&#39; = &#39;enriched_orders&#39;); -- Start the reading and writing job Flink SQL&gt; INSERT INTO enriched_orders SELECT o.order_id, o.order_date, o.customer_name, o.price, o.product_id, o.order_status, p.name, p.description FROM orders AS o LEFT JOIN products AS p ON o.product_id = p.id; Check data on Kibana # Open http://localhost:5601/app/kibana#/management/kibana/index_pattern and create index pattern enriched_orders, then go to http://localhost:5601/app/kibana#/discover, and you will see the data of enriched_orders.
Check data changes # Execute the following sql in OceanBase under ob database, you will find records in Kibana be updated after each step in real time.
INSERT INTO orders VALUES (default, &#39;2020-07-30 15:22:00&#39;, &#39;Jark&#39;, 29.71, 104, false); UPDATE orders SET order_status = true WHERE order_id = 10004; DELETE FROM orders WHERE order_id = 10004; Clean up # Execute the following command to stop all containers in the directory where docker-compose.yml is located.
docker-compose down Stop the flink cluster by following command.
./bin/stop-cluster.sh Back to top
`}),e.add({id:26,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/oracle-cdc/",title:"Oracle",section:"Flink Sources",content:` Oracle CDC Connector # The Oracle CDC connector allows for reading snapshot data and incremental data from Oracle database. This document describes how to setup the Oracle CDC connector to run SQL queries against Oracle databases.
Dependencies # In order to setup the Oracle CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency # &ltdependency&gt &ltgroupId&gtorg.apache.flink&lt/groupId&gt &ltartifactId&gtflink-connector-oracle-cdc&lt/artifactId&gt &ltversion&gt3.2-SNAPSHOT&lt/version&gt &lt/dependency&gt Copied to clipboard! SQL Client JAR # Download link is available only for stable releases.
Download flink-sql-connector-oracle-cdc-3.0.1.jar and put it under &lt;FLINK_HOME&gt;/lib/.
Note: Refer to flink-sql-connector-oracle-cdc, more released versions will be available in the Maven central warehouse.
Since Oracle Connector&rsquo;s FUTC license is incompatible with Flink CDC project, we can&rsquo;t provide Oracle connector in prebuilt connector jar packages. You may need to configure the following dependencies manually.
Dependency Item Description com.oracle.ojdbc:ojdbc8:19.3.0.0 Used for connecting to Oracle database. com.oracle.database.xml:xdb:19.3.0.0 Used for storaging XML files. Setup Oracle # You have to enable log archiving for Oracle database and define an Oracle user with appropriate permissions on all databases that the Debezium Oracle connector monitors.
For Non-CDB database # Enable log archiving
(1.1). Connect to the database as DBA
ORACLE_SID=SID export ORACLE_SID sqlplus /nolog CONNECT sys/password AS SYSDBA (1.2). Enable log archiving
alter system set db_recovery_file_dest_size = 10G; alter system set db_recovery_file_dest = &#39;/opt/oracle/oradata/recovery_area&#39; scope=spfile; shutdown immediate; startup mount; alter database archivelog; alter database open; Note:
Enable log archiving requires database restart, pay attention when try to do it The archived logs will occupy a large amount of disk space, so consider clean the expired logs the periodically (1.3). Check whether log archiving is enabled
-- Should now &#34;Database log mode: Archive Mode&#34; archive log list; Note:
Supplemental logging must be enabled for captured tables or the database in order for data changes to capture the before state of changed database rows. The following illustrates how to configure this on the table/database level.
-- Enable supplemental logging for a specific table: ALTER TABLE inventory.customers ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS; -- Enable supplemental logging for database ALTER DATABASE ADD SUPPLEMENTAL LOG DATA; Create an Oracle user with permissions
(2.1). Create Tablespace
sqlplus sys/password@host:port/SID AS SYSDBA; CREATE TABLESPACE logminer_tbs DATAFILE &#39;/opt/oracle/oradata/SID/logminer_tbs.dbf&#39; SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED; exit; (2.2). Create a user and grant permissions
sqlplus sys/password@host:port/SID AS SYSDBA; CREATE USER flinkuser IDENTIFIED BY flinkpw DEFAULT TABLESPACE LOGMINER_TBS QUOTA UNLIMITED ON LOGMINER_TBS; GRANT CREATE SESSION TO flinkuser; GRANT SET CONTAINER TO flinkuser; GRANT SELECT ON V_$DATABASE to flinkuser; GRANT FLASHBACK ANY TABLE TO flinkuser; GRANT SELECT ANY TABLE TO flinkuser; GRANT SELECT_CATALOG_ROLE TO flinkuser; GRANT EXECUTE_CATALOG_ROLE TO flinkuser; GRANT SELECT ANY TRANSACTION TO flinkuser; GRANT LOGMINING TO flinkuser; GRANT ANALYZE ANY TO flinkuser; GRANT CREATE TABLE TO flinkuser; -- need not to execute if set scan.incremental.snapshot.enabled=true(default) GRANT LOCK ANY TABLE TO flinkuser; GRANT ALTER ANY TABLE TO flinkuser; GRANT CREATE SEQUENCE TO flinkuser; GRANT EXECUTE ON DBMS_LOGMNR TO flinkuser; GRANT EXECUTE ON DBMS_LOGMNR_D TO flinkuser; GRANT SELECT ON V_$LOG TO flinkuser; GRANT SELECT ON V_$LOG_HISTORY TO flinkuser; GRANT SELECT ON V_$LOGMNR_LOGS TO flinkuser; GRANT SELECT ON V_$LOGMNR_CONTENTS TO flinkuser; GRANT SELECT ON V_$LOGMNR_PARAMETERS TO flinkuser; GRANT SELECT ON V_$LOGFILE TO flinkuser; GRANT SELECT ON V_$ARCHIVED_LOG TO flinkuser; GRANT SELECT ON V_$ARCHIVE_DEST_STATUS TO flinkuser; exit; For CDB database # Overall, the steps for configuring CDB database is quite similar to non-CDB database, but the commands may be different.
Enable log archiving
ORACLE_SID=ORCLCDB export ORACLE_SID sqlplus /nolog CONNECT sys/password AS SYSDBA alter system set db_recovery_file_dest_size = 10G; -- should exist alter system set db_recovery_file_dest = &#39;/opt/oracle/oradata/recovery_area&#39; scope=spfile; shutdown immediate startup mount alter database archivelog; alter database open; -- Should show &#34;Database log mode: Archive Mode&#34; archive log list exit; Note: You can also use the following commands to enable supplemental logging:
-- Enable supplemental logging for a specific table: ALTER TABLE inventory.customers ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS; -- Enable supplemental logging for database ALTER DATABASE ADD SUPPLEMENTAL LOG DATA; Create an Oracle user with permissions
sqlplus sys/password@//localhost:1521/ORCLCDB as sysdba CREATE TABLESPACE logminer_tbs DATAFILE &#39;/opt/oracle/oradata/ORCLCDB/logminer_tbs.dbf&#39; SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED; exit sqlplus sys/password@//localhost:1521/ORCLPDB1 as sysdba CREATE TABLESPACE logminer_tbs DATAFILE &#39;/opt/oracle/oradata/ORCLCDB/ORCLPDB1/logminer_tbs.dbf&#39; SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED; exit sqlplus sys/password@//localhost:1521/ORCLCDB as sysdba CREATE USER flinkuser IDENTIFIED BY flinkpw DEFAULT TABLESPACE logminer_tbs QUOTA UNLIMITED ON logminer_tbs CONTAINER=ALL; GRANT CREATE SESSION TO flinkuser CONTAINER=ALL; GRANT SET CONTAINER TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$DATABASE to flinkuser CONTAINER=ALL; GRANT FLASHBACK ANY TABLE TO flinkuser CONTAINER=ALL; GRANT SELECT ANY TABLE TO flinkuser CONTAINER=ALL; GRANT SELECT_CATALOG_ROLE TO flinkuser CONTAINER=ALL; GRANT EXECUTE_CATALOG_ROLE TO flinkuser CONTAINER=ALL; GRANT SELECT ANY TRANSACTION TO flinkuser CONTAINER=ALL; GRANT LOGMINING TO flinkuser CONTAINER=ALL; GRANT CREATE TABLE TO flinkuser CONTAINER=ALL; -- need not to execute if set scan.incremental.snapshot.enabled=true(default) GRANT LOCK ANY TABLE TO flinkuser CONTAINER=ALL; GRANT CREATE SEQUENCE TO flinkuser CONTAINER=ALL; GRANT EXECUTE ON DBMS_LOGMNR TO flinkuser CONTAINER=ALL; GRANT EXECUTE ON DBMS_LOGMNR_D TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$LOG TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$LOG_HISTORY TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$LOGMNR_LOGS TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$LOGMNR_CONTENTS TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$LOGMNR_PARAMETERS TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$LOGFILE TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$ARCHIVED_LOG TO flinkuser CONTAINER=ALL; GRANT SELECT ON V_$ARCHIVE_DEST_STATUS TO flinkuser CONTAINER=ALL; exit See more about the Setting up Oracle
How to create an Oracle CDC table # The Oracle CDC table can be defined as following:
-- register an Oracle table &#39;products&#39; in Flink SQL Flink SQL&gt; CREATE TABLE products ( ID INT NOT NULL, NAME STRING, DESCRIPTION STRING, WEIGHT DECIMAL(10, 3), PRIMARY KEY(id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;oracle-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;1521&#39;, &#39;username&#39; = &#39;flinkuser&#39;, &#39;password&#39; = &#39;flinkpw&#39;, &#39;database-name&#39; = &#39;ORCLCDB&#39;, &#39;schema-name&#39; = &#39;inventory&#39;, &#39;table-name&#39; = &#39;products&#39;); -- read snapshot and redo logs from products table Flink SQL&gt; SELECT * FROM products; Note: When working with the CDB + PDB model, you are expected to add an extra option 'debezium.database.pdb.name' = 'xxx' in Flink DDL to specific the name of the PDB to connect to.
Note: While the connector might work with a variety of Oracle versions and editions, only Oracle 9i, 10g, 11g and 12c have been tested.
Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'oracle-cdc'. hostname optional (none) String IP address or hostname of the Oracle database server. If the url is not empty, hostname may not be configured, otherwise hostname can not be empty username required (none) String Name of the Oracle database to use when connecting to the Oracle database server. password required (none) String Password to use when connecting to the Oracle database server. database-name required (none) String Database name of the Oracle server to monitor. schema-name required (none) String Schema name of the Oracle database to monitor. table-name required (none) String Table name of the Oracle database to monitor. port optional 1521 Integer Integer port number of the Oracle database server. url optional jdbc:oracle:thin:@{hostname}:{port}:{database-name} String JdbcUrl of the oracle database server . If the hostname and port parameter is configured, the URL is concatenated by hostname port database-name in SID format by default. Otherwise, you need to configure the URL parameter scan.startup.mode optional initial String Optional startup mode for Oracle CDC consumer, valid enumerations are "initial" and "latest-offset". Please see Startup Reading Position section for more detailed information. scan.incremental.snapshot.enabled optional true Boolean Incremental snapshot is a new mechanism to read snapshot of a table. Compared to the old snapshot mechanism, the incremental snapshot has many advantages, including: (1) source can be parallel during snapshot reading, (2) source can perform checkpoints in the chunk granularity during snapshot reading, (3) source doesn't need to acquire ROW SHARE MODE lock before snapshot reading. scan.incremental.snapshot.chunk.size optional 8096 Integer The chunk size (number of rows) of table snapshot, captured tables are split into multiple chunks when read the snapshot of table. scan.snapshot.fetch.size optional 1024 Integer The maximum fetch size for per poll when read table snapshot. connect.max-retries optional 3 Integer The max retry times that the connector should retry to build Oracle database server connection. connection.pool.size optional 20 Integer The connection pool size. debezium.* optional (none) String Pass-through Debezium's properties to Debezium Embedded Engine which is used to capture data changes from Oracle server. For example: 'debezium.snapshot.mode' = 'never'. See more about the Debezium's Oracle Connector properties scan.incremental.close-idle-reader.enabled optional false Boolean Whether to close idle readers at the end of the snapshot phase. The flink version is required to be greater than or equal to 1.14 when 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' is set to true.
If the flink version is greater than or equal to 1.15, the default value of 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' has been changed to true, so it does not need to be explicitly configured 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' = 'true' scan.incremental.snapshot.chunk.key-column optional (none) String The chunk key of table snapshot, captured tables are split into multiple chunks by a chunk key when read the snapshot of table. By default, the chunk key is 'ROWID'. This column must be a column of the primary key. Limitation # Can&rsquo;t perform checkpoint during scanning snapshot of tables # During scanning snapshot of database tables, since there is no recoverable position, we can&rsquo;t perform checkpoints. In order to not perform checkpoints, Oracle CDC source will keep the checkpoint waiting to timeout. The timeout checkpoint will be recognized as failed checkpoint, by default, this will trigger a failover for the Flink job. So if the database table is large, it is recommended to add following Flink configurations to avoid failover because of the timeout checkpoints:
execution.checkpointing.interval: 10min execution.checkpointing.tolerable-failed-checkpoints: 100 restart-strategy: fixed-delay restart-strategy.fixed-delay.attempts: 2147483647 Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Key DataType Description table_name STRING NOT NULL Name of the table that contain the row. schema_name STRING NOT NULL Name of the schema that contain the row. database_name STRING NOT NULL Name of the database that contain the row. op_ts TIMESTAMP_LTZ(3) NOT NULL It indicates the time that the change was made in the database. If the record is read from snapshot of the table instead of the change stream, the value is always 0. The extended CREATE TABLE example demonstrates the syntax for exposing these metadata fields:
CREATE TABLE products ( db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, schema_name STRING METADATA FROM &#39;schema_name&#39; VIRTUAL, table_name STRING METADATA FROM &#39;table_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, ID INT NOT NULL, NAME STRING, DESCRIPTION STRING, WEIGHT DECIMAL(10, 3), PRIMARY KEY(id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;oracle-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;1521&#39;, &#39;username&#39; = &#39;flinkuser&#39;, &#39;password&#39; = &#39;flinkpw&#39;, &#39;database-name&#39; = &#39;ORCLCDB&#39;, &#39;schema-name&#39; = &#39;inventory&#39;, &#39;table-name&#39; = &#39;products&#39;, &#39;debezium.log.mining.strategy&#39; = &#39;online_catalog&#39;, &#39;debezium.log.mining.continuous.mine&#39; = &#39;true&#39; ); Note : The Oracle dialect is case-sensitive, it converts field name to uppercase if the field name is not quoted, Flink SQL doesn&rsquo;t convert the field name. Thus for physical columns from oracle database, we should use its converted field name in Oracle when define an oracle-cdc table in Flink SQL.
Features # Exactly-Once Processing # The Oracle CDC connector is a Flink Source connector which will read database snapshot first and then continues to read change events with exactly-once processing even failures happen. Please read How the connector works.
Startup Reading Position # The config option scan.startup.mode specifies the startup mode for Oracle CDC consumer. The valid enumerations are:
initial (default): Performs an initial snapshot on the monitored database tables upon first startup, and continue to read the latest redo log. latest-offset: Never to perform a snapshot on the monitored database tables upon first startup, just read from the change since the connector was started. Note: the mechanism of scan.startup.mode option relying on Debezium&rsquo;s snapshot.mode configuration. So please do not use them together. If you specific both scan.startup.mode and debezium.snapshot.mode options in the table DDL, it may make scan.startup.mode doesn&rsquo;t work.
Single Thread Reading # The Oracle CDC source can&rsquo;t work in parallel reading, because there is only one task can receive change events.
DataStream Source # The Oracle CDC connector can also be a DataStream source. There are two modes for the DataStream source:
incremental snapshot based, which allows parallel reading SourceFunction based, which only supports single thread reading Incremental Snapshot based DataStream (Experimental) # import org.apache.flink.cdc.connectors.base.options.StartupOptions; import org.apache.flink.cdc.connectors.base.source.jdbc.JdbcIncrementalSource; import org.apache.flink.cdc.connectors.oracle.source.OracleSourceBuilder; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import java.util.Properties; public class OracleParallelSourceExample { public static void main(String[] args) throws Exception { Properties debeziumProperties = new Properties(); debeziumProperties.setProperty(&#34;log.mining.strategy&#34;, &#34;online_catalog&#34;); JdbcIncrementalSource&lt;String&gt; oracleChangeEventSource = new OracleSourceBuilder() .hostname(&#34;host&#34;) .port(1521) .databaseList(&#34;ORCLCDB&#34;) .schemaList(&#34;DEBEZIUM&#34;) .tableList(&#34;DEBEZIUM.PRODUCTS&#34;) .username(&#34;username&#34;) .password(&#34;password&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) .includeSchemaChanges(true) // output the schema changes as well .startupOptions(StartupOptions.initial()) .debeziumProperties(debeziumProperties) .splitSize(2) .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000L); // set the source parallelism to 4 env.fromSource( oracleChangeEventSource, WatermarkStrategy.noWatermarks(), &#34;OracleParallelSource&#34;) .setParallelism(4) .print() .setParallelism(1); env.execute(&#34;Print Oracle Snapshot + RedoLog&#34;); } } SourceFunction-based DataStream # import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.cdc.connectors.oracle.OracleSource; public class OracleSourceExample { public static void main(String[] args) throws Exception { SourceFunction&lt;String&gt; sourceFunction = OracleSource.&lt;String&gt;builder() .url(&#34;jdbc:oracle:thin:@{hostname}:{port}:{database}&#34;) .port(1521) .database(&#34;ORCLCDB&#34;) // monitor XE database .schemaList(&#34;inventory&#34;) // monitor inventory schema .tableList(&#34;inventory.products&#34;) // monitor products table .username(&#34;flinkuser&#34;) .password(&#34;flinkpw&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env .addSource(sourceFunction) .print().setParallelism(1); // use parallelism 1 for sink to keep message ordering env.execute(); } } Data Type Mapping # Oracle type Flink SQL type NUMBER(p, s <= 0), p - s < 3 TINYINT NUMBER(p, s <= 0), p - s < 5 SMALLINT NUMBER(p, s <= 0), p - s < 10 INT NUMBER(p, s <= 0), p - s < 19 BIGINT NUMBER(p, s <= 0), 19 <= p - s <= 38 DECIMAL(p - s, 0) NUMBER(p, s > 0) DECIMAL(p, s) NUMBER(p, s <= 0), p - s > 38 STRING FLOAT
BINARY_FLOAT FLOAT DOUBLE PRECISION
BINARY_DOUBLE DOUBLE NUMBER(1) BOOLEAN DATE
TIMESTAMP [(p)] TIMESTAMP [(p)] [WITHOUT TIMEZONE] TIMESTAMP [(p)] WITH TIME ZONE TIMESTAMP [(p)] WITH TIME ZONE TIMESTAMP [(p)] WITH LOCAL TIME ZONE TIMESTAMP_LTZ [(p)] CHAR(n)
NCHAR(n)
NVARCHAR2(n)
VARCHAR(n)
VARCHAR2(n)
CLOB
NCLOB
XMLType
SYS.XMLTYPE STRING BLOB
ROWID BYTES INTERVAL DAY TO SECOND
INTERVAL YEAR TO MONTH BIGINT Back to top
`}),e.add({id:27,href:"/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/paimon/",title:"Paimon",section:"Pipeline Connectors",content:` Paimon Pipeline Connector # The Paimon Pipeline connector can be used as the Data Sink of the pipeline, and write data to Paimon. This document describes how to set up the Paimon Pipeline connector.
What can the connector do? # Create table automatically if not exist Schema change synchronization Data synchronization How to create Pipeline # The pipeline for reading data from MySQL and sink to Paimon can be defined as follows:
source: type: mysql name: MySQL Source hostname: 127.0.0.1 port: 3306 username: admin password: pass tables: adb.\\.*, bdb.user_table_[0-9]+, [app|web].order_\\.* server-id: 5401-5404 sink: type: paimon name: Paimon Sink catalog.properties.metastore: filesystem catalog.properties.warehouse: /path/warehouse pipeline: name: MySQL to Paimon Pipeline parallelism: 2 Pipeline Connector Options # Option Required Default Type Description type required (none) String Specify what connector to use, here should be 'paimon'. name optional (none) String The name of the sink. catalog.properties.metastore required (none) String Metastore of paimon catalog, supports filesystem and hive. catalog.properties.warehouse optional (none) String The warehouse root path of catalog. catalog.properties.uri optional (none) String Uri of metastore server. commit.user optional "admin" String User name for committing data files. partition.key optional (none) String Partition keys for each partitioned table, allow setting multiple primary keys for multiTables. Each table are separated by ';', and each partition key are separated by ','. For example, we can set partition.key of two tables by 'testdb.table1:id1,id2;testdb.table2:name'. catalog.properties.* optional (none) String Pass options of Paimon catalog to pipeline，See Paimon catalog options。 table.properties.* optional (none) String Pass options of Paimon table to pipeline，See Paimon table options。 Usage Notes # Only support Paimon primary key table, so the source table must have primary keys.
Not support exactly-once. The connector uses at-least-once + primary key table for idempotent writing.
Data Type Mapping # CDC type Paimon type NOTE TINYINT TINYINT SMALLINT SMALLINT INT INT BIGINT BIGINT FLOAT FLOAT DOUBLE DOUBLE DECIMAL(p, s) DECIMAL(p, s) BOOLEAN BOOLEAN DATE DATE TIMESTAMP TIMESTAMP TIMESTAMP_LTZ TIMESTAMP_LTZ CHAR(n) CHAR(n) VARCHAR(n) VARCHAR(n) Back to top
`}),e.add({id:28,href:"/flink/flink-cdc-docs-master/docs/deployment/",title:"Deployment",section:"Docs",content:" "}),e.add({id:29,href:"/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/kafka/",title:"Kafka",section:"Pipeline Connectors",content:` Kafka Pipeline Connector # The Kafka Pipeline connector can be used as the Data Sink of the pipeline, and write data to Kafka. This document describes how to set up the Kafka Pipeline connector.
What can the connector do? # Data synchronization How to create Pipeline # The pipeline for reading data from MySQL and sink to Kafka can be defined as follows:
source: type: mysql name: MySQL Source hostname: 127.0.0.1 port: 3306 username: admin password: pass tables: adb.\\.*, bdb.user_table_[0-9]+, [app|web].order_\\.* server-id: 5401-5404 sink: type: kafka name: Kafka Sink properties.bootstrap.servers: PLAINTEXT://localhost:62510 pipeline: name: MySQL to Kafka Pipeline parallelism: 2 Pipeline Connector Options # Option Required Default Type Description type required (none) String Specify what connector to use, here should be 'kafka'. name optional (none) String The name of the sink. value.format optional (none) String The format used to serialize the value part of Kafka messages. Available options are debezium-json and canal-json, default option is \`debezium-json\`, and do not support user-defined format now. properties.bootstrap.servers required (none) String A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. topic optional (none) String If this parameter is configured, all events will be sent to this topic. sink.add-tableId-to-header-enabled optional (none) Boolean If this parameter is true, a header with key of 'namespace','schemaName','tableName' will be added for each Kafka record. Default value is false. properties.* optional (none) String Pass options of Kafka table to pipeline，See Kafka consume options. sink.custom-header optional (none) String custom headers for each kafka record. Each header are separated by ',', separate key and value by ':'. For example, we can set headers like 'key1:value1,key2:value2'. Usage Notes # The written topic of Kafka will be namespace.schemaName.tableName string of TableId，this can be changed using route function of pipeline.
If the written topic of Kafka is not existed, we will create one automatically.
Data Type Mapping # CDC type JSON type NOTE TINYINT TINYINT SMALLINT SMALLINT INT INT BIGINT BIGINT FLOAT FLOAT DOUBLE DOUBLE DECIMAL(p, s) DECIMAL(p, s) BOOLEAN BOOLEAN DATE DATE TIMESTAMP TIMESTAMP TIMESTAMP_LTZ TIMESTAMP_LTZ CHAR(n) CHAR(n) VARCHAR(n) VARCHAR(n) Back to top
`}),e.add({id:30,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/oracle-tutorial/",title:"Oracle Tutorial",section:"Tutorials",content:` Demo: Oracle CDC to Elasticsearch # Create docker-compose.yml file using following contents:
version: &#39;2.1&#39; services: oracle: image: goodboy008/oracle-19.3.0-ee:non-cdb ports: - &#34;1521:1521&#34; elasticsearch: image: elastic/elasticsearch:7.6.0 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34; - discovery.type=single-node ports: - &#34;9200:9200&#34; - &#34;9300:9300&#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 ports: - &#34;5601:5601&#34; volumes: - /var/run/docker.sock:/var/run/docker.sock The Docker Compose environment consists of the following containers:
Oracle: Oracle 19c database. Elasticsearch: store the join result of the orders and products table. Kibana: mainly used to visualize the data in Elasticsearch To start all containers, run the following command in the directory that contains the docker-compose.yml file.
docker-compose up -d This command automatically starts all the containers defined in the Docker Compose configuration in a detached mode. Run docker ps to check whether these containers are running properly. You can also visit http://localhost:5601/ to see if Kibana is running normally.
Don’t forget to run the following command to stop all containers after you finished the tutorial:
docker-compose down Download following JAR package to &lt;FLINK_HOME&gt;/lib
Download links are available only for stable releases, SNAPSHOT dependencies need to be built based on master or release-branches by yourself.
flink-sql-connector-elasticsearch7-3.0.1-1.17.jar flink-sql-connector-oracle-cdc-3.0-SNAPSHOT.jar Preparing data in Oracle database
Introduce the tables in Oracle:
docker-compose exec oracle sqlplus debezium/dbz@localhost:1521/ORCLCDB BEGIN EXECUTE IMMEDIATE &#39;DROP TABLE DEBEZIUM.PRODUCTS&#39;; EXCEPTION WHEN OTHERS THEN IF SQLCODE != -942 THEN RAISE; END IF; END; / CREATE TABLE DEBEZIUM.PRODUCTS ( ID NUMBER(9, 0) NOT NULL, NAME VARCHAR(255) NOT NULL, DESCRIPTION VARCHAR(512), WEIGHT FLOAT, PRIMARY KEY(ID) ); BEGIN EXECUTE IMMEDIATE &#39;DROP TABLE DEBEZIUM.ORDERS&#39;; EXCEPTION WHEN OTHERS THEN IF SQLCODE != -942 THEN RAISE; END IF; END; / CREATE TABLE DEBEZIUM.ORDERS ( ID NUMBER(9, 0) NOT NULL, ORDER_DATE TIMESTAMP(3) NOT NULL, PURCHASER VARCHAR(255) NOT NULL, QUANTITY NUMBER(9, 0) NOT NULL, PRODUCT_ID NUMBER(9, 0) NOT NULL, PRIMARY KEY(ID) ); ALTER TABLE DEBEZIUM.PRODUCTS ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS; ALTER TABLE DEBEZIUM.ORDERS ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS; INSERT INTO DEBEZIUM.PRODUCTS VALUES (101, &#39;scooter&#39;, &#39;Small 2-wheel scooter&#39;, 3.14); INSERT INTO DEBEZIUM.PRODUCTS VALUES (102, &#39;car battery&#39;, &#39;12V car battery&#39;, 8.1); INSERT INTO DEBEZIUM.PRODUCTS VALUES (103, &#39;12-pack drill bits&#39;, &#39;12-pack of drill bits with sizes ranging from #40 to #3&#39;, 0.8); INSERT INTO DEBEZIUM.PRODUCTS VALUES (104, &#39;hammer&#39;, &#39;12oz carpenter&#39;&#39;s hammer&#39;, 0.75); INSERT INTO DEBEZIUM.PRODUCTS VALUES (105, &#39;hammer&#39;, &#39;14oz carpenter&#39;&#39;s hammer&#39;, 0.875); INSERT INTO DEBEZIUM.PRODUCTS VALUES (106, &#39;hammer&#39;, &#39;16oz carpenter&#39;&#39;s hammer&#39;, 1.0); INSERT INTO DEBEZIUM.PRODUCTS VALUES (107, &#39;rocks&#39;, &#39;box of assorted rocks&#39;, 5.3); INSERT INTO DEBEZIUM.PRODUCTS VALUES (108, &#39;jacket&#39;, &#39;water resistent black wind breaker&#39;, 0.1); INSERT INTO DEBEZIUM.PRODUCTS VALUES (109, &#39;spare tire&#39;, &#39;24 inch spare tire&#39;, 22.2); INSERT INTO DEBEZIUM.ORDERS VALUES (1001, TO_TIMESTAMP(&#39;2020-07-30 10:08:22.001000&#39;, &#39;YYYY-MM-DD HH24:MI:SS.FF&#39;), &#39;Jark&#39;, 1, 101); INSERT INTO DEBEZIUM.ORDERS VALUES (1002, TO_TIMESTAMP(&#39;2020-07-30 10:11:09.001000&#39;, &#39;YYYY-MM-DD HH24:MI:SS.FF&#39;), &#39;Sally&#39;, 2, 102); INSERT INTO DEBEZIUM.ORDERS VALUES (1003, TO_TIMESTAMP(&#39;2020-07-30 12:00:30.001000&#39;, &#39;YYYY-MM-DD HH24:MI:SS.FF&#39;), &#39;Edward&#39;, 2, 103); INSERT INTO DEBEZIUM.ORDERS VALUES (1004, TO_TIMESTAMP(&#39;2020-07-30 15:22:00.001000&#39;, &#39;YYYY-MM-DD HH24:MI:SS.FF&#39;), &#39;Jark&#39;, 1, 104); Launch a Flink cluster and start a Flink SQL CLI
Execute following SQL statements in the Flink SQL CLI:
-- Flink SQL -- checkpoint every 3000 milliseconds Flink SQL&gt; SET execution.checkpointing.interval = 3s; Flink SQL&gt; CREATE TABLE products ( ID INT, NAME STRING, DESCRIPTION STRING, PRIMARY KEY (ID) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;oracle-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;1521&#39;, &#39;username&#39; = &#39;dbzuser&#39;, &#39;password&#39; = &#39;dbz&#39;, &#39;database-name&#39; = &#39;ORCLCDB&#39;, &#39;schema-name&#39; = &#39;DEBEZIUM&#39;, &#39;table-name&#39; = &#39;products&#39; ); Flink SQL&gt; CREATE TABLE orders ( ID INT, ORDER_DATE TIMESTAMP(3), PURCHASER STRING, QUANTITY INT, PRODUCT_ID INT, ORDER_STATUS BOOLEAN ) WITH ( &#39;connector&#39; = &#39;oracle-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;1521&#39;, &#39;username&#39; = &#39;dbzuser&#39;, &#39;password&#39; = &#39;dbz&#39;, &#39;database-name&#39; = &#39;ORCLCDB&#39;, &#39;schema-name&#39; = &#39;DEBEZIUM&#39;, &#39;table-name&#39; = &#39;orders&#39; ); Flink SQL&gt; CREATE TABLE enriched_orders ( ORDER_ID INT, ORDER_DATE TIMESTAMP(3), PURCHASER STRING, QUANTITY INT, PRODUCT_NAME STRING, PRODUCT_DESCRIPTION STRING, PRIMARY KEY (ORDER_ID) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;elasticsearch-7&#39;, &#39;hosts&#39; = &#39;http://localhost:9200&#39;, &#39;index&#39; = &#39;enriched_orders_1&#39; ); Flink SQL&gt; INSERT INTO enriched_orders SELECT o.ID,o.ORDER_DATE,o.PURCHASER,o.QUANTITY, p.NAME, p.DESCRIPTION FROM orders AS o LEFT JOIN products AS p ON o.PRODUCT_ID = p.ID; Check result in Elasticsearch
Check the data has been written to Elasticsearch successfully, you can visit Kibana to see the data.
Make changes in Oracle and watch result in Elasticsearch
Enter Oracle&rsquo;s container to make some changes in Oracle, then you can see the result in Elasticsearch will change after executing every SQL statement:
docker-compose exec oracle sqlplus debezium/dbz@localhost:1521/ORCLCDB INSERT INTO DEBEZIUM.ORDERS VALUES (1005, TO_TIMESTAMP(&#39;2020-07-30 15:22:00.001000&#39;, &#39;YYYY-MM-DD HH24:MI:SS.FF&#39;), &#39;Jark&#39;, 5, 105); UPDATE DEBEZIUM.ORDERS SET QUANTITY = 10 WHERE ID = 1002; DELETE FROM DEBEZIUM.ORDERS WHERE ID = 1004; Back to top
`}),e.add({id:31,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/sqlserver-cdc/",title:"SQL Server",section:"Flink Sources",content:` SQLServer CDC Connector # The SQLServer CDC connector allows for reading snapshot data and incremental data from SQLServer database. This document describes how to setup the SQLServer CDC connector to run SQL queries against SQLServer databases.
Dependencies # In order to setup the SQLServer CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency # &ltdependency&gt &ltgroupId&gtorg.apache.flink&lt/groupId&gt &ltartifactId&gtflink-connector-sqlserver-cdc&lt/artifactId&gt &ltversion&gt3.2-SNAPSHOT&lt/version&gt &lt/dependency&gt Copied to clipboard! SQL Client JAR # Download link is available only for stable releases.
Download flink-sql-connector-sqlserver-cdc-3.0.1.jar and put it under &lt;FLINK_HOME&gt;/lib/.
Note: Refer to flink-sql-connector-sqlserver-cdc, more released versions will be available in the Maven central warehouse.
Setup SQLServer Database # A SQL Server administrator must enable change data capture on the source tables that you want to capture. The database must already be enabled for CDC. To enable CDC on a table, a SQL Server administrator runs the stored procedure sys.sp_cdc_enable_table for the table.
Prerequisites:
CDC is enabled on the SQL Server database. The SQL Server Agent is running. You are a member of the db_owner fixed database role for the database. Procedure:
Connect to the SQL Server database by database management studio. Run the following SQL statement to enable CDC on the table. USE MyDB GO EXEC sys.sp_cdc_enable_table @source_schema = N&#39;dbo&#39;, -- Specifies the schema of the source table. @source_name = N&#39;MyTable&#39;, -- Specifies the name of the table that you want to capture. @role_name = N&#39;MyRole&#39;, -- Specifies a role MyRole to which you can add users to whom you want to grant SELECT permission on the captured columns of the source table. Users in the sysadmin or db_owner role also have access to the specified change tables. Set the value of @role_name to NULL, to allow only members in the sysadmin or db_owner to have full access to captured information. @filegroup_name = N&#39;MyDB_CT&#39;,-- Specifies the filegroup where SQL Server places the change table for the captured table. The named filegroup must already exist. It is best not to locate change tables in the same filegroup that you use for source tables. @supports_net_changes = 0 GO Verifying that the user has access to the CDC table --The following example runs the stored procedure sys.sp_cdc_help_change_data_capture on the database MyDB: USE MyDB; GO EXEC sys.sp_cdc_help_change_data_capture GO The query returns configuration information for each table in the database that is enabled for CDC and that contains change data that the caller is authorized to access. If the result is empty, verify that the user has privileges to access both the capture instance and the CDC tables.
How to create a SQLServer CDC table # The SqlServer CDC table can be defined as following:
-- register a SqlServer table &#39;orders&#39; in Flink SQL CREATE TABLE orders ( id INT, order_date DATE, purchaser INT, quantity INT, product_id INT, PRIMARY KEY (id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;sqlserver-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;1433&#39;, &#39;username&#39; = &#39;sa&#39;, &#39;password&#39; = &#39;Password!&#39;, &#39;database-name&#39; = &#39;inventory&#39;, &#39;table-name&#39; = &#39;dob.orders&#39; ); -- read snapshot and binlogs from orders table SELECT * FROM orders; Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'sqlserver-cdc'. hostname required (none) String IP address or hostname of the SQLServer database. username required (none) String Username to use when connecting to the SQLServer database. password required (none) String Password to use when connecting to the SQLServer database. database-name required (none) String Database name of the SQLServer database to monitor. table-name required (none) String Table name of the SQLServer database to monitor, e.g.: "db1.table1" port optional 1433 Integer Integer port number of the SQLServer database. server-time-zone optional UTC String The session time zone in database server, e.g. "Asia/Shanghai". scan.incremental.snapshot.enabled optional true Boolean Whether enable parallelism snapshot. chunk-meta.group.size optional 1000 Integer The group size of chunk meta, if the meta size exceeds the group size, the meta will be divided into multiple groups. chunk-key.even-distribution.factor.lower-bound optional 0.05d Double The lower bound of chunk key distribution factor. The distribution factor is used to determine whether the table is evenly distribution or not. The table chunks would use evenly calculation optimization when the data distribution is even, and the query for splitting would happen when it is uneven. The distribution factor could be calculated by (MAX(id) - MIN(id) + 1) / rowCount. chunk-key.even-distribution.factor.upper-bound optional 1000.0d Double The upper bound of chunk key distribution factor. The distribution factor is used to determine whether the table is evenly distribution or not. The table chunks would use evenly calculation optimization when the data distribution is even, and the query for splitting would happen when it is uneven. The distribution factor could be calculated by (MAX(id) - MIN(id) + 1) / rowCount. debezium.* optional (none) String Pass-through Debezium's properties to Debezium Embedded Engine which is used to capture data changes from SQLServer. For example: 'debezium.snapshot.mode' = 'initial_only'. See more about the Debezium's SQLServer Connector properties scan.incremental.close-idle-reader.enabled optional false Boolean Whether to close idle readers at the end of the snapshot phase. The flink version is required to be greater than or equal to 1.14 when 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' is set to true.
If the flink version is greater than or equal to 1.15, the default value of 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' has been changed to true, so it does not need to be explicitly configured 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' = 'true' scan.incremental.snapshot.chunk.key-column optional (none) String The chunk key of table snapshot, captured tables are split into multiple chunks by a chunk key when read the snapshot of table. By default, the chunk key is the first column of the primary key. This column must be a column of the primary key. Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Key DataType Description table_name STRING NOT NULL Name of the table that contain the row. schema_name STRING NOT NULL Name of the schema that contain the row. database_name STRING NOT NULL Name of the database that contain the row. op_ts TIMESTAMP_LTZ(3) NOT NULL It indicates the time that the change was made in the database. If the record is read from snapshot of the table instead of the change stream, the value is always 0. Limitation # Can&rsquo;t perform checkpoint during scanning snapshot of tables # During scanning snapshot of database tables, since there is no recoverable position, we can&rsquo;t perform checkpoints. In order to not perform checkpoints, SqlServer CDC source will keep the checkpoint waiting to timeout. The timeout checkpoint will be recognized as failed checkpoint, by default, this will trigger a failover for the Flink job. So if the database table is large, it is recommended to add following Flink configurations to avoid failover because of the timeout checkpoints:
execution.checkpointing.interval: 10min execution.checkpointing.tolerable-failed-checkpoints: 100 restart-strategy: fixed-delay restart-strategy.fixed-delay.attempts: 2147483647 The extended CREATE TABLE example demonstrates the syntax for exposing these metadata fields:
CREATE TABLE products ( table_name STRING METADATA FROM &#39;table_name&#39; VIRTUAL, schema_name STRING METADATA FROM &#39;schema_name&#39; VIRTUAL, db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, id INT NOT NULL, name STRING, description STRING, weight DECIMAL(10,3) ) WITH ( &#39;connector&#39; = &#39;sqlserver-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;1433&#39;, &#39;username&#39; = &#39;sa&#39;, &#39;password&#39; = &#39;Password!&#39;, &#39;database-name&#39; = &#39;inventory&#39;, &#39;table-name&#39; = &#39;dbo.products&#39; ); Features # Exactly-Once Processing # The SQLServer CDC connector is a Flink Source connector which will read database snapshot first and then continues to read change events with exactly-once processing even failures happen. Please read How the connector works.
Startup Reading Position # The config option scan.startup.mode specifies the startup mode for SQLServer CDC consumer. The valid enumerations are:
initial (default): Takes a snapshot of structure and data of captured tables; useful if topics should be populated with a complete representation of the data from the captured tables. initial-only: Takes a snapshot of structure and data like initial but instead does not transition into streaming changes once the snapshot has completed. latest-offset: Takes a snapshot of the structure of captured tables only; useful if only changes happening from now onwards should be propagated to topics. Note: the mechanism of scan.startup.mode option relying on Debezium&rsquo;s snapshot.mode configuration. So please do not use them together. If you specific both scan.startup.mode and debezium.snapshot.mode options in the table DDL, it may make scan.startup.mode doesn&rsquo;t work.
Single Thread Reading # The SQLServer CDC source can&rsquo;t work in parallel reading, because there is only one task can receive change events.
DataStream Source # The SQLServer CDC connector can also be a DataStream source. You can create a SourceFunction as the following shows:
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.cdc.connectors.sqlserver.SqlServerSource; public class SqlServerSourceExample { public static void main(String[] args) throws Exception { SourceFunction&lt;String&gt; sourceFunction = SqlServerSource.&lt;String&gt;builder() .hostname(&#34;localhost&#34;) .port(1433) .database(&#34;sqlserver&#34;) // monitor sqlserver database .tableList(&#34;dbo.products&#34;) // monitor products table .username(&#34;sa&#34;) .password(&#34;Password!&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env .addSource(sourceFunction) .print().setParallelism(1); // use parallelism 1 for sink to keep message ordering env.execute(); } } The SQLServer CDC incremental connector (after 2.4.0) can be used as the following shows:
import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.cdc.connectors.base.options.StartupOptions; import org.apache.flink.cdc.connectors.sqlserver.source.SqlServerSourceBuilder; import org.apache.flink.cdc.connectors.sqlserver.source.SqlServerSourceBuilder.SqlServerIncrementalSource; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; public class SqlServerIncrementalSourceExample { public static void main(String[] args) throws Exception { SqlServerIncrementalSource&lt;String&gt; sqlServerSource = new SqlServerSourceBuilder() .hostname(&#34;localhost&#34;) .port(1433) .databaseList(&#34;inventory&#34;) .tableList(&#34;dbo.products&#34;) .username(&#34;sa&#34;) .password(&#34;Password!&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) .startupOptions(StartupOptions.initial()) .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000); // set the source parallelism to 2 env.fromSource( sqlServerSource, WatermarkStrategy.noWatermarks(), &#34;SqlServerIncrementalSource&#34;) .setParallelism(2) .print() .setParallelism(1); env.execute(&#34;Print SqlServer Snapshot + Change Stream&#34;); } } Data Type Mapping # SQLServer type Flink SQL type char(n) CHAR(n) varchar(n)
nvarchar(n)
nchar(n) VARCHAR(n) text
ntext
xml STRING decimal(p, s)
money
smallmoney DECIMAL(p, s) numeric NUMERIC float
real DOUBLE bit BOOLEAN int INT tinyint SMALLINT smallint SMALLINT bigint BIGINT date DATE time(n) TIME(n) datetime2
datetime
smalldatetime TIMESTAMP(n) datetimeoffset TIMESTAMP_LTZ(3) Back to top
`}),e.add({id:32,href:"/flink/flink-cdc-docs-master/docs/core-concept/table-id/",title:"Table ID",section:"Core Concept",content:` Definition # When connecting to external systems, it is necessary to establish a mapping relationship with the storage objects of the external system. This is what Table Id refers to.
Example # To be compatible with most external systems, the Table Id is represented by a 3-tuple : (namespace, schemaName, tableName).
Connectors should establish the mapping between Table Id and storage objects in external systems.
The following table lists the parts in table Id of different data systems:
data system parts in tableId String example Oracle/PostgreSQL database, schema, table mydb.default.orders MySQL/Doris/StarRocks database, table mydb.orders Kafka topic orders `}),e.add({id:33,href:"/flink/flink-cdc-docs-master/docs/developer-guide/",title:"Developer Guide",section:"Docs",content:" "}),e.add({id:34,href:"/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/doris/",title:"Doris",section:"Pipeline Connectors",content:` Doris Connector # This article introduces of Doris Connector
Example # source: type: values name: ValuesSource sink: type: doris name: Doris Sink fenodes: 127.0.0.1:8030 username: root password: &#34;&#34; table.create.properties.replication_num: 1 pipeline: parallelism: 1 Connector Options # Option Required Default Type Description type required (none) String Specify the Sink to use, here is 'doris'. name optional (none) String Name of PipeLine fenodes required (none) String Http address of Doris cluster FE, such as 127.0.0.1:8030 benodes optional (none) String Http address of Doris cluster BE, such as 127.0.0.1:8040 jdbc-url optional (none) String JDBC address of Doris cluster, for example: jdbc:mysql://127.0.0.1:9030/db username required (none) String Username of Doris cluster password optional (none) String Password for Doris cluster auto-redirect optional false String Whether to write through FE redirection and directly connect to BE to write sink.enable.batch-mode optional true Boolean Whether to use the batch method to write to Doris sink.flush.queue-size optional 2 Integer Queue size for batch writing sink.buffer-flush.max-rows optional 50000 Integer Maximum number of Flush records in a single batch sink.buffer-flush.max-bytes optional 10485760(10MB) Integer Maximum number of bytes flushed in a single batch sink.buffer-flush.interval optional 10s String Flush interval duration. If this time is exceeded, the data will be flushed asynchronously sink.properties. optional (none) String Parameters of StreamLoad. For example: sink.properties.strict_mode: true. See more about StreamLoad Properties properties table.create.properties.* optional (none) String Create the Properties configuration of the table. For example: table.create.properties.replication_num: 1. See more about Doris Table Properties properties Data Type Mapping # Flink CDC Type Doris Type Note TINYINT TINYINT SMALLINT SMALLINT INT INT BIGINT BIGINT DECIMAL DECIMAL FLOAT FLOAT DOUBLE DOUBLE BOOLEAN BOOLEAN DATE DATE TIMESTAMP [(p)] DATETIME [(p)] TIMESTAMP_LTZ [(p)] DATETIME [(p)] CHAR(n) CHAR(n*3) In Doris, strings are stored in UTF-8 encoding, so English characters occupy 1 byte and Chinese characters occupy 3 bytes. The length here is multiplied by 3. The maximum length of CHAR is 255. Once exceeded, it will automatically be converted to VARCHAR type. VARCHAR(n) VARCHAR(n*3) Same as above. The length here is multiplied by 3. The maximum length of VARCHAR is 65533. Once exceeded, it will automatically be converted to STRING type. BINARY(n) STRING VARBINARY(N) STRING STRING STRING Back to top
`}),e.add({id:35,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/polardbx-tutorial/",title:"PolarDB-X Tutorial",section:"Tutorials",content:` Demo: PolarDB-X CDC to Elasticsearch # This tutorial is to show how to quickly build streaming ETL for PolarDB-X with Flink CDC.
Assuming we are running an e-commerce business. The product and order data stored in PolarDB-X. We want to enrich the orders using the product table, and then load the enriched orders to ElasticSearch in real time.
In the following sections, we will describe how to use Flink PolarDB-X CDC to implement it. All exercises in this tutorial are performed in the Flink SQL CLI, and the entire process uses standard SQL syntax, without a single line of Java/Scala code or IDE installation.
Preparation # Prepare a Linux or MacOS computer with Docker installed.
Starting components required # The components required in this demo are all managed in containers, so we will use docker-compose to start them.
Create docker-compose.yml file using following contents:
version: &#39;2.1&#39; services: polardbx: polardbx: image: polardbx/polardb-x:2.0.1 container_name: polardbx ports: - &#34;8527:8527&#34; elasticsearch: image: &#39;elastic/elasticsearch:7.6.0&#39; container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - ES_JAVA_OPTS=-Xms512m -Xmx512m - discovery.type=single-node ports: - &#39;9200:9200&#39; - &#39;9300:9300&#39; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: &#39;elastic/kibana:7.6.0&#39; container_name: kibana ports: - &#39;5601:5601&#39; volumes: - &#39;/var/run/docker.sock:/var/run/docker.sock&#39; The Docker Compose environment consists of the following containers:
PolarDB-X: the products,orders tables will be store in the database. They will be joined enrich the orders. Elasticsearch: mainly used as a data sink to store enriched orders. Kibana: used to visualize the data in Elasticsearch. To start all containers, run the following command in the directory that contains the docker-compose.yml file.
docker-compose up -d This command automatically starts all the containers defined in the Docker Compose configuration in a detached mode. Run docker ps to check whether these containers are running properly. We can also visit http://localhost:5601/ to see if Kibana is running normally.
Preparing Flink and JAR package required # Download Flink 1.18.0 and unzip it to the directory flink-1.18.0
Download following JAR package required and put them under flink-1.18.0/lib/:
Download links are available only for stable releases, SNAPSHOT dependencies need to be built based on master or release branches by yourself.
flink-sql-connector-mysql-cdc-3.0-SNAPSHOT.jar flink-sql-connector-elasticsearch7-3.0.1-1.17.jar Preparing data in databases # Preparing data in PolarDB-X # Enter PolarDB-X Database: mysql -h127.0.0.1 -P8527 -upolardbx_root -p&#34;123456&#34; Create tables and populate data: -- PolarDB-X CREATE TABLE products ( id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(512) ) AUTO_INCREMENT = 101; INSERT INTO products VALUES (default,&#34;scooter&#34;,&#34;Small 2-wheel scooter&#34;), (default,&#34;car battery&#34;,&#34;12V car battery&#34;), (default,&#34;12-pack drill bits&#34;,&#34;12-pack of drill bits with sizes ranging from #40 to #3&#34;), (default,&#34;hammer&#34;,&#34;12oz carpenter&#39;s hammer&#34;), (default,&#34;hammer&#34;,&#34;14oz carpenter&#39;s hammer&#34;), (default,&#34;hammer&#34;,&#34;16oz carpenter&#39;s hammer&#34;), (default,&#34;rocks&#34;,&#34;box of assorted rocks&#34;), (default,&#34;jacket&#34;,&#34;water resistent black wind breaker&#34;), (default,&#34;spare tire&#34;,&#34;24 inch spare tire&#34;); CREATE TABLE orders ( order_id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, order_date DATETIME NOT NULL, customer_name VARCHAR(255) NOT NULL, price DECIMAL(10, 5) NOT NULL, product_id INTEGER NOT NULL, order_status BOOLEAN NOT NULL -- Whether order has been placed ) AUTO_INCREMENT = 10001; INSERT INTO orders VALUES (default, &#39;2020-07-30 10:08:22&#39;, &#39;Jark&#39;, 50.50, 102, false), (default, &#39;2020-07-30 10:11:09&#39;, &#39;Sally&#39;, 15.00, 105, false), (default, &#39;2020-07-30 12:00:30&#39;, &#39;Edward&#39;, 25.25, 106, false); Starting Flink cluster and Flink SQL CLI # Use the following command to change to the Flink directory:
cd flink-1.18.0 Use the following command to start a Flink cluster:
./bin/start-cluster.sh Then we can visit http://localhost:8081/ to see if Flink is running normally, and the web page looks like:
Use the following command to start a Flink SQL CLI:
./bin/sql-client.sh We should see the welcome screen of the CLI client.
Creating tables using Flink DDL in Flink SQL CLI # First, enable checkpoints every 3 seconds
-- Flink SQL Flink SQL&gt; SET execution.checkpointing.interval = 3s; Then, create tables that capture the change data from the corresponding database tables.
-- Flink SQL Flink SQL&gt; SET execution.checkpointing.interval = 3s; -- create source table2 - orders Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;127.0.0.1&#39;, &#39;port&#39; = &#39;8527&#39;, &#39;username&#39; = &#39;polardbx_root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;orders&#39; ); -- create source table2 - products CREATE TABLE products ( id INT, name STRING, description STRING, PRIMARY KEY (id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;127.0.0.1&#39;, &#39;port&#39; = &#39;8527&#39;, &#39;username&#39; = &#39;polardbx_root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;products&#39; ); Finally, create enriched_orders table that is used to load data to the Elasticsearch.
-- Flink SQL -- create sink table - enrich_orders Flink SQL&gt; CREATE TABLE enriched_orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, product_name STRING, product_description STRING, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;elasticsearch-7&#39;, &#39;hosts&#39; = &#39;http://localhost:9200&#39;, &#39;index&#39; = &#39;enriched_orders&#39; ); Enriching orders and load to ElasticSearch # Use Flink SQL to join the order table with the products table to enrich orders and write to the Elasticsearch.
-- Flink SQL Flink SQL&gt; INSERT INTO enriched_orders SELECT o.order_id, o.order_date, o.customer_name, o.price, o.product_id, o.order_status, p.name, p.description FROM orders AS o LEFT JOIN products AS p ON o.product_id = p.id; Now, the enriched orders should be shown in Kibana. Visit http://localhost:5601/app/kibana#/management/kibana/index_pattern to create an index pattern enriched_orders.
Visit http://localhost:5601/app/kibana#/discover to find the enriched orders.
Next, do some change in the databases, and then the enriched orders shown in Kibana will be updated after each step in real time.
Insert a new order in PolarDB-X --PolarDB-X INSERT INTO orders VALUES (default, &#39;2020-07-30 15:22:00&#39;, &#39;Jark&#39;, 29.71, 104, false); Update the order status in PolarDB-X --PolarDB-X UPDATE orders SET order_status = true WHERE order_id = 10004; Delete the order in PolarDB-X --PolarDB-X DELETE FROM orders WHERE order_id = 10004; The changes of enriched orders in Kibana are as follows: Clean up # After finishing the tutorial, run the following command to stop all containers in the directory of docker-compose.yml:
docker-compose down Run the following command to stop the Flink cluster in the directory of Flink flink-1.18.0:
./bin/stop-cluster.sh Back to top
`}),e.add({id:36,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/postgres-cdc/",title:"Postgres",section:"Flink Sources",content:` Postgres CDC Connector # The Postgres CDC connector allows for reading snapshot data and incremental data from PostgreSQL database. This document describes how to setup the Postgres CDC connector to run SQL queries against PostgreSQL databases.
Dependencies # In order to setup the Postgres CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency # &ltdependency&gt &ltgroupId&gtorg.apache.flink&lt/groupId&gt &ltartifactId&gtflink-connector-postgres-cdc&lt/artifactId&gt &ltversion&gt3.2-SNAPSHOT&lt/version&gt &lt/dependency&gt Copied to clipboard! SQL Client JAR # Download link is available only for stable releases.
Download flink-sql-connector-postgres-cdc-3.0.1.jar and put it under &lt;FLINK_HOME&gt;/lib/.
Note: Refer to flink-sql-connector-postgres-cdc, more released versions will be available in the Maven central warehouse.
How to create a Postgres CDC table # The Postgres CDC table can be defined as following:
-- register a PostgreSQL table &#39;shipments&#39; in Flink SQL CREATE TABLE shipments ( shipment_id INT, order_id INT, origin STRING, destination STRING, is_arrived BOOLEAN ) WITH ( &#39;connector&#39; = &#39;postgres-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;5432&#39;, &#39;username&#39; = &#39;postgres&#39;, &#39;password&#39; = &#39;postgres&#39;, &#39;database-name&#39; = &#39;postgres&#39;, &#39;schema-name&#39; = &#39;public&#39;, &#39;table-name&#39; = &#39;shipments&#39;, &#39;slot.name&#39; = &#39;flink&#39;, -- experimental feature: incremental snapshot (default off) &#39;scan.incremental.snapshot.enabled&#39; = &#39;true&#39; ); -- read snapshot and binlogs from shipments table SELECT * FROM shipments; Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'postgres-cdc'. hostname required (none) String IP address or hostname of the PostgreSQL database server. username required (none) String Name of the PostgreSQL database to use when connecting to the PostgreSQL database server. password required (none) String Password to use when connecting to the PostgreSQL database server. database-name required (none) String Database name of the PostgreSQL server to monitor. schema-name required (none) String Schema name of the PostgreSQL database to monitor. table-name required (none) String Table name of the PostgreSQL database to monitor. port optional 5432 Integer Integer port number of the PostgreSQL database server. slot.name required (none) String The name of the PostgreSQL logical decoding slot that was created for streaming changes from a particular plug-in for a particular database/schema. The server uses this slot to stream events to the connector that you are configuring. Slot names must conform to PostgreSQL replication slot naming rules, which state: "Each replication slot has a name, which can contain lower-case letters, numbers, and the underscore character." decoding.plugin.name optional decoderbufs String The name of the Postgres logical decoding plug-in installed on the server. Supported values are decoderbufs, wal2json, wal2json_rds, wal2json_streaming, wal2json_rds_streaming and pgoutput. changelog-mode optional all String The changelog mode used for encoding streaming changes. Supported values are all (which encodes changes as retract stream using all RowKinds) and upsert (which encodes changes as upsert stream that describes idempotent updates on a key). upsert mode can be used for tables with primary keys when replica identity FULL is not an option. Primary keys must be set to use upsert mode. heartbeat.interval.ms optional 30s Duration The interval of sending heartbeat event for tracing the latest available replication slot offsets debezium.* optional (none) String Pass-through Debezium's properties to Debezium Embedded Engine which is used to capture data changes from Postgres server. For example: 'debezium.snapshot.mode' = 'never'. See more about the Debezium's Postgres Connector properties debezium.snapshot.select.statement.overrides optional (none) String If you encounter a situation where there is a large amount of data in the table and you don't need all the historical data. You can try to specify the underlying configuration in debezium to select the data range you want to snapshot. This parameter only affects snapshots and does not affect subsequent data reading consumption. Note: PostgreSQL must use schema name and table name. For example: 'debezium.snapshot.select.statement.overrides' = 'schema.table'. After specifying the above attributes, you must also add the following attributes: debezium.snapshot.select.statement.overrides.[schema].[table] debezium.snapshot.select.statement.overrides.[schema].[table] optional (none) String You can specify SQL statements to limit the data range of snapshot. Note1: Schema and table need to be specified in the SQL statement, and the SQL should conform to the syntax of the data source.Currently. For example: 'debezium.snapshot.select.statement.overrides.schema.table' = 'select * from schema.table where 1 != 1'. Note2: The Flink SQL client submission task does not support functions with single quotation marks in the content. For example: 'debezium.snapshot.select.statement.overrides.schema.table' = 'select * from schema.table where to_char(rq, 'yyyy-MM-dd')'. scan.incremental.snapshot.enabled optional false Boolean Incremental snapshot is a new mechanism to read snapshot of a table. Compared to the old snapshot mechanism, the incremental snapshot has many advantages, including: (1) source can be parallel during snapshot reading, (2) source can perform checkpoints in the chunk granularity during snapshot reading, (3) source doesn't need to acquire global read lock (FLUSH TABLES WITH READ LOCK) before snapshot reading. Please see Incremental Snapshot Readingsection for more detailed information. scan.incremental.close-idle-reader.enabled optional false Boolean Whether to close idle readers at the end of the snapshot phase. The flink version is required to be greater than or equal to 1.14 when 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' is set to true.
If the flink version is greater than or equal to 1.15, the default value of 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' has been changed to true, so it does not need to be explicitly configured 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' = 'true' Note: slot.name is recommended to set for different tables to avoid the potential PSQLException: ERROR: replication slot &quot;flink&quot; is active for PID 974 error. See more here.
Incremental Snapshot Options # The following options is available only when scan.incremental.snapshot.enabled=true:
Option Required Default Type Description scan.incremental.snapshot.chunk.size optional 8096 Integer The chunk size (number of rows) of table snapshot, captured tables are split into multiple chunks when read the snapshot of table. scan.startup.mode optional initial String Optional startup mode for Postgres CDC consumer, valid enumerations are "initial" and "latest-offset". Please see Startup Reading Position section for more detailed information. chunk-meta.group.size optional 1000 Integer The group size of chunk meta, if the meta size exceeds the group size, the meta will be divided into multiple groups. connect.timeout optional 30s Duration The maximum time that the connector should wait after trying to connect to the PostgreSQL database server before timing out. connect.pool.size optional 30 Integer The connection pool size. connect.max-retries optional 3 Integer The max retry times that the connector should retry to build database server connection. scan.snapshot.fetch.size optional 1024 Integer The maximum fetch size for per poll when read table snapshot. scan.incremental.snapshot.chunk.key-column optional (none) String The chunk key of table snapshot, captured tables are split into multiple chunks by a chunk key when read the snapshot of table. By default, the chunk key is the first column of the primary key. This column must be a column of the primary key. chunk-key.even-distribution.factor.lower-bound optional 0.05d Double The lower bound of chunk key distribution factor. The distribution factor is used to determine whether the table is evenly distribution or not. The table chunks would use evenly calculation optimization when the data distribution is even, and the query for splitting would happen when it is uneven. The distribution factor could be calculated by (MAX(id) - MIN(id) + 1) / rowCount. chunk-key.even-distribution.factor.upper-bound optional 1000.0d Double The upper bound of chunk key distribution factor. The distribution factor is used to determine whether the table is evenly distribution or not. The table chunks would use evenly calculation optimization when the data distribution is even, and the query for splitting would happen when it is uneven. The distribution factor could be calculated by (MAX(id) - MIN(id) + 1) / rowCount. Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Key DataType Description table_name STRING NOT NULL Name of the table that contain the row. schema_name STRING NOT NULL Name of the schema that contain the row. database_name STRING NOT NULL Name of the database that contain the row. op_ts TIMESTAMP_LTZ(3) NOT NULL It indicates the time that the change was made in the database. If the record is read from snapshot of the table instead of the change stream, the value is always 0. Limitation # Can&rsquo;t perform checkpoint during scanning snapshot of tables when incremental snapshot is disabled # When scan.incremental.snapshot.enabled=false, we have the following limitation.
During scanning snapshot of database tables, since there is no recoverable position, we can&rsquo;t perform checkpoints. In order to not perform checkpoints, Postgres CDC source will keep the checkpoint waiting to timeout. The timeout checkpoint will be recognized as failed checkpoint, by default, this will trigger a failover for the Flink job. So if the database table is large, it is recommended to add following Flink configurations to avoid failover because of the timeout checkpoints:
execution.checkpointing.interval: 10min execution.checkpointing.tolerable-failed-checkpoints: 100 restart-strategy: fixed-delay restart-strategy.fixed-delay.attempts: 2147483647 The extended CREATE TABLE example demonstrates the syntax for exposing these metadata fields:
CREATE TABLE products ( db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, table_name STRING METADATA FROM &#39;table_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, shipment_id INT, order_id INT, origin STRING, destination STRING, is_arrived BOOLEAN ) WITH ( &#39;connector&#39; = &#39;postgres-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;5432&#39;, &#39;username&#39; = &#39;postgres&#39;, &#39;password&#39; = &#39;postgres&#39;, &#39;database-name&#39; = &#39;postgres&#39;, &#39;schema-name&#39; = &#39;public&#39;, &#39;table-name&#39; = &#39;shipments&#39;, &#39;slot.name&#39; = &#39;flink&#39; ); Features # Incremental Snapshot Reading (Experimental) # Incremental snapshot reading is a new mechanism to read snapshot of a table. Compared to the old snapshot mechanism, the incremental snapshot has many advantages, including:
(1) PostgreSQL CDC Source can be parallel during snapshot reading (2) PostgreSQL CDC Source can perform checkpoints in the chunk granularity during snapshot reading (3) PostgreSQL CDC Source doesn&rsquo;t need to acquire global read lock before snapshot reading During the incremental snapshot reading, the PostgreSQL CDC Source firstly splits snapshot chunks (splits) by primary key of table, and then PostgreSQL CDC Source assigns the chunks to multiple readers to read the data of snapshot chunk.
Exactly-Once Processing # The Postgres CDC connector is a Flink Source connector which will read database snapshot first and then continues to read binlogs with exactly-once processing even failures happen. Please read How the connector works.
DataStream Source # The Postgres CDC connector can also be a DataStream source. There are two modes for the DataStream source:
incremental snapshot based, which allows parallel reading SourceFunction based, which only supports single thread reading Incremental Snapshot based DataStream (Experimental) # import org.apache.flink.cdc.connectors.base.source.jdbc.JdbcIncrementalSource; import org.apache.flink.cdc.connectors.postgres.source.PostgresSourceBuilder; import org.apache.flink.cdc.debezium.DebeziumDeserializationSchema; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; public class PostgresParallelSourceExample { public static void main(String[] args) throws Exception { DebeziumDeserializationSchema&lt;String&gt; deserializer = new JsonDebeziumDeserializationSchema(); JdbcIncrementalSource&lt;String&gt; postgresIncrementalSource = PostgresSourceBuilder.PostgresIncrementalSource.&lt;String&gt;builder() .hostname(&#34;localhost&#34;) .port(5432) .database(&#34;postgres&#34;) .schemaList(&#34;inventory&#34;) .tableList(&#34;inventory.products&#34;) .username(&#34;postgres&#34;) .password(&#34;postgres&#34;) .slotName(&#34;flink&#34;) .decodingPluginName(&#34;decoderbufs&#34;) // use pgoutput for PostgreSQL 10+ .deserializer(deserializer) .includeSchemaChanges(true) // output the schema changes as well .splitSize(2) // the split size of each snapshot split .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(3000); env.fromSource( postgresIncrementalSource, WatermarkStrategy.noWatermarks(), &#34;PostgresParallelSource&#34;) .setParallelism(2) .print(); env.execute(&#34;Output Postgres Snapshot&#34;); } } SourceFunction-based DataStream # import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.cdc.connectors.postgres.PostgreSQLSource; public class PostgreSQLSourceExample { public static void main(String[] args) throws Exception { SourceFunction&lt;String&gt; sourceFunction = PostgreSQLSource.&lt;String&gt;builder() .hostname(&#34;localhost&#34;) .port(5432) .database(&#34;postgres&#34;) // monitor postgres database .schemaList(&#34;inventory&#34;) // monitor inventory schema .tableList(&#34;inventory.products&#34;) // monitor products table .username(&#34;flinkuser&#34;) .password(&#34;flinkpw&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env .addSource(sourceFunction) .print().setParallelism(1); // use parallelism 1 for sink to keep message ordering env.execute(); } } Data Type Mapping # PostgreSQL type Flink SQL type TINYINT SMALLINT
INT2
SMALLSERIAL
SERIAL2 SMALLINT INTEGER
SERIAL INT BIGINT
BIGSERIAL BIGINT DECIMAL(20, 0) BIGINT BIGINT REAL
FLOAT4 FLOAT FLOAT8
DOUBLE PRECISION DOUBLE NUMERIC(p, s)
DECIMAL(p, s) DECIMAL(p, s) BOOLEAN BOOLEAN DATE DATE TIME [(p)] [WITHOUT TIMEZONE] TIME [(p)] [WITHOUT TIMEZONE] TIMESTAMP [(p)] [WITHOUT TIMEZONE] TIMESTAMP [(p)] [WITHOUT TIMEZONE] CHAR(n)
CHARACTER(n)
VARCHAR(n)
CHARACTER VARYING(n)
TEXT STRING BYTEA BYTES Back to top
`}),e.add({id:37,href:"/flink/flink-cdc-docs-master/docs/core-concept/transform/",title:"Transform",section:"Core Concept",content:` Definition # Transform module helps users delete and expand data columns based on the data columns in the table. What&rsquo;s more, it also helps users filter some unnecessary data during the synchronization process.
Parameters # To describe a transform rule, the following parameters can be used:
Parameter Meaning Optional/Required source-table Source table id, supports regular expressions required projection Projection rule, supports syntax similar to the select clause in SQL optional filter Filter rule, supports syntax similar to the where clause in SQL optional primary-keys Sink table primary keys, separated by commas optional partition-keys Sink table partition keys, separated by commas optional table-options used to the configure table creation statement when automatically creating tables optional description Transform rule description optional Multiple rules can be declared in one single pipeline YAML file.
Metadata Fields # Fields definition # There are some hidden columns used to access metadata information. They will only take effect when explicitly referenced in the transform rules.
Field Data Type Description namespace_name String Name of the namespace that contains the row. schema_name String Name of the schema that contains the row. table_name String Name of the table that contains the row. Metadata relationship # Type Namespace SchemaName Table JDBC Catalog Schema Table Debezium Catalog Schema Table MySQL Database - Table Postgres Database Schema Table Oracle - Schema Table Microsoft SQL Server Database Schema Table StarRocks Database - Table Doris Database - Table Functions # Flink CDC uses Calcite to parse expressions and Janino script to evaluate expressions with function call.
Comparison Functions # Function Janino Code Description value1 = value2 valueEquals(value1, value2) Returns TRUE if value1 is equal to value2; returns FALSE if value1 or value2 is NULL. value1 &lt;&gt; value2 !valueEquals(value1, value2) Returns TRUE if value1 is not equal to value2; returns FALSE if value1 or value2 is NULL. value1 &gt; value2 value1 &gt; value2 Returns TRUE if value1 is greater than value2; returns FALSE if value1 or value2 is NULL. value1 &gt;= value2 value1 &gt;= value2 Returns TRUE if value1 is greater than or equal to value2; returns FALSE if value1 or value2 is NULL. value1 &lt; value2 value1 &lt; value2 Returns TRUE if value1 is less than value2; returns FALSE if value1 or value2 is NULL. value1 &lt;= value2 value1 &lt;= value2 Returns TRUE if value1 is less than or equal to value2; returns FALSE if value1 or value2 is NULL. value IS NULL null == value Returns TRUE if value is NULL. value IS NOT NULL null != value Returns TRUE if value is not NULL. value1 BETWEEN value2 AND value3 betweenAsymmetric(value1, value2, value3) Returns TRUE if value1 is greater than or equal to value2 and less than or equal to value3. value1 NOT BETWEEN value2 AND value3 notBetweenAsymmetric(value1, value2, value3) Returns TRUE if value1 is less than value2 or greater than value3. string1 LIKE string2 like(string1, string2) Returns TRUE if string1 matches pattern string2. string1 NOT LIKE string2 notLike(string1, string2) Returns TRUE if string1 does not match pattern string2. value1 IN (value2 [, value3]* ) in(value1, value2 [, value3]*) Returns TRUE if value1 exists in the given list (value2, value3, …). value1 NOT IN (value2 [, value3]* ) notIn(value1, value2 [, value3]*) Returns TRUE if value1 does not exist in the given list (value2, value3, …). Logical Functions # Function Janino Code Description boolean1 OR boolean2 boolean1 || boolean2 Returns TRUE if BOOLEAN1 is TRUE or BOOLEAN2 is TRUE. boolean1 AND boolean2 boolean1 &amp;&amp; boolean2 Returns TRUE if BOOLEAN1 and BOOLEAN2 are both TRUE. NOT boolean !boolean Returns TRUE if boolean is FALSE; returns FALSE if boolean is TRUE. boolean IS FALSE false == boolean Returns TRUE if boolean is FALSE; returns FALSE if boolean is TRUE. boolean IS NOT FALSE true == boolean Returns TRUE if BOOLEAN is TRUE; returns FALSE if BOOLEAN is FALSE. boolean IS TRUE true == boolean Returns TRUE if BOOLEAN is TRUE; returns FALSE if BOOLEAN is FALSE. boolean IS NOT TRUE false == boolean Returns TRUE if boolean is FALSE; returns FALSE if boolean is TRUE. Arithmetic Functions # Function Janino Code Description numeric1 + numeric2 numeric1 + numeric2 Returns NUMERIC1 plus NUMERIC2. numeric1 - numeric2 numeric1 - numeric2 Returns NUMERIC1 minus NUMERIC2. numeric1 * numeric2 numeric1 * numeric2 Returns NUMERIC1 multiplied by NUMERIC2. numeric1 / numeric2 numeric1 / numeric2 Returns NUMERIC1 divided by NUMERIC2. numeric1 % numeric2 numeric1 % numeric2 Returns the remainder (modulus) of numeric1 divided by numeric2. ABS(numeric) abs(numeric) Returns the absolute value of numeric. CEIL(numeric) ceil(numeric) Rounds numeric up, and returns the smallest number that is greater than or equal to numeric. FLOOR(numeric) floor(numeric) Rounds numeric down, and returns the largest number that is less than or equal to numeric. ROUND(numeric, int) round(numeric) Returns a number rounded to INT decimal places for NUMERIC. UUID() uuid() Returns an UUID (Universally Unique Identifier) string (e.g., &ldquo;3d3c68f7-f608-473f-b60c-b0c44ad4cc4e&rdquo;) according to RFC 4122 type 4 (pseudo randomly generated) UUID. String Functions # Function Janino Code Description string1 || string2 concat(string1, string2) Returns the concatenation of STRING1 and STRING2. CHAR_LENGTH(string) charLength(string) Returns the number of characters in STRING. UPPER(string) upper(string) Returns string in uppercase. LOWER(string) lower(string) Returns string in lowercase. TRIM(string1) trim(&lsquo;BOTH&rsquo;,string1) Returns a string that removes whitespaces at both sides. REGEXP_REPLACE(string1, string2, string3) regexpReplace(string1, string2, string3) Returns a string from STRING1 with all the substrings that match a regular expression STRING2 consecutively being replaced with STRING3. E.g., &lsquo;foobar&rsquo;.regexpReplace(&lsquo;oo|ar&rsquo;, &lsquo;&rsquo;) returns &ldquo;fb&rdquo;. SUBSTRING(string FROM integer1 [ FOR integer2 ]) substring(string,integer1,integer2) Returns a substring of STRING starting from position INT1 with length INT2 (to the end by default). CONCAT(string1, string2,…) concat(string1, string2,…) Returns a string that concatenates string1, string2, …. E.g., CONCAT(&lsquo;AA&rsquo;, &lsquo;BB&rsquo;, &lsquo;CC&rsquo;) returns &lsquo;AABBCC&rsquo;. Temporal Functions # Function Janino Code Description LOCALTIME localtime() Returns the current SQL time in the local time zone, the return type is TIME(0). LOCALTIMESTAMP localtimestamp() Returns the current SQL timestamp in local time zone, the return type is TIMESTAMP(3). CURRENT_TIME currentTime() Returns the current SQL time in the local time zone, this is a synonym of LOCAL_TIME. CURRENT_DATE currentDate() Returns the current SQL date in the local time zone. CURRENT_TIMESTAMP currentTimestamp() Returns the current SQL timestamp in the local time zone, the return type is TIMESTAMP_LTZ(3). NOW() now() Returns the current SQL timestamp in the local time zone, this is a synonym of CURRENT_TIMESTAMP. DATE_FORMAT(timestamp, string) dateFormat(timestamp, string) Converts timestamp to a value of string in the format specified by the date format string. The format string is compatible with Java&rsquo;s SimpleDateFormat. TIMESTAMPDIFF(timepointunit, timepoint1, timepoint2) timestampDiff(timepointunit, timepoint1, timepoint2) Returns the (signed) number of timepointunit between timepoint1 and timepoint2. The unit for the interval is given by the first argument, which should be one of the following values: SECOND, MINUTE, HOUR, DAY, MONTH, or YEAR. TO_DATE(string1[, string2]) toDate(string1[, string2]) Converts a date string string1 with format string2 (by default &lsquo;yyyy-MM-dd&rsquo;) to a date. TO_TIMESTAMP(string1[, string2]) toTimestamp(string1[, string2]) Converts date time string string1 with format string2 (by default: &lsquo;yyyy-MM-dd HH:mm:ss&rsquo;) to a timestamp, without time zone. Conditional Functions # Function Janino Code Description CASE value WHEN value1_1 [, value1_2]* THEN RESULT1 (WHEN value2_1 [, value2_2 ]* THEN result_2)* (ELSE result_z) END Nested ternary expression Returns resultX when the first time value is contained in (valueX_1, valueX_2, …). When no value matches, returns result_z if it is provided and returns NULL otherwise. CASE WHEN condition1 THEN result1 (WHEN condition2 THEN result2)* (ELSE result_z) END Nested ternary expression Returns resultX when the first conditionX is met. When no condition is met, returns result_z if it is provided and returns NULL otherwise. COALESCE(value1 [, value2]*) coalesce(Object&hellip; objects) Returns the first argument that is not NULL.If all arguments are NULL, it returns NULL as well. The return type is the least restrictive, common type of all of its arguments. The return type is nullable if all arguments are nullable as well. IF(condition, true_value, false_value) condition ? true_value : false_value Returns the true_value if condition is met, otherwise false_value. E.g., IF(5 &gt; 3, 5, 3) returns 5. Example # Add computed columns # Evaluation expressions can be used to generate new columns. For example, if we want to append two computed columns based on the table web_order in the database mydb, we may define a transform rule as follows:
transform: - source-table: mydb.web_order projection: id, order_id, UPPER(product_name) as product_name, localtimestamp as new_timestamp description: append calculated columns based on source table Reference metadata columns # We may reference metadata column in projection expressions. For example, given a table web_order in the database mydb, we may define a transform rule as follows:
transform: - source-table: mydb.web_order projection: id, order_id, __namespace_name__ || &#39;.&#39; || __schema_name__ || &#39;.&#39; || __table_name__ identifier_name description: access metadata columns from source table Use wildcard character to project all fields # A wildcard character (*) can be used to reference all fields in a table. For example, given two tables web_order and app_order in the database mydb, we may define a transform rule as follows:
transform: - source-table: mydb.web_order projection: \\*, UPPER(product_name) as product_name description: project fields with wildcard character from source table - source-table: mydb.app_order projection: UPPER(product_name) as product_name, * description: project fields with wildcard character from source table Notice: When * character presents at the beginning of expressions, an escaping backslash is required.
Add filter rule # Use reference columns when adding filtering rules to the table web_order in the database mydb, we may define a transform rule as follows:
transform: - source-table: mydb.web_order filter: id &gt; 10 AND order_id &gt; 100 description: filtering rows from source table Computed columns can be used in filtering conditions, too. For example, given a table web_order in the database mydb, we may define a transform rule as follows:
transform: - source-table: mydb.web_order projection: id, order_id, UPPER(province) as new_province filter: new_province = &#39;SHANGHAI&#39; description: filtering rows based on computed columns Reassign primary key # We can reassign the primary key in transform rules. For example, given a table web_order in the database mydb, we may define a transform rule as follows:
transform: - source-table: mydb.web_order projection: id, order_id primary-keys: order_id description: reassign primary key example Composite primary keys are also supported:
transform: - source-table: mydb.web_order projection: id, order_id, UPPER(product_name) as product_name primary-keys: order_id, product_name description: reassign composite primary keys example Reassign partition key # We can reassign the partition key in transform rules. For example, given a table web_order in the database mydb, we may define a transform rule as follows:
transform: - source-table: mydb.web_order projection: id, order_id, UPPER(product_name) as product_name partition-keys: product_name description: reassign partition key example Specify table creation configuration # Extra options can be defined in a transform rule, and will be applied when creating downstream tables. Given a table web_order in the database mydb, we may define a transform rule as follows:
transform: - source-table: mydb.web_order projection: id, order_id, UPPER(product_name) as product_name table-options: comment=web order description: auto creating table options example Tips: The format of table-options is key1=value1,key2=value2.
Classification mapping # Multiple transform rules can be defined to classify input data rows and apply different processings. For example, we may define a transform rule as follows:
transform: - source-table: mydb.web_order projection: id, order_id filter: UPPER(province) = &#39;SHANGHAI&#39; description: classification mapping example - source-table: mydb.web_order projection: order_id as id, id as order_id filter: UPPER(province) = &#39;BEIJING&#39; description: classification mapping example Known limitations # Currently, transform doesn&rsquo;t work with route rules. It will be supported in future versions. Computed columns cannot reference trimmed columns that do not present in final projection results. This will be fixed in future versions. Regular matching of tables with different schemas is not supported. If necessary, multiple rules need to be written. `}),e.add({id:38,href:"/flink/flink-cdc-docs-master/docs/faq/",title:"FAQ",section:"Docs",content:" "}),e.add({id:39,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/mongodb-cdc/",title:"MongoDB",section:"Flink Sources",content:` MongoDB CDC Connector # The MongoDB CDC connector allows for reading snapshot data and incremental data from MongoDB. This document describes how to setup the MongoDB CDC connector to run SQL queries against MongoDB.
Dependencies # In order to setup the MongoDB CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency # &ltdependency&gt &ltgroupId&gtorg.apache.flink&lt/groupId&gt &ltartifactId&gtflink-connector-mongodb-cdc&lt/artifactId&gt &ltversion&gt3.2-SNAPSHOT&lt/version&gt &lt/dependency&gt Copied to clipboard! SQL Client JAR # Download link is available only for stable releases.
Download flink-sql-connector-mongodb-cdc-3.0.1.jar and put it under &lt;FLINK_HOME&gt;/lib/.
Note: Refer to flink-sql-connector-mongodb-cdc, more released versions will be available in the Maven central warehouse.
Setup MongoDB # Availability # MongoDB version
MongoDB version &gt;= 3.6 We use change streams feature (new in version 3.6) to capture change data.
Cluster Deployment
replica sets or sharded clusters is required.
Storage Engine
WiredTiger storage engine is required.
Replica set protocol version
Replica set protocol version 1 (pv1) is required. Starting in version 4.0, MongoDB only supports pv1. pv1 is the default for all new replica sets created with MongoDB 3.2 or later.
Privileges
changeStream and read privileges are required by MongoDB Kafka Connector.
You can use the following example for simple authorization.
For more detailed authorization, please refer to MongoDB Database User Roles.
use admin; db.createRole( { role: &#34;flinkrole&#34;, privileges: [{ // Grant privileges on all non-system collections in all databases resource: { db: &#34;&#34;, collection: &#34;&#34; }, actions: [ &#34;splitVector&#34;, &#34;listDatabases&#34;, &#34;listCollections&#34;, &#34;collStats&#34;, &#34;find&#34;, &#34;changeStream&#34; ] }], roles: [ // Read config.collections and config.chunks // for sharded cluster snapshot splitting. { role: &#39;read&#39;, db: &#39;config&#39; } ] } ); db.createUser( { user: &#39;flinkuser&#39;, pwd: &#39;flinkpw&#39;, roles: [ { role: &#39;flinkrole&#39;, db: &#39;admin&#39; } ] } ); How to create a MongoDB CDC table # The MongoDB CDC table can be defined as following:
-- register a MongoDB table &#39;products&#39; in Flink SQL CREATE TABLE products ( _id STRING, // must be declared name STRING, weight DECIMAL(10,3), tags ARRAY&lt;STRING&gt;, -- array price ROW&lt;amount DECIMAL(10,2), currency STRING&gt;, -- embedded document suppliers ARRAY&lt;ROW&lt;name STRING, address STRING&gt;&gt;, -- embedded documents PRIMARY KEY(_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mongodb-cdc&#39;, &#39;hosts&#39; = &#39;localhost:27017,localhost:27018,localhost:27019&#39;, &#39;username&#39; = &#39;flinkuser&#39;, &#39;password&#39; = &#39;flinkpw&#39;, &#39;database&#39; = &#39;inventory&#39;, &#39;collection&#39; = &#39;products&#39; ); -- read snapshot and change events from products collection SELECT * FROM products; Note that
MongoDB&rsquo;s change event record doesn&rsquo;t have updated before message. So, we can only convert it to Flink&rsquo;s UPSERT changelog stream. An upsert stream requires a unique key, so we must declare _id as primary key. We can&rsquo;t declare other column as primary key, because delete operation does not contain the key and value besides _id and sharding key.
Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be mongodb-cdc. scheme optional mongodb String The protocol connected to MongoDB. eg. mongodb or mongodb+srv. hosts required (none) String The comma-separated list of hostname and port pairs of the MongoDB servers.
eg. localhost:27017,localhost:27018 username optional (none) String Name of the database user to be used when connecting to MongoDB.
This is required only when MongoDB is configured to use authentication. password optional (none) String Password to be used when connecting to MongoDB.
This is required only when MongoDB is configured to use authentication. database optional (none) String Name of the database to watch for changes. If not set then all databases will be captured. The database also supports regular expressions to monitor multiple databases matching the regular expression. collection optional (none) String Name of the collection in the database to watch for changes. If not set then all collections will be captured.
The collection also supports regular expressions to monitor multiple collections matching fully-qualified collection identifiers. connection.options optional (none) String The ampersand-separated connection options of MongoDB. eg. replicaSet=test&connectTimeoutMS=300000 scan.startup.mode optional initial String Optional startup mode for MongoDB CDC consumer, valid enumerations are "initial", "latest-offset" and "timestamp". Please see Startup Reading Position section for more detailed information. scan.startup.timestamp-millis optional (none) Long Timestamp in millis of the start point, only used for 'timestamp' startup mode. copy.existing.queue.size optional 10240 Integer The max size of the queue to use when copying data. batch.size optional 1024 Integer The cursor batch size. poll.max.batch.size optional 1024 Integer Maximum number of change stream documents to include in a single batch when polling for new data. poll.await.time.ms optional 1000 Integer The amount of time to wait before checking for new results on the change stream. heartbeat.interval.ms optional 0 Integer The length of time in milliseconds between sending heartbeat messages. Use 0 to disable. scan.full-changelog optional false Boolean Whether try to generate full-mode changelog based on pre- and post-images in MongoDB. Refer to Full Changelog for more details. Supports MongoDB 6.0 and above only. scan.incremental.snapshot.enabled optional false Boolean Whether enable incremental snapshot. The incremental snapshot feature only supports after MongoDB 4.0. scan.incremental.snapshot.chunk.size.mb optional 64 Integer The chunk size mb of incremental snapshot. scan.incremental.snapshot.chunk.samples optional 20 Integer The samples count per chunk when using sample partition strategy during incremental snapshot. scan.incremental.close-idle-reader.enabled optional false Boolean Whether to close idle readers at the end of the snapshot phase. The flink version is required to be greater than or equal to 1.14 when 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' is set to true.
If the flink version is greater than or equal to 1.15, the default value of 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' has been changed to true, so it does not need to be explicitly configured 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' = 'true' scan.cursor.no-timeout optional true Boolean MongoDB server normally times out idle cursors after an inactivity period (10 minutes) to prevent excess memory use. Set this option to true to prevent that. Only available when parallelism snapshot is enabled. Note: heartbeat.interval.ms is highly recommended setting a proper value larger than 0 if the collection changes slowly. The heartbeat event can push the resumeToken forward to avoid resumeToken being expired when we recover the Flink job from a checkpoint or savepoint.
Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Key DataType Description database_name STRING NOT NULL Name of the database that contain the row. collection_name STRING NOT NULL Name of the collection that contain the row. op_ts TIMESTAMP_LTZ(3) NOT NULL It indicates the time that the change was made in the database. If the record is read from snapshot of the table instead of the change stream, the value is always 0. The extended CREATE TABLE example demonstrates the syntax for exposing these metadata fields:
CREATE TABLE products ( db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, collection_name STRING METADATA FROM &#39;collection_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, _id STRING, // must be declared name STRING, weight DECIMAL(10,3), tags ARRAY&lt;STRING&gt;, -- array price ROW&lt;amount DECIMAL(10,2), currency STRING&gt;, -- embedded document suppliers ARRAY&lt;ROW&lt;name STRING, address STRING&gt;&gt;, -- embedded documents PRIMARY KEY(_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mongodb-cdc&#39;, &#39;hosts&#39; = &#39;localhost:27017,localhost:27018,localhost:27019&#39;, &#39;username&#39; = &#39;flinkuser&#39;, &#39;password&#39; = &#39;flinkpw&#39;, &#39;database&#39; = &#39;inventory&#39;, &#39;collection&#39; = &#39;products&#39; ); Features # Exactly-Once Processing # The MongoDB CDC connector is a Flink Source connector which will read database snapshot first and then continues to read change stream events with exactly-once processing even failures happen.
Startup Reading Position # The config option scan.startup.mode specifies the startup mode for MongoDB CDC consumer. The valid enumerations are:
initial (default): Performs an initial snapshot on the monitored database tables upon first startup, and continue to read the latest oplog. latest-offset: Never to perform snapshot on the monitored database tables upon first startup, just read from the end of the oplog which means only have the changes since the connector was started. timestamp: Skip snapshot phase and start reading oplog events from a specific timestamp. For example in DataStream API:
MongoDBSource.builder() .startupOptions(StartupOptions.latest()) // Start from latest offset .startupOptions(StartupOptions.timestamp(1667232000000L) // Start from timestamp .build() and with SQL:
CREATE TABLE mongodb_source (...) WITH ( &#39;connector&#39; = &#39;mongodb-cdc&#39;, &#39;scan.startup.mode&#39; = &#39;latest-offset&#39;, -- Start from latest offset ... &#39;scan.startup.mode&#39; = &#39;timestamp&#39;, -- Start from timestamp &#39;scan.startup.timestamp-millis&#39; = &#39;1667232000000&#39; -- Timestamp under timestamp startup mode ... ) Change Streams # We integrate the MongoDB&rsquo;s official Kafka Connector to read snapshot or change events from MongoDB and drive it by Debezium&rsquo;s EmbeddedEngine.
Debezium&rsquo;s EmbeddedEngine provides a mechanism for running a single Kafka Connect SourceConnector within an application&rsquo;s process, and it can drive any standard Kafka Connect SourceConnector properly even which is not provided by Debezium.
We choose MongoDB&rsquo;s official Kafka Connector instead of the Debezium&rsquo;s MongoDB Connector because they use a different change data capture mechanism.
For Debezium&rsquo;s MongoDB Connector, it reads the oplog.rs collection of each replica-set&rsquo;s master node. For MongoDB&rsquo;s Kafka Connector, it subscribes Change Stream of MongoDB. MongoDB&rsquo;s oplog.rs collection doesn&rsquo;t keep the changed record&rsquo;s update before state, so it&rsquo;s hard to extract the full document state by a single oplog.rs record and convert it to change log stream accepted by Flink (Insert Only, Upsert, All). Additionally, MongoDB 5 (released in July 2021) has changed the oplog format, so the current Debezium connector cannot be used with it.
Change Stream is a new feature provided by MongoDB 3.6 for replica sets and sharded clusters that allows applications to access real-time data changes without the complexity and risk of tailing the oplog.
Applications can use change streams to subscribe to all data changes on a single collection, a database, or an entire deployment, and immediately react to them.
Lookup Full Document for Update Operations is a feature provided by Change Stream which can configure the change stream to return the most current majority-committed version of the updated document. Because of this feature, we can easily collect the latest full document and convert the change log to Flink&rsquo;s Upsert Changelog Stream.
By the way, Debezium&rsquo;s MongoDB change streams exploration mentioned by DBZ-435 is on roadmap.
If it&rsquo;s done, we can consider integrating two kinds of source connector for users to choose.
DataStream Source # The MongoDB CDC connector can also be a DataStream source. You can create a SourceFunction as the following shows:
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.cdc.connectors.mongodb.MongoDBSource; public class MongoDBSourceExample { public static void main(String[] args) throws Exception { SourceFunction&lt;String&gt; sourceFunction = MongoDBSource.&lt;String&gt;builder() .hosts(&#34;localhost:27017&#34;) .username(&#34;flink&#34;) .password(&#34;flinkpw&#34;) .databaseList(&#34;inventory&#34;) // set captured database, support regex .collectionList(&#34;inventory.products&#34;, &#34;inventory.orders&#34;) //set captured collections, support regex .deserializer(new JsonDebeziumDeserializationSchema()) .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.addSource(sourceFunction) .print().setParallelism(1); // use parallelism 1 for sink to keep message ordering env.execute(); } } The MongoDB CDC incremental connector (after 2.3.0) can be used as the following shows:
import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.cdc.connectors.mongodb.source.MongoDBSource; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; public class MongoDBIncrementalSourceExample { public static void main(String[] args) throws Exception { MongoDBSource&lt;String&gt; mongoSource = MongoDBSource.&lt;String&gt;builder() .hosts(&#34;localhost:27017&#34;) .databaseList(&#34;inventory&#34;) // set captured database, support regex .collectionList(&#34;inventory.products&#34;, &#34;inventory.orders&#34;) //set captured collections, support regex .username(&#34;flink&#34;) .password(&#34;flinkpw&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000); // set the source parallelism to 2 env.fromSource(mongoSource, WatermarkStrategy.noWatermarks(), &#34;MongoDBIncrementalSource&#34;) .setParallelism(2) .print() .setParallelism(1); env.execute(&#34;Print MongoDB Snapshot + Change Stream&#34;); } } Note:
If database regex is used, readAnyDatabase role is required. The incremental snapshot feature only supports after MongoDB 4.0. Full Changelog # MongoDB 6.0 and above supports emitting change stream events containing document before and after the change was made (aka. pre- and post-images).
The pre-image is the document before it was replaced, updated, or deleted. There is no pre-image for an inserted document.
The post-image is the document after it was inserted, replaced, or updated. There is no post-image for a deleted document.
MongoDB CDC could make uses of pre-image and post-images to generate full-mode changelog stream including Insert, Update Before, Update After, and Delete data rows, thereby avoiding additional ChangelogNormalize downstream node.
To enable this feature, here&rsquo;s some prerequisites:
MongoDB version must be 6.0 or above; Enable preAndPostImages feature at the database level: db.runCommand({ setClusterParameter: { changeStreamOptions: { preAndPostImages: { expireAfterSeconds: &#39;off&#39; // replace with custom image expiration time } } } }) Enable changeStreamPreAndPostImages feature for collections to be monitored: db.runCommand({ collMod: &#34;&lt;&lt; collection name &gt;&gt;&#34;, changeStreamPreAndPostImages: { enabled: true } }) Enable MongoDB CDC&rsquo;s scan.full-changelog feature: MongoDBSource.builder() .scanFullChangelog(true) ... .build() or with Flink SQL:
CREATE TABLE mongodb_source (...) WITH ( &#39;connector&#39; = &#39;mongodb-cdc&#39;, &#39;scan.full-changelog&#39; = &#39;true&#39;, ... ) Data Type Mapping # BSON short for Binary JSON is a binary-encoded serialization of JSON-like format used to store documents and make remote procedure calls in MongoDB.
Flink SQL Data Type is similar to the SQL standard’s data type terminology which describes the logical type of a value in the table ecosystem. It can be used to declare input and/or output types of operations.
In order to enable Flink SQL to process data from heterogeneous data sources, the data types of heterogeneous data sources need to be uniformly converted to Flink SQL data types.
The following is the mapping of BSON type and Flink SQL type.
BSON type Flink SQL type TINYINT SMALLINT Int
INT Long BIGINT FLOAT Double DOUBLE Decimal128 DECIMAL(p, s) Boolean BOOLEAN DateTimestamp DATE DateTimestamp TIME Date TIMESTAMP(3)TIMESTAMP_LTZ(3) Timestamp TIMESTAMP(0)TIMESTAMP_LTZ(0) String
ObjectId
UUID
Symbol
MD5
JavaScript Regex STRING BinData BYTES Object ROW Array ARRAY DBPointer ROW&lt;$ref STRING, $id STRING&gt; GeoJSON Point : ROW&lt;type STRING, coordinates ARRAY&lt;DOUBLE&gt;&gt; Line : ROW&lt;type STRING, coordinates ARRAY&lt;ARRAY&lt; DOUBLE&gt;&gt;&gt; ... Reference # MongoDB Kafka Connector Change Streams Replication Sharding Database User Roles WiredTiger Replica set protocol Connection String Options Document Pre- and Post-Images BSON Types Flink DataTypes Back to top
`}),e.add({id:40,href:"/flink/flink-cdc-docs-master/docs/core-concept/route/",title:"Route",section:"Core Concept",content:` Definition # Route specifies the rule of matching a list of source-table and mapping to sink-table. The most typical scenario is the merge of sub-databases and sub-tables, routing multiple upstream source tables to the same sink table.
Parameters # To describe a route, the follows are required:
parameter meaning optional/required source-table Source table id, supports regular expressions required sink-table Sink table id, supports regular expressions required description Routing rule description(a default value provided) optional A route module can contain a list of source-table/sink-table rules.
Example # Route one Data Source table to one Data Sink table # if synchronize the table web_order in the database mydb to a Doris table ods_web_order, we can use this yaml file to define this route：
route: - source-table: mydb.web_order sink-table: mydb.ods_web_order description: sync table to one destination table with given prefix ods_ Route multiple Data Source tables to one Data Sink table # What&rsquo;s more, if you want to synchronize the sharding tables in the database mydb to a Doris table ods_web_order, we can use this yaml file to define this route：
route: - source-table: mydb\\.* sink-table: mydb.ods_web_order description: sync sharding tables to one destination table Complex Route via combining route rules # What&rsquo;s more, if you want to specify many different mapping rules, we can use this yaml file to define this route：
route: - source-table: mydb.orders sink-table: ods_db.ods_orders description: sync orders table to orders - source-table: mydb.shipments sink-table: ods_db.ods_shipments description: sync shipments table to ods_shipments - source-table: mydb.products sink-table: ods_db.ods_products description: sync products table to ods_products `}),e.add({id:41,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/sqlserver-tutorial/",title:"SqlServer Tutorial",section:"Tutorials",content:` Demo: SqlServer CDC to Elasticsearch # Create docker-compose.yml file using following contents:
version: &#39;2.1&#39; services: sqlserver: image: mcr.microsoft.com/mssql/server:2019-latest container_name: sqlserver ports: - &#34;1433:1433&#34; environment: - &#34;MSSQL_AGENT_ENABLED=true&#34; - &#34;MSSQL_PID=Standard&#34; - &#34;ACCEPT_EULA=Y&#34; - &#34;SA_PASSWORD=Password!&#34; elasticsearch: image: elastic/elasticsearch:7.6.0 container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34; - discovery.type=single-node ports: - &#34;9200:9200&#34; - &#34;9300:9300&#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 container_name: kibana ports: - &#34;5601:5601&#34; volumes: - /var/run/docker.sock:/var/run/docker.sock The Docker Compose environment consists of the following containers:
SqlServer: SqlServer database. Elasticsearch: store the join result of the orders and products table. Kibana: mainly used to visualize the data in Elasticsearch. To start all containers, run the following command in the directory that contains the docker-compose.yml file:
docker-compose up -d This command automatically starts all the containers defined in the Docker Compose configuration in a detached mode. Run docker ps to check whether these containers are running properly. You can also visit http://localhost:5601/ to see if Kibana is running normally.
Don’t forget to run the following command to stop and remove all containers after you finished the tutorial:
docker-compose down Download following JAR package to &lt;FLINK_HOME&gt;/lib:
Download links are available only for stable releases, SNAPSHOT dependencies need to be built based on master or release branches by yourself.
flink-sql-connector-elasticsearch7-3.0.1-1.17.jar flink-sql-connector-sqlserver-cdc-3.0-SNAPSHOT.jar Preparing data in SqlServer database
Create databases/tables and populate data
-- Sqlserver CREATE DATABASE inventory; GO USE inventory; EXEC sys.sp_cdc_enable_db; -- Create and populate our products using a single insert with many rows CREATE TABLE products ( id INTEGER IDENTITY(101,1) NOT NULL PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(512), weight FLOAT ); INSERT INTO products(name,description,weight) VALUES (&#39;scooter&#39;,&#39;Small 2-wheel scooter&#39;,3.14); INSERT INTO products(name,description,weight) VALUES (&#39;car battery&#39;,&#39;12V car battery&#39;,8.1); INSERT INTO products(name,description,weight) VALUES (&#39;12-pack drill bits&#39;,&#39;12-pack of drill bits with sizes ranging from #40 to #3&#39;,0.8); INSERT INTO products(name,description,weight) VALUES (&#39;hammer&#39;,&#39;12oz carpenter&#39;&#39;s hammer&#39;,0.75); INSERT INTO products(name,description,weight) VALUES (&#39;hammer&#39;,&#39;14oz carpenter&#39;&#39;s hammer&#39;,0.875); INSERT INTO products(name,description,weight) VALUES (&#39;hammer&#39;,&#39;16oz carpenter&#39;&#39;s hammer&#39;,1.0); INSERT INTO products(name,description,weight) VALUES (&#39;rocks&#39;,&#39;box of assorted rocks&#39;,5.3); INSERT INTO products(name,description,weight) VALUES (&#39;jacket&#39;,&#39;water resistent black wind breaker&#39;,0.1); INSERT INTO products(name,description,weight) VALUES (&#39;spare tire&#39;,&#39;24 inch spare tire&#39;,22.2); EXEC sys.sp_cdc_enable_table @source_schema = &#39;dbo&#39;, @source_name = &#39;products&#39;, @role_name = NULL, @supports_net_changes = 0; -- Create some very simple orders CREATE TABLE orders ( id INTEGER IDENTITY(10001,1) NOT NULL PRIMARY KEY, order_date DATE NOT NULL, purchaser INTEGER NOT NULL, quantity INTEGER NOT NULL, product_id INTEGER NOT NULL, FOREIGN KEY (product_id) REFERENCES products(id) ); INSERT INTO orders(order_date,purchaser,quantity,product_id) VALUES (&#39;16-JAN-2016&#39;, 1001, 1, 102); INSERT INTO orders(order_date,purchaser,quantity,product_id) VALUES (&#39;17-JAN-2016&#39;, 1002, 2, 105); INSERT INTO orders(order_date,purchaser,quantity,product_id) VALUES (&#39;19-FEB-2016&#39;, 1002, 2, 106); INSERT INTO orders(order_date,purchaser,quantity,product_id) VALUES (&#39;21-FEB-2016&#39;, 1003, 1, 107); EXEC sys.sp_cdc_enable_table @source_schema = &#39;dbo&#39;, @source_name = &#39;orders&#39;, @role_name = NULL, @supports_net_changes = 0; GO Launch a Flink cluster and start a Flink SQL CLI:
-- Flink SQL -- checkpoint every 3000 milliseconds Flink SQL&gt; SET execution.checkpointing.interval = 3s; Flink SQL&gt; CREATE TABLE products ( id INT, name STRING, description STRING, PRIMARY KEY (id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;sqlserver-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;1433&#39;, &#39;username&#39; = &#39;sa&#39;, &#39;password&#39; = &#39;Password!&#39;, &#39;database-name&#39; = &#39;inventory&#39;, &#39;table-name&#39; = &#39;dbo.products&#39; ); Flink SQL&gt; CREATE TABLE orders ( id INT, order_date DATE, purchaser INT, quantity INT, product_id INT, PRIMARY KEY (id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;sqlserver-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;1433&#39;, &#39;username&#39; = &#39;sa&#39;, &#39;password&#39; = &#39;Password!&#39;, &#39;database-name&#39; = &#39;inventory&#39;, &#39;table-name&#39; = &#39;dbo.orders&#39; ); Flink SQL&gt; CREATE TABLE enriched_orders ( order_id INT, order_date DATE, purchaser INT, quantity INT, product_name STRING, product_description STRING, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;elasticsearch-7&#39;, &#39;hosts&#39; = &#39;http://localhost:9200&#39;, &#39;index&#39; = &#39;enriched_orders_1&#39; ); Flink SQL&gt; INSERT INTO enriched_orders SELECT o.id,o.order_date,o.purchaser,o.quantity, p.name, p.description FROM orders AS o LEFT JOIN products AS p ON o.product_id = p.id; Check result in Elasticsearch
Check the data has been written to Elasticsearch successfully, you can visit Kibana to see the data.
Make changes in SqlServer and watch result in Elasticsearch
Do some changes in the databases, and then the enriched orders shown in Kibana will be updated after each step in real time.
INSERT INTO orders(order_date,purchaser,quantity,product_id) VALUES (&#39;22-FEB-2016&#39;, 1006, 22, 107); GO UPDATE orders SET quantity = 11 WHERE id = 10001; GO DELETE FROM orders WHERE id = 10004; GO Back to top
`}),e.add({id:42,href:"/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/starrocks/",title:"StarRocks",section:"Pipeline Connectors",content:` StarRocks Connector # StarRocks connector can be used as the Data Sink of the pipeline, and write data to StarRocks. This document describes how to set up the StarRocks connector.
What can the connector do? # Create table automatically if not exist Schema change synchronization Data synchronization Example # The pipeline for reading data from MySQL and sink to StarRocks can be defined as follows:
source: type: mysql name: MySQL Source hostname: 127.0.0.1 port: 3306 username: admin password: pass tables: adb.\\.*, bdb.user_table_[0-9]+, [app|web].order_\\.* server-id: 5401-5404 sink: type: starrocks name: StarRocks Sink jdbc-url: jdbc:mysql://127.0.0.1:9030 load-url: 127.0.0.1:8030 username: root password: pass pipeline: name: MySQL to StarRocks Pipeline parallelism: 2 Connector Options # Option Required Default Type Description type required (none) String Specify what connector to use, here should be 'starrocks'. name optional (none) String The name of the sink. jdbc-url required (none) String The address that is used to connect to the MySQL server of the FE. You can specify multiple addresses, which must be separated by a comma (,). Format: \`jdbc:mysql://fe_host1:fe_query_port1,fe_host2:fe_query_port2,fe_host3:fe_query_port3\`. load-url required (none) String The address that is used to connect to the HTTP server of the FE. You can specify multiple addresses, which must be separated by a semicolon (;). Format: \`fe_host1:fe_http_port1;fe_host2:fe_http_port2\`. username required (none) String User name to use when connecting to the StarRocks database. password required (none) String Password to use when connecting to the StarRocks database. sink.label-prefix optional (none) String The label prefix used by Stream Load. sink.connect.timeout-ms optional 30000 String The timeout for establishing HTTP connection. Valid values: 100 to 60000. sink.wait-for-continue.timeout-ms optional 30000 String Timeout in millisecond to wait for 100-continue response from FE http server. Valid values: 3000 to 600000. sink.buffer-flush.max-bytes optional 157286400 Long The maximum size of data that can be accumulated in memory before being sent to StarRocks at a time. The value ranges from 64 MB to 10 GB. This buffer is shared by all tables in the sink. If the buffer is full, the connector will choose one or more tables to flush. sink.buffer-flush.interval-ms optional 300000 Long The interval at which data is flushed for each table. The unit is in millisecond. sink.scan-frequency.ms optional 50 Long Scan frequency in milliseconds to check whether the buffered data for a table should be flushed because of reaching the flush interval. sink.io.thread-count optional 2 Integer Number of threads used for concurrent stream loads among different tables. sink.at-least-once.use-transaction-stream-load optional true Boolean Whether to use transaction stream load for at-least-once when it's available. sink.properties.* optional (none) String The parameters that control Stream Load behavior. For example, the parameter \`sink.properties.timeout\` specifies the timeout of Stream Load. For a list of supported parameters and their descriptions, see STREAM LOAD. table.create.num-buckets optional (none) Integer Number of buckets when creating a StarRocks table automatically. For StarRocks 2.5 or later, it's not required to set the option because StarRocks can determine the number of buckets automatically. For StarRocks prior to 2.5, you must set this option. table.create.properties.* optional (none) String Properties used for creating a StarRocks table. For example: 'table.create.properties.fast_schema_evolution' = 'true' will enable fast schema evolution if you are using StarRocks 3.2 or later. For more information, see how to create a primary key table. table.schema-change.timeout optional 30min Duration Timeout for a schema change on StarRocks side, and must be an integral multiple of seconds. StarRocks will cancel the schema change after timeout which will cause the sink failure. Usage Notes # Only support StarRocks primary key table, so the source table must have primary keys.
Not support exactly-once. The connector uses at-least-once + primary key table for idempotent writing.
For creating table automatically
the distribution keys are the same as the primary keys there is no partition key the number of buckets is controlled by table.create.num-buckets. If you are using StarRocks 2.5 or later, it&rsquo;s not required to set the option because StarRocks can determine the number of buckets automatically, otherwise you must set the option. For schema change synchronization
only supports add/drop columns the new column will always be added to the last position if your StarRocks version is 3.2 or later, and using the connector to create table automatically, you can set table.create.properties.fast_schema_evolution to true to speed up the schema change. For data synchronization, the pipeline connector uses StarRocks Sink Connector to write data to StarRocks. You can see sink documentation for how it works.
Data Type Mapping # Flink CDC type StarRocks type Note TINYINT TINYINT SMALLINT SMALLINT INT INT BIGINT BIGINT FLOAT FLOAT DOUBLE DOUBLE DECIMAL(p, s) DECIMAL(p, s) BOOLEAN BOOLEAN DATE DATE TIMESTAMP DATETIME TIMESTAMP_LTZ DATETIME CHAR(n) where n <= 85 CHAR(n * 3) CDC defines the length by characters, and StarRocks defines it by bytes. According to UTF-8, one Chinese character is equal to three bytes, so the length for StarRocks is n * 3. Because the max length of StarRocks CHAR is 255, map CDC CHAR to StarRocks CHAR only when the CDC length is no larger than 85. CHAR(n) where n > 85 VARCHAR(n * 3) CDC defines the length by characters, and StarRocks defines it by bytes. According to UTF-8, one Chinese character is equal to three bytes, so the length for StarRocks is n * 3. Because the max length of StarRocks CHAR is 255, map CDC CHAR to StarRocks VARCHAR if the CDC length is larger than 85. VARCHAR(n) VARCHAR(n * 3) CDC defines the length by characters, and StarRocks defines it by bytes. According to UTF-8, one Chinese character is equal to three bytes, so the length for StarRocks is n * 3. Back to top
`}),e.add({id:43,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/db2-cdc/",title:"Db2",section:"Flink Sources",content:` Db2 CDC Connector # The Db2 CDC connector allows for reading snapshot data and incremental data from Db2 database. This document describes how to setup the db2 CDC connector to run SQL queries against Db2 databases.
Supported Databases # Connector Database Driver Db2-cdc Db2: 11.5 Db2 Driver: 11.5.0.0 Dependencies # In order to set up the Db2 CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency # &ltdependency&gt &ltgroupId&gtorg.apache.flink&lt/groupId&gt &ltartifactId&gtflink-connector-db2-cdc&lt/artifactId&gt &ltversion&gt3.2-SNAPSHOT&lt/version&gt &lt/dependency&gt Copied to clipboard! SQL Client JAR # Download flink-sql-connector-db2-cdc-3.0.1.jar and put it under &lt;FLINK_HOME&gt;/lib/.
Note: Refer to flink-sql-connector-db2-cdc, more released versions will be available in the Maven central warehouse.
Since Db2 Connector&rsquo;s IPLA license is incompatible with Flink CDC project, we can&rsquo;t provide Db2 connector in prebuilt connector jar packages. You may need to configure the following dependencies manually.
Dependency Item Description com.ibm.db2.jcc:db2jcc:db2jcc4 Used for connecting to Db2 database. Setup Db2 server # Follow the steps in the Debezium Db2 Connector.
Notes # Not support BOOLEAN type in SQL Replication on Db2 # Only snapshots can be taken from tables with BOOLEAN type columns. Currently, SQL Replication on Db2 does not support BOOLEAN, so Debezium can not perform CDC on those tables. Consider using another type to replace BOOLEAN type.
How to create a Db2 CDC table # The Db2 CDC table can be defined as following:
-- checkpoint every 3 seconds Flink SQL&gt; SET &#39;execution.checkpointing.interval&#39; = &#39;3s&#39;; -- register a Db2 table &#39;products&#39; in Flink SQL Flink SQL&gt; CREATE TABLE products ( ID INT NOT NULL, NAME STRING, DESCRIPTION STRING, WEIGHT DECIMAL(10,3), PRIMARY KEY(ID) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;db2-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;50000&#39;, &#39;username&#39; = &#39;root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;myschema.products&#39;); -- read snapshot and redo logs from products table Flink SQL&gt; SELECT * FROM products; Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'db2-cdc'. hostname required (none) String IP address or hostname of the Db2 database server. username required (none) String Name of the Db2 database to use when connecting to the Db2 database server. password required (none) String Password to use when connecting to the Db2 database server. database-name required (none) String Database name of the Db2 server to monitor. table-name required (none) String Table name of the Db2 database to monitor, e.g.: "db1.table1" port optional 50000 Integer Integer port number of the Db2 database server. scan.startup.mode optional initial String Optional startup mode for Db2 CDC consumer, valid enumerations are "initial" and "latest-offset". Please see Startup Reading Position section for more detailed information. server-time-zone optional (none) String The session time zone in database server, e.g. "Asia/Shanghai". It controls how the TIMESTAMP type in Db2 converted to STRING. See more here. If not set, then ZoneId.systemDefault() is used to determine the server time zone. scan.incremental.snapshot.enabled optional true Boolean Whether enable parallelism snapshot. chunk-meta.group.size optional 1000 Integer The group size of chunk meta, if the meta size exceeds the group size, the meta will be divided into multiple groups. chunk-key.even-distribution.factor.lower-bound optional 0.05d Double The lower bound of chunk key distribution factor. The distribution factor is used to determine whether the table is evenly distribution or not. The table chunks would use evenly calculation optimization when the data distribution is even, and the query for splitting would happen when it is uneven. The distribution factor could be calculated by (MAX(id) - MIN(id) + 1) / rowCount. chunk-key.even-distribution.factor.upper-bound optional 1000.0d Double The upper bound of chunk key distribution factor. The distribution factor is used to determine whether the table is evenly distribution or not. The table chunks would use evenly calculation optimization when the data distribution is even, and the query for splitting would happen when it is uneven. The distribution factor could be calculated by (MAX(id) - MIN(id) + 1) / rowCount. scan.incremental.snapshot.chunk.key-column optional (none) String The chunk key of table snapshot, captured tables are split into multiple chunks by a chunk key when read the snapshot of table. By default, the chunk key is the first column of the primary key. This column must be a column of the primary key. debezium.* optional (none) String Pass-through Debezium's properties to Debezium Embedded Engine which is used to capture data changes from Db2 server. For example: 'debezium.snapshot.mode' = 'never'. See more about the Debezium's Db2 Connector properties scan.incremental.close-idle-reader.enabled optional false Boolean Whether to close idle readers at the end of the snapshot phase. The flink version is required to be greater than or equal to 1.14 when 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' is set to true.
If the flink version is greater than or equal to 1.15, the default value of 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' has been changed to true, so it does not need to be explicitly configured 'execution.checkpointing.checkpoints-after-tasks-finish.enabled' = 'true' Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Key DataType Description table_name STRING NOT NULL Name of the table that contain the row. schema_name STRING NOT NULL Name of the schema that contain the row. database_name STRING NOT NULL Name of the database that contain the row. op_ts TIMESTAMP_LTZ(3) NOT NULL It indicates the time that the change was made in the database. If the record is read from snapshot of the table instead of the change stream, the value is always 0. Features # Startup Reading Position # The config option scan.startup.mode specifies the startup mode for DB2 CDC consumer. The valid enumerations are:
initial (default): Performs an initial snapshot on the monitored database tables upon first startup, and continue to read the latest redo logs. latest-offset: Never to perform snapshot on the monitored database tables upon first startup, just read from the end of the redo logs which means only have the changes since the connector was started. Note: the mechanism of scan.startup.mode option relying on Debezium&rsquo;s snapshot.mode configuration. So please do not using them together. If you speicifying both scan.startup.mode and debezium.snapshot.mode options in the table DDL, it may make scan.startup.mode doesn&rsquo;t work.
DataStream Source # import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; public class Db2SourceExample { public static void main(String[] args) throws Exception { SourceFunction&lt;String&gt; db2Source = Db2Source.&lt;String&gt;builder() .hostname(&#34;yourHostname&#34;) .port(50000) .database(&#34;yourDatabaseName&#34;) // set captured database .tableList(&#34;yourSchemaName.yourTableName&#34;) // set captured table .username(&#34;yourUsername&#34;) .password(&#34;yourPassword&#34;) .deserializer( new JsonDebeziumDeserializationSchema()) // converts SourceRecord to // JSON String .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000); env.addSource(db2Source) .print() .setParallelism(1); // use parallelism 1 for sink to keep message ordering env.execute(&#34;Print Db2 Snapshot + Change Stream&#34;); } } The DB2 CDC incremental connector (after 3.1.0) can be used as the following shows:
import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.cdc.connectors.base.options.StartupOptions; import org.apache.flink.cdc.connectors.db2.source.Db2SourceBuilder; import org.apache.flink.cdc.connectors.db2.source.Db2SourceBuilder.Db2IncrementalSource; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; public class Db2ParallelSourceExample { public static void main(String[] args) throws Exception { Db2IncrementalSource&lt;String&gt; sqlServerSource = new Db2SourceBuilder() .hostname(&#34;localhost&#34;) .port(50000) .databaseList(&#34;TESTDB&#34;) .tableList(&#34;DB2INST1.CUSTOMERS&#34;) .username(&#34;flink&#34;) .password(&#34;flinkpw&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) .startupOptions(StartupOptions.initial()) .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000); // set the source parallelism to 2 env.fromSource(sqlServerSource, WatermarkStrategy.noWatermarks(), &#34;Db2IncrementalSource&#34;) .setParallelism(2) .print() .setParallelism(1); env.execute(&#34;Print DB2 Snapshot + Change Stream&#34;); } } Data Type Mapping # Db2 type Flink SQL type NOTE SMALLINT
SMALLINT INTEGER INT BIGINT BIGINT REAL FLOAT DOUBLE DOUBLE NUMERIC(p, s)
DECIMAL(p, s) DECIMAL(p, s) DATE DATE TIME TIME TIMESTAMP [(p)] TIMESTAMP [(p)] CHARACTER(n) CHAR(n) VARCHAR(n) VARCHAR(n) BINARY(n) BINARY(n) VARBINARY(N) VARBINARY(N) BLOB
CLOB
DBCLOB
BYTES VARGRAPHIC
XML STRING Back to top
`}),e.add({id:44,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/tidb-tutorial/",title:"TiDB Tutorial",section:"Tutorials",content:` Demo: TiDB CDC to Elasticsearch # First,we will start TiDB cluster with docker.
$ git clone https://github.com/pingcap/tidb-docker-compose.git Next,replace docker-compose.yml file using following contents in directory tidb-docker-compose:
version: &#34;2.1&#34; services: pd: image: pingcap/pd:v5.3.1 ports: - &#34;2379:2379&#34; volumes: - ./config/pd.toml:/pd.toml - ./logs:/logs command: - --client-urls=http://0.0.0.0:2379 - --peer-urls=http://0.0.0.0:2380 - --advertise-client-urls=http://pd:2379 - --advertise-peer-urls=http://pd:2380 - --initial-cluster=pd=http://pd:2380 - --data-dir=/data/pd - --config=/pd.toml - --log-file=/logs/pd.log restart: on-failure tikv: image: pingcap/tikv:v5.3.1 ports: - &#34;20160:20160&#34; volumes: - ./config/tikv.toml:/tikv.toml - ./logs:/logs command: - --addr=0.0.0.0:20160 - --advertise-addr=tikv:20160 - --data-dir=/data/tikv - --pd=pd:2379 - --config=/tikv.toml - --log-file=/logs/tikv.log depends_on: - &#34;pd&#34; restart: on-failure tidb: image: pingcap/tidb:v5.3.1 ports: - &#34;4000:4000&#34; volumes: - ./config/tidb.toml:/tidb.toml - ./logs:/logs command: - --store=tikv - --path=pd:2379 - --config=/tidb.toml - --log-file=/logs/tidb.log - --advertise-address=tidb depends_on: - &#34;tikv&#34; restart: on-failure elasticsearch: image: elastic/elasticsearch:7.6.0 container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34; - discovery.type=single-node ports: - &#34;9200:9200&#34; - &#34;9300:9300&#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 container_name: kibana ports: - &#34;5601:5601&#34; volumes: - /var/run/docker.sock:/var/run/docker.sock The Docker Compose environment consists of the following containers:
TiDB cluster: tikv、pd、tidb. Elasticsearch: store the join result of the orders and products table. Kibana: mainly used to visualize the data in Elasticsearch. Add pd and tikv mapping to 127.0.0.1 in host file. To start all containers, run the following command in the directory that contains the docker-compose.yml file:
docker-compose up -d mysql -h 127.0.0.1 -P 4000 -u root # Just test tidb cluster is ready,if you have install mysql local. This command automatically starts all the containers defined in the Docker Compose configuration in a detached mode. Run docker ps to check whether these containers are running properly. You can also visit http://localhost:5601/ to see if Kibana is running normally.
Don’t forget to run the following command to stop and remove all containers after you finished the tutorial:
docker-compose down Download following JAR package to &lt;FLINK_HOME&gt;/lib:
Download links are available only for stable releases, SNAPSHOT dependencies need to be built based on master or release branches by yourself.
flink-sql-connector-elasticsearch7-3.0.1-1.17.jar flink-sql-connector-tidb-cdc-3.0-SNAPSHOT.jar Preparing data in TiDB database
Create databases/tables and populate data
-- TiDB CREATE DATABASE mydb; USE mydb; CREATE TABLE products ( id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(512) ) AUTO_INCREMENT = 101; INSERT INTO products VALUES (default,&#34;scooter&#34;,&#34;Small 2-wheel scooter&#34;), (default,&#34;car battery&#34;,&#34;12V car battery&#34;), (default,&#34;12-pack drill bits&#34;,&#34;12-pack of drill bits with sizes ranging from #40 to #3&#34;), (default,&#34;hammer&#34;,&#34;12oz carpenter&#39;s hammer&#34;), (default,&#34;hammer&#34;,&#34;14oz carpenter&#39;s hammer&#34;), (default,&#34;hammer&#34;,&#34;16oz carpenter&#39;s hammer&#34;), (default,&#34;rocks&#34;,&#34;box of assorted rocks&#34;), (default,&#34;jacket&#34;,&#34;water resistent black wind breaker&#34;), (default,&#34;spare tire&#34;,&#34;24 inch spare tire&#34;); CREATE TABLE orders ( order_id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, order_date DATETIME NOT NULL, customer_name VARCHAR(255) NOT NULL, price DECIMAL(10, 5) NOT NULL, product_id INTEGER NOT NULL, order_status BOOLEAN NOT NULL -- Whether order has been placed ) AUTO_INCREMENT = 10001; INSERT INTO orders VALUES (default, &#39;2020-07-30 10:08:22&#39;, &#39;Jark&#39;, 50.50, 102, false), (default, &#39;2020-07-30 10:11:09&#39;, &#39;Sally&#39;, 15.00, 105, false), (default, &#39;2020-07-30 12:00:30&#39;, &#39;Edward&#39;, 25.25, 106, false); Launch a Flink cluster and start a Flink SQL CLI:
-- Flink SQL -- checkpoint every 3000 milliseconds Flink SQL&gt; SET execution.checkpointing.interval = 3s; Flink SQL&gt; CREATE TABLE products ( id INT, name STRING, description STRING, PRIMARY KEY (id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;tidb-cdc&#39;, &#39;tikv.grpc.timeout_in_ms&#39; = &#39;20000&#39;, &#39;pd-addresses&#39; = &#39;127.0.0.1:2379&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;products&#39; ); Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(3), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;tidb-cdc&#39;, &#39;tikv.grpc.timeout_in_ms&#39; = &#39;20000&#39;, &#39;pd-addresses&#39; = &#39;127.0.0.1:2379&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;orders&#39; ); Flink SQL&gt; CREATE TABLE enriched_orders ( order_id INT, order_date DATE, customer_name STRING, order_status BOOLEAN, product_name STRING, product_description STRING, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;elasticsearch-7&#39;, &#39;hosts&#39; = &#39;http://localhost:9200&#39;, &#39;index&#39; = &#39;enriched_orders_1&#39; ); Flink SQL&gt; INSERT INTO enriched_orders SELECT o.order_id, o.order_date, o.customer_name, o.order_status, p.name, p.description FROM orders AS o LEFT JOIN products AS p ON o.product_id = p.id; Check result in Elasticsearch
Check the data has been written to Elasticsearch successfully, you can visit Kibana to see the data.
Make changes in TiDB and watch result in Elasticsearch
Do some changes in the databases, and then the enriched orders shown in Kibana will be updated after each step in real time.
INSERT INTO orders VALUES (default, &#39;2020-07-30 15:22:00&#39;, &#39;Jark&#39;, 29.71, 104, false); UPDATE orders SET order_status = true WHERE order_id = 10004; DELETE FROM orders WHERE order_id = 10004; Back to top
`}),e.add({id:45,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tidb-cdc/",title:"TiDB",section:"Flink Sources",content:` TiDB CDC Connector # The TiDB CDC connector allows for reading snapshot data and incremental data from TiDB database. This document describes how to setup the TiDB CDC connector to run SQL queries against TiDB databases.
Dependencies # In order to setup the TiDB CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency # &ltdependency&gt &ltgroupId&gtorg.apache.flink&lt/groupId&gt &ltartifactId&gtflink-connector-tidb-cdc&lt/artifactId&gt &ltversion&gt3.2-SNAPSHOT&lt/version&gt &lt/dependency&gt Copied to clipboard! SQL Client JAR # Download link is available only for stable releases.
Download flink-sql-connector-tidb-cdc-3.0.1.jar and put it under &lt;FLINK_HOME&gt;/lib/.
Note: Refer to flink-sql-connector-tidb-cdc, more released versions will be available in the Maven central warehouse.
How to create a TiDB CDC table # The TiDB CDC table can be defined as following:
-- checkpoint every 3000 milliseconds Flink SQL&gt; SET &#39;execution.checkpointing.interval&#39; = &#39;3s&#39;; -- register a TiDB table &#39;orders&#39; in Flink SQL Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(3), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY(order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;tidb-cdc&#39;, &#39;tikv.grpc.timeout_in_ms&#39; = &#39;20000&#39;, &#39;pd-addresses&#39; = &#39;localhost:2379&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;orders&#39; ); -- read snapshot and binlogs from orders table Flink SQL&gt; SELECT * FROM orders; Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'tidb-cdc'. database-name required (none) String Database name of the TiDB server to monitor. table-name required (none) String Table name of the TiDB database to monitor. scan.startup.mode optional initial String Optional startup mode for TiDB CDC consumer, valid enumerations are "initial" and "latest-offset". pd-addresses required (none) String TiKV cluster's PD address. host-mapping optional (none) String TiKV cluster's host-mapping used to configure public IP and intranet IP mapping. When the TiKV cluster is running on the intranet, you can map a set of intranet IPs to public IPs for an outside Flink cluster to access. The format is {Intranet IP1}:{Public IP1};{Intranet IP2}:{Public IP2}, e.g. 192.168.0.2:8.8.8.8;192.168.0.3:9.9.9.9. tikv.grpc.timeout_in_ms optional (none) Long TiKV GRPC timeout in ms. tikv.grpc.scan_timeout_in_ms optional (none) Long TiKV GRPC scan timeout in ms. tikv.batch_get_concurrency optional 20 Integer TiKV GRPC batch get concurrency. tikv.* optional (none) String Pass-through TiDB client's properties. Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Key DataType Description table_name STRING NOT NULL Name of the table that contain the row. database_name STRING NOT NULL Name of the database that contain the row. op_ts TIMESTAMP_LTZ(3) NOT NULL It indicates the time that the change was made in the database. If the record is read from snapshot of the table instead of the binlog, the value is always 0. The extended CREATE TABLE example demonstrates the syntax for exposing these metadata fields:
CREATE TABLE products ( db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, table_name STRING METADATA FROM &#39;table_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY(order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;tidb-cdc&#39;, &#39;tikv.grpc.timeout_in_ms&#39; = &#39;20000&#39;, &#39;pd-addresses&#39; = &#39;localhost:2379&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;orders&#39; ); Features # Exactly-Once Processing # The TiDB CDC connector is a Flink Source connector which will read database snapshot first and then continues to read change events with exactly-once processing even failures happen.
Startup Reading Position # The config option scan.startup.mode specifies the startup mode for TiDB CDC consumer. The valid enumerations are:
initial (default): Takes a snapshot of structure and data of captured tables; useful if you want fetch a complete representation of the data from the captured tables. latest-offset: Takes a snapshot of the structure of captured tables only; useful if only changes happening from now onwards should be fetched. Multi Thread Reading # The TiDB CDC source can work in parallel reading, because there is multiple tasks can receive change events.
DataStream Source # The TiDB CDC connector can also be a DataStream source. You can create a SourceFunction as the following shows:
DataStream Source # import org.apache.flink.api.common.typeinfo.BasicTypeInfo; import org.apache.flink.api.common.typeinfo.TypeInformation; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.util.Collector; import org.apache.flink.cdc.connectors.tidb.TDBSourceOptions; import org.apache.flink.cdc.connectors.tidb.TiDBSource; import org.apache.flink.cdc.connectors.tidb.TiKVChangeEventDeserializationSchema; import org.apache.flink.cdc.connectors.tidb.TiKVSnapshotEventDeserializationSchema; import org.tikv.kvproto.Cdcpb; import org.tikv.kvproto.Kvrpcpb; import java.util.HashMap; public class TiDBSourceExample { public static void main(String[] args) throws Exception { SourceFunction&lt;String&gt; tidbSource = TiDBSource.&lt;String&gt;builder() .database(&#34;mydb&#34;) // set captured database .tableName(&#34;products&#34;) // set captured table .tiConf( TDBSourceOptions.getTiConfiguration( &#34;localhost:2399&#34;, new HashMap&lt;&gt;())) .snapshotEventDeserializer( new TiKVSnapshotEventDeserializationSchema&lt;String&gt;() { @Override public void deserialize( Kvrpcpb.KvPair record, Collector&lt;String&gt; out) throws Exception { out.collect(record.toString()); } @Override public TypeInformation&lt;String&gt; getProducedType() { return BasicTypeInfo.STRING_TYPE_INFO; } }) .changeEventDeserializer( new TiKVChangeEventDeserializationSchema&lt;String&gt;() { @Override public void deserialize( Cdcpb.Event.Row record, Collector&lt;String&gt; out) throws Exception { out.collect(record.toString()); } @Override public TypeInformation&lt;String&gt; getProducedType() { return BasicTypeInfo.STRING_TYPE_INFO; } }) .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000); env.addSource(tidbSource).print().setParallelism(1); env.execute(&#34;Print TiDB Snapshot + Binlog&#34;); } } Data Type Mapping # TiDB type Flink SQL type NOTE TINYINT TINYINT SMALLINT
TINYINT UNSIGNED SMALLINT INT
MEDIUMINT
SMALLINT UNSIGNED INT BIGINT
INT UNSIGNED BIGINT BIGINT UNSIGNED DECIMAL(20, 0) FLOAT
FLOAT REAL
DOUBLE DOUBLE NUMERIC(p, s)
DECIMAL(p, s)
where p <= 38
DECIMAL(p, s) NUMERIC(p, s)
DECIMAL(p, s)
where 38 < p <= 65
STRING The precision for DECIMAL data type is up to 65 in TiDB, but the precision for DECIMAL is limited to 38 in Flink. So if you define a decimal column whose precision is greater than 38, you should map it to STRING to avoid precision loss. BOOLEAN
TINYINT(1)
BIT(1) BOOLEAN DATE DATE TIME [(p)] TIME [(p)] TIMESTAMP [(p)] TIMESTAMP_LTZ [(p)] DATETIME [(p)] TIMESTAMP [(p)] CHAR(n) CHAR(n) VARCHAR(n) VARCHAR(n) BIT(n) BINARY(⌈n/8⌉) BINARY(n) BINARY(n) TINYTEXT
TEXT
MEDIUMTEXT
LONGTEXT
STRING TINYBLOB
BLOB
MEDIUMBLOB
LONGBLOB
BYTES Currently, for BLOB data type in TiDB, only the blob whose length isn't greater than 2,147,483,647(2 ** 31 - 1) is supported. YEAR INT ENUM STRING JSON STRING The JSON data type will be converted into STRING with JSON format in Flink. SET ARRAY&lt;STRING&gt; As the SET data type in TiDB is a string object that can have zero or more values, it should always be mapped to an array of string Back to top
`}),e.add({id:46,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/oceanbase-cdc/",title:"OceanBase",section:"Flink Sources",content:` OceanBase CDC Connector # The OceanBase CDC connector allows for reading snapshot data and incremental data from OceanBase. This document describes how to set up the OceanBase CDC connector to run SQL queries against OceanBase.
OceanBase CDC Solutions # Glossary:
OceanBase CE: OceanBase Community Edition. It&rsquo;s compatible with MySQL and has been open sourced at https://github.com/oceanbase/oceanbase. OceanBase EE: OceanBase Enterprise Edition. It supports two compatibility modes: MySQL and Oracle. See https://en.oceanbase.com. OceanBase Cloud: OceanBase Enterprise Edition on Cloud. See https://en.oceanbase.com/product/cloud. Log Proxy CE: OceanBase Log Proxy Community Edition (CDC mode). It&rsquo;s a proxy service which can fetch the commit log data of OceanBase CE. It has been open sourced at https://github.com/oceanbase/oblogproxy. Log Proxy EE: OceanBase Log Proxy Enterprise Edition (CDC mode). It&rsquo;s a proxy service which can fetch the commit log data of OceanBase EE. Limited support is available on OceanBase Cloud only, you can contact the provider support for more details. Binlog Service CE: OceanBase Binlog Service Community Edition. It is a solution of OceanBase CE that is compatible with the MySQL replication protocol. See the docs of Log Proxy CE (Binlog mode) for details. Binlog Service EE: OceanBase Binlog Service Enterprise Edition. It is a solution of OceanBase EE MySQL mode that is compatible with the MySQL replication protocol, and it&rsquo;s only available for users of Alibaba Cloud, see User Guide. MySQL Driver: mysql-connector-java which can be used with OceanBase CE and OceanBase EE MySQL mode. OceanBase Driver: The Jdbc driver for OceanBase, which supports both MySQL mode and Oracle mode of all OceanBase versions. It&rsquo;s open sourced at https://github.com/oceanbase/obconnector-j. CDC Source Solutions for OceanBase:
Database Supported Driver CDC Source Connector Other Required Components OceanBase CE MySQL Driver: 5.1.4x, 8.0.x OceanBase Driver: 2.4.x OceanBase CDC Connector Log Proxy CE MySQL Driver: 8.0.x MySQL CDC Connector Binlog Service CE OceanBase EE (MySQL Mode) MySQL Driver: 5.1.4x, 8.0.x OceanBase Driver: 2.4.x OceanBase CDC Connector Log Proxy EE MySQL Driver: 8.0.x MySQL CDC Connector Binlog Service EE OceanBase EE (Oracle Mode) OceanBase Driver: 2.4.x OceanBase CDC Connector Log Proxy EE (CDC Mode) Note: For users of OceanBase CE or OceanBase EE MySQL Mode, we recommend that you follow the MySQL CDC documentation to use the MySQL CDC source connector with the Binlog service.
Dependencies # In order to set up the OceanBase CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency # &ltdependency&gt &ltgroupId&gtorg.apache.flink&lt/groupId&gt &ltartifactId&gtflink-connector-oceanbase-cdc&lt/artifactId&gt &ltversion&gt3.2-SNAPSHOT&lt/version&gt &lt/dependency&gt Copied to clipboard! SQL Client JAR # Download link is available only for stable releases.
Download flink-sql-connector-oceanbase-cdc-3.0.1.jar and put it under &lt;FLINK_HOME&gt;/lib/.
Note: Refer to flink-sql-connector-oceanbase-cdc, more released versions will be available in the Maven central warehouse.
Since the licenses of MySQL Driver and OceanBase Driver are incompatible with Flink CDC project, we can&rsquo;t provide them in prebuilt connector jar packages. You may need to configure the following dependencies manually.
Dependency Item Description mysql:mysql-connector-java:8.0.27 Used for connecting to MySQL tenant of OceanBase. com.oceanbase:oceanbase-client:2.4.9 Used for connecting to MySQL or Oracle tenant of OceanBase. Setup OceanBase and LogProxy Server # Set up the OceanBase cluster following the doc.
Create a user with password in sys tenant, this user is used in OceanBase LogProxy.
mysql -h\${host} -P\${port} -uroot mysql&gt; SHOW TENANT; mysql&gt; CREATE USER \${sys_username} IDENTIFIED BY &#39;\${sys_password}&#39;; mysql&gt; GRANT ALL PRIVILEGES ON *.* TO \${sys_username} WITH GRANT OPTION; Create a user in the tenant you want to monitor, this is used to read data for snapshot and change event.
For users of OceanBase Community Edition, you need to get the rootserver-list. You can use the following command to get the value:
mysql&gt; show parameters like &#39;rootservice_list&#39;; For users of OceanBase Enterprise Edition, you need to get the config-url. You can use the following command to get the value:
mysql&gt; show parameters like &#39;obconfig_url&#39;; Setup OceanBase LogProxy. For users of OceanBase Community Edition, you can follow the docs (Chinese).
How to create a OceanBase CDC table # The OceanBase CDC table can be defined as following:
-- checkpoint every 3000 milliseconds Flink SQL&gt; SET &#39;execution.checkpointing.interval&#39; = &#39;3s&#39;; -- register a OceanBase table &#39;orders&#39; in Flink SQL Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;oceanbase-cdc&#39;, &#39;scan.startup.mode&#39; = &#39;initial&#39;, &#39;username&#39; = &#39;user@test_tenant#cluster_name&#39;, &#39;password&#39; = &#39;pswd&#39;, &#39;tenant-name&#39; = &#39;test_tenant&#39;, &#39;database-name&#39; = &#39;^test_db$&#39;, &#39;table-name&#39; = &#39;^orders$&#39;, &#39;hostname&#39; = &#39;127.0.0.1&#39;, &#39;port&#39; = &#39;2881&#39;, &#39;rootserver-list&#39; = &#39;127.0.0.1:2882:2881&#39;, &#39;logproxy.host&#39; = &#39;127.0.0.1&#39;, &#39;logproxy.port&#39; = &#39;2983&#39;, &#39;working-mode&#39; = &#39;memory&#39; ); -- read snapshot and binlogs from orders table Flink SQL&gt; SELECT * FROM orders; If you want to use OceanBase Oracle mode, you need to add the OceanBase jdbc jar file to Flink and set up the enterprise edition of oblogproxy, then you can create a table in Flink as following:
Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;oceanbase-cdc&#39;, &#39;scan.startup.mode&#39; = &#39;initial&#39;, &#39;username&#39; = &#39;user@test_tenant#cluster_name&#39;, &#39;password&#39; = &#39;pswd&#39;, &#39;tenant-name&#39; = &#39;test_tenant&#39;, &#39;database-name&#39; = &#39;^test_db$&#39;, &#39;table-name&#39; = &#39;^orders$&#39;, &#39;hostname&#39; = &#39;127.0.0.1&#39;, &#39;port&#39; = &#39;2881&#39;, &#39;compatible-mode&#39; = &#39;oracle&#39;, &#39;jdbc.driver&#39; = &#39;com.oceanbase.jdbc.Driver&#39;, &#39;config-url&#39; = &#39;http://127.0.0.1:8080/services?Action=ObRootServiceInfo&amp;User_ID=xxx&amp;UID=xxx&amp;ObRegion=xxx&#39;, &#39;logproxy.host&#39; = &#39;127.0.0.1&#39;, &#39;logproxy.port&#39; = &#39;2983&#39;, &#39;working-mode&#39; = &#39;memory&#39; ); You can also try the quickstart tutorial that sync data from OceanBase to Elasticsearch, please refer Flink CDC Tutorial for more information.
Connector Options # The OceanBase CDC Connector contains some options for both sql and stream api as the following sheet.
Note: The connector supports two ways to specify the table list to listen to, and will get the union of the results when both way are used at the same time.
Use database-name and table-name to match database and table names in regex. Use table-list to match the exact value of database and table names. Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'oceanbase-cdc'. scan.startup.mode optional initial String Specify the startup mode for OceanBase CDC consumer, valid enumerations are 'initial','latest-offset','timestamp' or 'snapshot'. scan.startup.timestamp optional (none) Long Timestamp in seconds of the start point, only used for 'timestamp' startup mode. username required (none) String Username to be used when connecting to OceanBase. password required (none) String Password to be used when connecting to OceanBase. tenant-name optional (none) String Tenant name of OceanBase to monitor, should be exact value. Required when 'scan.startup.mode' is not 'snapshot'. database-name optional (none) String Database name of OceanBase to monitor, should be regular expression. table-name optional (none) String Table name of OceanBase to monitor, should be regular expression. table-list optional (none) String List of full names of tables, separated by commas, e.g. "db1.table1, db2.table2". hostname required (none) String IP address or hostname of the OceanBase database server or OceanBase Proxy server. port required (none) Integer Integer port number to connect to OceanBase. It can be the SQL port of OceanBase server, which is 2881 by default, or the port of OceanBase proxy service, which is 2883 by default. connect.timeout optional 30s Duration The maximum time that the connector should wait after trying to connect to the OceanBase database server before timing out. server-time-zone optional +00:00 String The session timezone which controls how temporal types are converted to STRING in OceanBase. Can be UTC offset in format "±hh:mm", or named time zones if the time zone information tables in the mysql database have been created and populated. logproxy.host optional (none) String Hostname or IP address of OceanBase log proxy service. Required when 'scan.startup.mode' is not 'snapshot'. logproxy.port optional (none) Integer Port number of OceanBase log proxy service. Required when 'scan.startup.mode' is not 'snapshot'. logproxy.client.id optional By rule. String Id of a log proxy client connection, will be in format {flink_ip}_{process_id}_{timestamp}_{thread_id}_{tenant} by default. rootserver-list optional (none) String The semicolon-separated list of OceanBase root servers in format \`ip:rpc_port:sql_port\`, required for OceanBase CE. config-url optional (none) String The url to get the server info from the config server, required for OceanBase EE. working-mode optional storage String Working mode of \`obcdc\` in LogProxy, can be \`storage\` or \`memory\`. compatible-mode optional mysql String Compatible mode of OceanBase, can be \`mysql\` or \`oracle\`. jdbc.driver optional com.mysql.cj.jdbc.Driver String JDBC driver class for snapshot reading. jdbc.properties.* optional (none) String Option to pass custom JDBC URL properties. User can pass custom properties like 'jdbc.properties.useSSL' = 'false'. obcdc.properties.* optional (none) String Option to pass custom configurations to the libobcdc, eg: 'obcdc.properties.sort_trans_participants' = '1'. Please refer to obcdc parameters for more details. Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Key DataType Description tenant_name STRING Name of the tenant that contains the row. database_name STRING Name of the database that contains the row. schema_name STRING Name of the schema that contains the row. table_name STRING NOT NULL Name of the table that contains the row. op_ts TIMESTAMP_LTZ(3) NOT NULL It indicates the time that the change was made in the database. If the record is read from snapshot of the table instead of the change stream, the value is always 0. The extended CREATE TABLE example demonstrates the syntax for exposing these metadata fields:
CREATE TABLE products ( tenant_name STRING METADATA FROM &#39;tenant_name&#39; VIRTUAL, db_name STRING METADATA FROM &#39;database_name&#39; VIRTUAL, table_name STRING METADATA FROM &#39;table_name&#39; VIRTUAL, operation_ts TIMESTAMP_LTZ(3) METADATA FROM &#39;op_ts&#39; VIRTUAL, order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY(order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;oceanbase-cdc&#39;, &#39;scan.startup.mode&#39; = &#39;initial&#39;, &#39;username&#39; = &#39;user@test_tenant&#39;, &#39;password&#39; = &#39;pswd&#39;, &#39;tenant-name&#39; = &#39;test_tenant&#39;, &#39;database-name&#39; = &#39;^test_db$&#39;, &#39;table-name&#39; = &#39;^orders$&#39;, &#39;hostname&#39; = &#39;127.0.0.1&#39;, &#39;port&#39; = &#39;2881&#39;, &#39;rootserver-list&#39; = &#39;127.0.0.1:2882:2881&#39;, &#39;logproxy.host&#39; = &#39;127.0.0.1&#39;, &#39;logproxy.port&#39; = &#39;2983&#39;, &#39;working-mode&#39; = &#39;memory&#39; ); Features # At-Least-Once Processing # The OceanBase CDC connector is a Flink Source connector which will read database snapshot first and then continues to read change events with at-least-once processing.
OceanBase is a kind of distributed database whose log files are distributed on different servers. As there is no position information like MySQL binlog offset, we can only use timestamp as the position mark. In order to ensure the completeness of reading data, liboblog (a C++ library to read OceanBase log record) might read some log data before the given timestamp. So in this way we may read duplicate data whose timestamp is around the start point, and only &lsquo;at-least-once&rsquo; can be guaranteed.
Startup Reading Position # The config option scan.startup.mode specifies the startup mode for OceanBase CDC consumer. The valid enumerations are:
initial: Performs an initial snapshot on the monitored table upon first startup, and continue to read the latest commit log. latest-offset: Never to perform snapshot on the monitored table upon first startup and just read the latest commit log since the connector is started. timestamp: Never to perform snapshot on the monitored table upon first startup and just read the commit log from the given scan.startup.timestamp. snapshot: Only perform snapshot on the monitored table. Consume Commit Log # The OceanBase CDC Connector using oblogclient to consume commit log from OceanBase LogProxy.
DataStream Source # The OceanBase CDC connector can also be a DataStream source. You can create a SourceFunction as the following shows:
import org.apache.flink.cdc.connectors.base.options.StartupOptions; import org.apache.flink.cdc.connectors.oceanbase.OceanBaseSource; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; public class OceanBaseSourceExample { public static void main(String[] args) throws Exception { SourceFunction&lt;String&gt; oceanBaseSource = OceanBaseSource.&lt;String&gt;builder() .startupOptions(StartupOptions.initial()) .hostname(&#34;127.0.0.1&#34;) .port(2881) .username(&#34;user@test_tenant&#34;) .password(&#34;pswd&#34;) .compatibleMode(&#34;mysql&#34;) .jdbcDriver(&#34;com.mysql.cj.jdbc.Driver&#34;) .tenantName(&#34;test_tenant&#34;) .databaseName(&#34;^test_db$&#34;) .tableName(&#34;^test_table$&#34;) .logProxyHost(&#34;127.0.0.1&#34;) .logProxyPort(2983) .rsList(&#34;127.0.0.1:2882:2881&#34;) .serverTimeZone(&#34;+08:00&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000); env.addSource(oceanBaseSource).print().setParallelism(1); env.execute(&#34;Print OceanBase Snapshot + Change Events&#34;); } } Data Type Mapping # Mysql Mode # OceanBase type Flink SQL type NOTE BOOLEAN
TINYINT(1)
BIT(1) BOOLEAN TINYINT TINYINT SMALLINT
TINYINT UNSIGNED SMALLINT INT
MEDIUMINT
SMALLINT UNSIGNED INT BIGINT
INT UNSIGNED BIGINT BIGINT UNSIGNED DECIMAL(20, 0) REAL
FLOAT
FLOAT DOUBLE DOUBLE NUMERIC(p, s)
DECIMAL(p, s)
where p <= 38
DECIMAL(p, s) NUMERIC(p, s)
DECIMAL(p, s)
where 38 < p <=65
STRING DECIMAL is equivalent to NUMERIC. The precision for DECIMAL data type is up to 65 in OceanBase, but the precision for DECIMAL is limited to 38 in Flink. So if you define a decimal column whose precision is greater than 38, you should map it to STRING to avoid precision loss. DATE DATE TIME [(p)] TIME [(p)] DATETIME [(p)] TIMESTAMP [(p)] TIMESTAMP [(p)] TIMESTAMP_LTZ [(p)] CHAR(n) CHAR(n) VARCHAR(n) VARCHAR(n) BIT(n) BINARY(⌈(n + 7) / 8⌉) BINARY(n) BINARY(n) VARBINARY(N) VARBINARY(N) TINYTEXT
TEXT
MEDIUMTEXT
LONGTEXT
STRING TINYBLOB
BLOB
MEDIUMBLOB
LONGBLOB
BYTES YEAR INT ENUM STRING SET ARRAY&lt;STRING&gt; As the SET data type in OceanBase is a string object that can have zero or more values, it should always be mapped to an array of string JSON STRING The JSON data type will be converted into STRING with JSON format in Flink. Oracle Mode # OceanBase type Flink SQL type NOTE NUMBER(1) BOOLEAN NUMBER(p, s <= 0), p - s < 3 TINYINT NUMBER(p, s <= 0), p - s < 5 SMALLINT NUMBER(p, s <= 0), p - s < 10 INT NUMBER(p, s <= 0), p - s < 19 BIGINT NUMBER(p, s <= 0), 19 <=p - s <=38 DECIMAL(p - s, 0) NUMBER(p, s > 0) DECIMAL(p, s) NUMBER(p, s <= 0), p - s> 38 STRING FLOAT
BINARY_FLOAT FLOAT BINARY_DOUBLE DOUBLE DATE
TIMESTAMP [(p)] TIMESTAMP [(p)] CHAR(n)
NCHAR(n)
VARCHAR(n)
VARCHAR2(n)
NVARCHAR2(n)
CLOB
STRING RAW
BLOB
ROWID BYTES Back to top
`}),e.add({id:47,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/vitess-cdc/",title:"Vitess",section:"Flink Sources",content:` Vitess CDC Connector # The Vitess CDC connector allows for reading of incremental data from Vitess cluster. The connector does not support snapshot feature at the moment. This document describes how to setup the Vitess CDC connector to run SQL queries against Vitess databases. Vitess debezium documentation
Dependencies # In order to setup the Vitess CDC connector, the following table provides dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency # &ltdependency&gt &ltgroupId&gtorg.apache.flink&lt/groupId&gt &ltartifactId&gtflink-connector-vitess-cdc&lt/artifactId&gt &ltversion&gt3.2-SNAPSHOT&lt/version&gt &lt/dependency&gt Copied to clipboard! SQL Client JAR # Download flink-sql-connector-vitess-cdc-3.0.1.jar and put it under &lt;FLINK_HOME&gt;/lib/.
Note: Refer to flink-sql-connector-vitess-cdc, more released versions will be available in the Maven central warehouse.
Setup Vitess server # You can follow the Local Install via Docker guide, or the Vitess Operator for Kubernetes guide to install Vitess. No special setup is needed to support Vitess connector.
Checklist # Make sure that the VTGate host and its gRPC port (default is 15991) is accessible from the machine where the Vitess connector is installed gRPC authentication # Because Vitess connector reads change events from the VTGate VStream gRPC server, it does not need to connect directly to MySQL instances. Therefore, no special database user and permissions are needed. At the moment, Vitess connector only supports unauthenticated access to the VTGate gRPC server.
How to create a Vitess CDC table # The Vitess CDC table can be defined as following:
-- checkpoint every 3000 milliseconds Flink SQL&gt; SET &#39;execution.checkpointing.interval&#39; = &#39;3s&#39;; -- register a Vitess table &#39;orders&#39; in Flink SQL Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY(order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;vitess-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;keyspace&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;orders&#39;); -- read snapshot and binlogs from orders table Flink SQL&gt; SELECT * FROM orders; Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be &lsquo;vitess-cdc&rsquo;. hostname required (none) String IP address or hostname of the Vitess database server (VTGate). keyspace required (none) String The name of the keyspace from which to stream the changes. username optional (none) String An optional username of the Vitess database server (VTGate). If not configured, unauthenticated VTGate gRPC is used. password optional (none) String An optional password of the Vitess database server (VTGate). If not configured, unauthenticated VTGate gRPC is used. shard optional (none) String An optional name of the shard from which to stream the changes. If not configured, in case of unsharded keyspace, the connector streams changes from the only shard, in case of sharded keyspace, the connector streams changes from all shards in the keyspace. gtid optional current String An optional GTID position for a shard to stream from. stopOnReshard optional false Boolean Controls Vitess flag stop_on_reshard. tombstonesOnDelete optional true Boolean Controls whether a delete event is followed by a tombstone event. tombstonesOnDelete optional true Boolean Controls whether a delete event is followed by a tombstone event. schemaNameAdjustmentMode optional avro String Specifies how schema names should be adjusted for compatibility with the message converter used by the connector. table-name required (none) String Table name of the MySQL database to monitor. tablet.type optional RDONLY String The type of Tablet (hence MySQL) from which to stream the changes: MASTER represents streaming from the master MySQL instance REPLICA represents streaming from the replica slave MySQL instance RDONLY represents streaming from the read-only slave MySQL instance. Features # Incremental Reading # The Vitess connector spends all its time streaming changes from the VTGate’s VStream gRPC service to which it is subscribed. The client receives changes from VStream as they are committed in the underlying MySQL server’s binlog at certain positions, which are referred to as VGTID.
The VGTID in Vitess is the equivalent of GTID in MySQL, it describes the position in the VStream in which a change event happens. Typically, A VGTID has multiple shard GTIDs, each shard GTID is a tuple of (Keyspace, Shard, GTID), which describes the GTID position of a given shard.
When subscribing to a VStream service, the connector needs to provide a VGTID and a Tablet Type (e.g. MASTER, REPLICA). The VGTID describes the position from which VStream should starts sending change events; the Tablet type describes which underlying MySQL instance (master or replica) in each shard do we read change events from.
The first time the connector connects to a Vitess cluster, it gets and provides the current VGTID to VStream.
The Debezium Vitess connector acts as a gRPC client of VStream. When the connector receives changes it transforms the events into Debezium create, update, or delete events that include the VGTID of the event. The Vitess connector forwards these change events in records to the Kafka Connect framework, which is running in the same process. The Kafka Connect process asynchronously writes the change event records in the same order in which they were generated to the appropriate Kafka topic.
Checkpoint # Incremental snapshot reading provides the ability to perform checkpoint in chunk level. It resolves the checkpoint timeout problem in previous version with old snapshot reading mechanism.
Exactly-Once Processing # The Vitess CDC connector is a Flink Source connector which will read table snapshot chunks first and then continues to read binlog, both snapshot phase and binlog phase, Vitess CDC connector read with exactly-once processing even failures happen.
DataStream Source # The Incremental Reading feature of Vitess CDC Source only exposes in SQL currently, if you&rsquo;re using DataStream, please use Vitess Source:
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.cdc.connectors.vitess.VitessSource; public class VitessSourceExample { public static void main(String[] args) throws Exception { SourceFunction&lt;String&gt; sourceFunction = VitessSource.&lt;String&gt;builder() .hostname(&#34;localhost&#34;) .port(15991) .keyspace(&#34;inventory&#34;) .username(&#34;flinkuser&#34;) .password(&#34;flinkpw&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env .addSource(sourceFunction) .print().setParallelism(1); // use parallelism 1 for sink to keep message ordering env.execute(); } } Data Type Mapping # MySQL type Flink SQL type TINYINT TINYINT SMALLINT
TINYINT UNSIGNED SMALLINT INT
MEDIUMINT
SMALLINT UNSIGNED INT BIGINT
INT UNSIGNED BIGINT BIGINT UNSIGNED DECIMAL(20, 0) BIGINT BIGINT FLOAT FLOAT DOUBLE
DOUBLE PRECISION DOUBLE NUMERIC(p, s)
DECIMAL(p, s) DECIMAL(p, s) BOOLEAN
TINYINT(1) BOOLEAN CHAR(n)
VARCHAR(n)
TEXT STRING Back to top
`}),e.add({id:48,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/build-real-time-data-lake-tutorial/",title:"Building a Real-time Data Lake with Flink CDC",section:"Tutorials",content:` Building a Real-time Data Lake with Flink CDC # For OLTP databases, to deal with a huge number of data in a single table, we usually do database and table sharding to get better throughput. But sometimes, for convenient analysis, we need to merge them into one table when loading them to data warehouse or data lake.
This tutorial will show how to use Flink CDC to build a real-time data lake for such a scenario. You can walk through the tutorial easily in the docker environment. The entire process uses standard SQL syntax without a single line of Java/Scala code or IDE installation.
The following sections will take the pipeline from MySQL to Iceberg as an example. The overview of the architecture is as follows:
You can also use other data sources like Oracle/Postgres and sinks like Hudi to build your own pipeline.
Preparation # Prepare a Linux or MacOS computer with Docker installed.
Preparing JAR package required # Download links are available only for stable releases, SNAPSHOT dependencies need to be built based on master or release-branches by yourself.
flink-sql-connector-mysql-cdc-3.0-SNAPSHOT.jar flink-shaded-hadoop-2-uber-2.7.5-10.0.jar iceberg-flink-runtime-1.16-1.3.1.jar Starting components required # The components required in this tutorial are all managed in containers, so we will use docker-compose to start them.
Create Dockerfile file using following contents:
FROM flink:1.16.0-scala_2.12 # Place the downloaded jar packages in the lib directory at the same level. COPY ./lib /opt/flink/lib RUN apt-get update &amp;&amp; apt-get install tree Create docker-compose.yml file using following contents:
version: &#39;2.1&#39; services: sql-client: user: flink:flink build: . command: bin/sql-client.sh depends_on: - jobmanager - mysql environment: - MYSQL_HOST=mysql - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager rest.address: jobmanager volumes: - shared-tmpfs:/tmp/iceberg jobmanager: user: flink:flink build: . ports: - &#34;8081:8081&#34; command: jobmanager environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager volumes: - shared-tmpfs:/tmp/iceberg taskmanager: user: flink:flink build: . depends_on: - jobmanager command: taskmanager environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager taskmanager.numberOfTaskSlots: 2 volumes: - shared-tmpfs:/tmp/iceberg mysql: image: debezium/example-mysql:1.1 ports: - &#34;3306:3306&#34; environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_USER=mysqluser - MYSQL_PASSWORD=mysqlpw volumes: shared-tmpfs: driver: local driver_opts: type: &#34;tmpfs&#34; device: &#34;tmpfs&#34; The Docker Compose environment consists of the following containers:
SQL-Client: Flink SQL Client, used to submit queries and visualize their results. Flink Cluster: a Flink JobManager and a Flink TaskManager container to execute queries. MySQL: mainly used as a data source to store the sharding table. To start all containers, run the following command in the directory that contains the docker-compose.yml file:
docker-compose up -d This command automatically starts all the containers defined in the Docker Compose configuration in a detached mode. Run docker ps to check whether these containers are running properly. We can also visit http://localhost:8081/ to see if Flink is running normally.
Note:
If you want to run with your own Flink environment, remember to download the jar packages and then put them to FLINK_HOME/lib/. All the following commands involving docker-compose should be executed in the directory of the docker-compose.yml file. Preparing data in databases # Enter mysql&rsquo;s container:
docker-compose exec mysql mysql -uroot -p123456 Create databases/tables and populate data:
Create a logical sharding table user sharded in different databases and tables physically.
CREATE DATABASE db_1; USE db_1; CREATE TABLE user_1 ( id INTEGER NOT NULL PRIMARY KEY, name VARCHAR(255) NOT NULL DEFAULT &#39;flink&#39;, address VARCHAR(1024), phone_number VARCHAR(512), email VARCHAR(255) ); INSERT INTO user_1 VALUES (110,&#34;user_110&#34;,&#34;Shanghai&#34;,&#34;123567891234&#34;,&#34;user_110@foo.com&#34;); CREATE TABLE user_2 ( id INTEGER NOT NULL PRIMARY KEY, name VARCHAR(255) NOT NULL DEFAULT &#39;flink&#39;, address VARCHAR(1024), phone_number VARCHAR(512), email VARCHAR(255) ); INSERT INTO user_2 VALUES (120,&#34;user_120&#34;,&#34;Shanghai&#34;,&#34;123567891234&#34;,&#34;user_120@foo.com&#34;); CREATE DATABASE db_2; USE db_2; CREATE TABLE user_1 ( id INTEGER NOT NULL PRIMARY KEY, name VARCHAR(255) NOT NULL DEFAULT &#39;flink&#39;, address VARCHAR(1024), phone_number VARCHAR(512), email VARCHAR(255) ); INSERT INTO user_1 VALUES (110,&#34;user_110&#34;,&#34;Shanghai&#34;,&#34;123567891234&#34;, NULL); CREATE TABLE user_2 ( id INTEGER NOT NULL PRIMARY KEY, name VARCHAR(255) NOT NULL DEFAULT &#39;flink&#39;, address VARCHAR(1024), phone_number VARCHAR(512), email VARCHAR(255) ); INSERT INTO user_2 VALUES (220,&#34;user_220&#34;,&#34;Shanghai&#34;,&#34;123567891234&#34;,&#34;user_220@foo.com&#34;); Creating tables using Flink DDL in Flink SQL CLI # First, use the following command to enter the Flink SQL CLI Container:
docker-compose run sql-client We should see the welcome screen of the CLI client:
Then do the following steps in Flink SQL CLI:
Enable checkpoints every 3 seconds
Checkpoint is disabled by default, we need to enable it to commit Iceberg transactions. Besides, the beginning of mysql-cdc binlog phase also requires waiting a complete checkpoint to avoid disorder of binlog records.
-- Flink SQL Flink SQL&gt; SET execution.checkpointing.interval = 3s; Create MySQL sharding source table
Create a source table that captures the data from the logical sharding table user. Here, we use regex to match all the physical tables. Besides, the table defines metadata column to identify which database/table the record comes from.
-- Flink SQL Flink SQL&gt; CREATE TABLE user_source ( database_name STRING METADATA VIRTUAL, table_name STRING METADATA VIRTUAL, \`id\` DECIMAL(20, 0) NOT NULL, name STRING, address STRING, phone_number STRING, email STRING, PRIMARY KEY (\`id\`) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;mysql&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;username&#39; = &#39;root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;db_[0-9]+&#39;, &#39;table-name&#39; = &#39;user_[0-9]+&#39; ); Create Iceberg sink table
Create a sink table all_users_sink used to load data to Iceberg. We define database_name, table_name and id as a combined primary key, because id maybe not unique across different databases and tables.
-- Flink SQL Flink SQL&gt; CREATE TABLE all_users_sink ( database_name STRING, table_name STRING, \`id\` DECIMAL(20, 0) NOT NULL, name STRING, address STRING, phone_number STRING, email STRING, PRIMARY KEY (database_name, table_name, \`id\`) NOT ENFORCED ) WITH ( &#39;connector&#39;=&#39;iceberg&#39;, &#39;catalog-name&#39;=&#39;iceberg_catalog&#39;, &#39;catalog-type&#39;=&#39;hadoop&#39;, &#39;warehouse&#39;=&#39;file:///tmp/iceberg/warehouse&#39;, &#39;format-version&#39;=&#39;2&#39; ); Streaming to Iceberg # Streaming write data from MySQL to Iceberg using the following Flink SQL:
-- Flink SQL Flink SQL&gt; INSERT INTO all_users_sink select * from user_source; It will start a streaming job which will synchronize historical and incremental data from MySQL to Iceberg continuously. The running job can be found in Flink UI, and it looks like:
Then, we can use the following command to see the files written to Iceberg:
docker-compose exec sql-client tree /tmp/iceberg/warehouse/default_database/ It should look like:
The actual files may differ in your environment, but the structure of the directory should be similar.
Use the following Flink SQL to query the data written to all_users_sink:
-- Flink SQL Flink SQL&gt; SELECT * FROM all_users_sink; We can see the data queried in the Flink SQL CLI:
Make some changes in the MySQL databases, and then the data in Iceberg table all_users_sink will also change in real time.
(3.1) Insert a new user in table db_1.user_1
--- db_1 INSERT INTO db_1.user_1 VALUES (111,&#34;user_111&#34;,&#34;Shanghai&#34;,&#34;123567891234&#34;,&#34;user_111@foo.com&#34;); (3.2) Update a user in table db_1.user_2
--- db_1 UPDATE db_1.user_2 SET address=&#39;Beijing&#39; WHERE id=120; (3.3) Delete a user in table db_2.user_2
--- db_2 DELETE FROM db_2.user_2 WHERE id=220; After executing each step, we can query the table all_users_sink using SELECT * FROM all_users_sink in Flink SQL CLI to see the changes.
The final query result is as follows:
From the latest result in Iceberg, we can see that there is a new record of (db_1, user_1, 111), and the address of (db_1, user_2, 120) has been updated to Beijing. Besides, the record of (db_2, user_2, 220) has been deleted. The result is exactly the same with the changes we did in MySQL.
Clean up # After finishing the tutorial, run the following command in the directory of docker-compose.yml to stop all containers:
docker-compose down Back to top
`}),e.add({id:49,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/datastream-api-package-guidance/",title:"DataStream API Package Guidance",section:"Flink Sources",content:" DataStream API Package Guidance # This guide provides a simple pom.xml example for packaging DataStream job JARs with MySQL CDC source.\nExample for pom.xml # &lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt; &lt;project xmlns=&#34;http://maven.apache.org/POM/4.0.0&#34; xmlns:xsi=&#34;http://www.w3.org/2001/XMLSchema-instance&#34; xsi:schemaLocation=&#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&#34;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;FlinkCDCTest&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;scala.binary.version&gt;2.12&lt;/scala.binary.version&gt; &lt;maven.compiler.source&gt;${java.version}&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;${java.version}&lt;/maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;!-- Enforce single fork execution due to heavy mini cluster use in the tests --&gt; &lt;flink.forkCount&gt;1&lt;/flink.forkCount&gt; &lt;flink.reuseForks&gt;true&lt;/flink.reuseForks&gt; &lt;!-- dependencies versions --&gt; &lt;flink.version&gt;1.17.2&lt;/flink.version&gt; &lt;slf4j.version&gt;1.7.15&lt;/slf4j.version&gt; &lt;log4j.version&gt;2.17.1&lt;/log4j.version&gt; &lt;debezium.version&gt;1.9.7.Final&lt;/debezium.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner_${scala.binary.version}&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-runtime&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-core&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Checked the dependencies of the Flink project and below is a feasible reference. --&gt; &lt;!-- Use flink shaded guava 18.0-13.0 for flink 1.13 --&gt; &lt;!-- Use flink shaded guava 30.1.1-jre-14.0 for flink-1.14 --&gt; &lt;!-- Use flink shaded guava 30.1.1-jre-15.0 for flink-1.15 --&gt; &lt;!-- Use flink shaded guava 30.1.1-jre-15.0 for flink-1.16 --&gt; &lt;!-- Use flink shaded guava 30.1.1-jre-16.1 for flink-1.17 --&gt; &lt;!-- Use flink shaded guava 31.1-jre-17.0 for flink-1.18 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-shaded-guava&lt;/artifactId&gt; &lt;version&gt;30.1.1-jre-16.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-mysql-cdc&lt;/artifactId&gt; &lt;version&gt;2.4.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.debezium&lt;/groupId&gt; &lt;artifactId&gt;debezium-connector-mysql&lt;/artifactId&gt; &lt;version&gt;${debezium.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;shade-flink&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;!-- Shading test jar have bug in some previous version, so close this configuration here, see https://issues.apache.org/jira/browse/MSHADE-284 --&gt; &lt;shadeTestJar&gt;false&lt;/shadeTestJar&gt; &lt;shadedArtifactAttached&gt;false&lt;/shadedArtifactAttached&gt; &lt;createDependencyReducedPom&gt;true&lt;/createDependencyReducedPom&gt; &lt;dependencyReducedPomLocation&gt; ${project.basedir}/target/dependency-reduced-pom.xml &lt;/dependencyReducedPomLocation&gt; &lt;filters combine.children=&#34;append&#34;&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;module-info.class&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;artifactSet&gt; &lt;includes&gt; &lt;!-- include nothing --&gt; &lt;include&gt;io.debezium:debezium-api&lt;/include&gt; &lt;include&gt;io.debezium:debezium-embedded&lt;/include&gt; &lt;include&gt;io.debezium:debezium-core&lt;/include&gt; &lt;include&gt;io.debezium:debezium-ddl-parser&lt;/include&gt; &lt;include&gt;io.debezium:debezium-connector-mysql&lt;/include&gt; &lt;include&gt;org.apache.flink:flink-connector-debezium&lt;/include&gt; &lt;include&gt;org.apache.flink:flink-connector-mysql-cdc&lt;/include&gt; &lt;include&gt;org.antlr:antlr4-runtime&lt;/include&gt; &lt;include&gt;org.apache.kafka:*&lt;/include&gt; &lt;include&gt;mysql:mysql-connector-java&lt;/include&gt; &lt;include&gt;com.zendesk:mysql-binlog-connector-java&lt;/include&gt; &lt;include&gt;com.fasterxml.*:*&lt;/include&gt; &lt;include&gt;com.google.guava:*&lt;/include&gt; &lt;include&gt;com.esri.geometry:esri-geometry-api&lt;/include&gt; &lt;include&gt;com.zaxxer:HikariCP&lt;/include&gt; &lt;!-- Include fixed version 30.1.1-jre-16.0 of flink shaded guava --&gt; &lt;include&gt;org.apache.flink:flink-shaded-guava&lt;/include&gt; &lt;/includes&gt; &lt;/artifactSet&gt; &lt;relocations&gt; &lt;relocation&gt; &lt;pattern&gt;org.apache.kafka&lt;/pattern&gt; &lt;shadedPattern&gt; org.apache.flink.cdc.connectors.shaded.org.apache.kafka &lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;relocation&gt; &lt;pattern&gt;org.antlr&lt;/pattern&gt; &lt;shadedPattern&gt; org.apache.flink.cdc.connectors.shaded.org.antlr &lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;relocation&gt; &lt;pattern&gt;com.fasterxml&lt;/pattern&gt; &lt;shadedPattern&gt; org.apache.flink.cdc.connectors.shaded.com.fasterxml &lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;relocation&gt; &lt;pattern&gt;com.google&lt;/pattern&gt; &lt;shadedPattern&gt; org.apache.flink.cdc.connectors.shaded.com.google &lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;relocation&gt; &lt;pattern&gt;com.esri.geometry&lt;/pattern&gt; &lt;shadedPattern&gt;org.apache.flink.cdc.connectors.shaded.com.esri.geometry&lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;relocation&gt; &lt;pattern&gt;com.zaxxer&lt;/pattern&gt; &lt;shadedPattern&gt; org.apache.flink.cdc.connectors.shaded.com.zaxxer &lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;/relocations&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; Example for Code # package org.apache.flink.flink.cdc; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.cdc.connectors.mysql.source.MySqlSource; import org.apache.flink.cdc.debezium.JsonDebeziumDeserializationSchema; public class CdcTest { public static void main(String[] args) throws Exception { MySqlSource&lt;String&gt; mySqlSource = MySqlSource.&lt;String&gt;builder() .hostname(&#34;yourHostname&#34;) .port(yourPort) .databaseList(&#34;yourDatabaseName&#34;) // set captured database, If you need to synchronize the whole database, Please set tableList to &#34;.*&#34;. .tableList(&#34;yourDatabaseName.yourTableName&#34;) // set captured table .username(&#34;yourUsername&#34;) .password(&#34;yourPassword&#34;) .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String .build(); final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enable checkpoint env.enableCheckpointing(3000); env .fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), &#34;MySQL Source&#34;) // set 1 parallel source tasks .setParallelism(1) .print().setParallelism(1); // use parallelism 1 for sink env.execute(&#34;Print MySQL Snapshot + Binlog&#34;); } } Back to top\n"}),e.add({id:50,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/build-streaming-etl-tutorial/",title:"Building a Streaming ETL with Flink CDC",section:"Tutorials",content:` Building a Streaming ETL with Flink CDC # This tutorial is to show how to quickly build streaming ETL for MySQL and Postgres with Flink CDC.
Assuming we are running an e-commerce business. The product and order data stored in MySQL, the shipment data related to the order is stored in Postgres. We want to enrich the orders using the product and shipment table, and then load the enriched orders to ElasticSearch in real time.
In the following sections, we will describe how to use Flink Mysql/Postgres CDC to implement it. All exercises in this tutorial are performed in the Flink SQL CLI, and the entire process uses standard SQL syntax, without a single line of Java/Scala code or IDE installation.
The overview of the architecture is as follows: Preparation # Prepare a Linux or MacOS computer with Docker installed.
Starting components required # The components required in this demo are all managed in containers, so we will use docker-compose to start them.
Create docker-compose.yml file using following contents:
version: &#39;2.1&#39; services: postgres: image: debezium/example-postgres:1.1 ports: - &#34;5432:5432&#34; environment: - POSTGRES_DB=postgres - POSTGRES_USER=postgres - POSTGRES_PASSWORD=postgres mysql: image: debezium/example-mysql:1.1 ports: - &#34;3306:3306&#34; environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_USER=mysqluser - MYSQL_PASSWORD=mysqlpw elasticsearch: image: elastic/elasticsearch:7.6.0 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - &#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34; - discovery.type=single-node ports: - &#34;9200:9200&#34; - &#34;9300:9300&#34; ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 kibana: image: elastic/kibana:7.6.0 ports: - &#34;5601:5601&#34; The Docker Compose environment consists of the following containers:
MySQL: the products,orders tables will be store in the database. They will be joined with data in Postgres to enrich the orders. Postgres: the shipments table will be store in the database. Elasticsearch: mainly used as a data sink to store enriched orders. Kibana: used to visualize the data in Elasticsearch. To start all containers, run the following command in the directory that contains the docker-compose.yml file.
docker-compose up -d This command automatically starts all the containers defined in the Docker Compose configuration in a detached mode. Run docker ps to check whether these containers are running properly. We can also visit http://localhost:5601/ to see if Kibana is running normally.
Preparing Flink and JAR package required # Download Flink 1.18.0 and unzip it to the directory flink-1.18.0
Download following JAR package required and put them under flink-1.18.0/lib/:
Download links are available only for stable releases, SNAPSHOT dependencies need to be built based on master or release branches by yourself.
flink-sql-connector-elasticsearch7-3.0.1-1.17.jar flink-sql-connector-mysql-cdc-3.0-SNAPSHOT.jar flink-sql-connector-postgres-cdc-3.0-SNAPSHOT.jar Preparing data in databases # Preparing data in MySQL # Enter mysql&rsquo;s container: docker-compose exec mysql mysql -uroot -p123456 Create tables and populate data: -- MySQL CREATE DATABASE mydb; USE mydb; CREATE TABLE products ( id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(512) ); ALTER TABLE products AUTO_INCREMENT = 101; INSERT INTO products VALUES (default,&#34;scooter&#34;,&#34;Small 2-wheel scooter&#34;), (default,&#34;car battery&#34;,&#34;12V car battery&#34;), (default,&#34;12-pack drill bits&#34;,&#34;12-pack of drill bits with sizes ranging from #40 to #3&#34;), (default,&#34;hammer&#34;,&#34;12oz carpenter&#39;s hammer&#34;), (default,&#34;hammer&#34;,&#34;14oz carpenter&#39;s hammer&#34;), (default,&#34;hammer&#34;,&#34;16oz carpenter&#39;s hammer&#34;), (default,&#34;rocks&#34;,&#34;box of assorted rocks&#34;), (default,&#34;jacket&#34;,&#34;water resistent black wind breaker&#34;), (default,&#34;spare tire&#34;,&#34;24 inch spare tire&#34;); CREATE TABLE orders ( order_id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, order_date DATETIME NOT NULL, customer_name VARCHAR(255) NOT NULL, price DECIMAL(10, 5) NOT NULL, product_id INTEGER NOT NULL, order_status BOOLEAN NOT NULL -- Whether order has been placed ) AUTO_INCREMENT = 10001; INSERT INTO orders VALUES (default, &#39;2020-07-30 10:08:22&#39;, &#39;Jark&#39;, 50.50, 102, false), (default, &#39;2020-07-30 10:11:09&#39;, &#39;Sally&#39;, 15.00, 105, false), (default, &#39;2020-07-30 12:00:30&#39;, &#39;Edward&#39;, 25.25, 106, false); Preparing data in Postgres # Enter postgres&rsquo;s container: docker-compose exec postgres psql -h localhost -U postgres Create tables and populate data -- PG CREATE TABLE shipments ( shipment_id SERIAL NOT NULL PRIMARY KEY, order_id SERIAL NOT NULL, origin VARCHAR(255) NOT NULL, destination VARCHAR(255) NOT NULL, is_arrived BOOLEAN NOT NULL ); ALTER SEQUENCE public.shipments_shipment_id_seq RESTART WITH 1001; ALTER TABLE public.shipments REPLICA IDENTITY FULL; INSERT INTO shipments VALUES (default,10001,&#39;Beijing&#39;,&#39;Shanghai&#39;,false), (default,10002,&#39;Hangzhou&#39;,&#39;Shanghai&#39;,false), (default,10003,&#39;Shanghai&#39;,&#39;Hangzhou&#39;,false); Starting Flink cluster and Flink SQL CLI # Use the following command to change to the Flink directory:
cd flink-1.18.0 Use the following command to start a Flink cluster:
./bin/start-cluster.sh Then we can visit http://localhost:8081/ to see if Flink is running normally, and the web page looks like:
Use the following command to start a Flink SQL CLI:
./bin/sql-client.sh We should see the welcome screen of the CLI client.
Creating tables using Flink DDL in Flink SQL CLI # First, enable checkpoints every 3 seconds
-- Flink SQL Flink SQL&gt; SET execution.checkpointing.interval = 3s; Then, create tables that capture the change data from the corresponding database tables.
-- Flink SQL Flink SQL&gt; CREATE TABLE products ( id INT, name STRING, description STRING, PRIMARY KEY (id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;username&#39; = &#39;root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;products&#39; ); Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;mysql-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;3306&#39;, &#39;username&#39; = &#39;root&#39;, &#39;password&#39; = &#39;123456&#39;, &#39;database-name&#39; = &#39;mydb&#39;, &#39;table-name&#39; = &#39;orders&#39; ); Flink SQL&gt; CREATE TABLE shipments ( shipment_id INT, order_id INT, origin STRING, destination STRING, is_arrived BOOLEAN, PRIMARY KEY (shipment_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;postgres-cdc&#39;, &#39;hostname&#39; = &#39;localhost&#39;, &#39;port&#39; = &#39;5432&#39;, &#39;username&#39; = &#39;postgres&#39;, &#39;password&#39; = &#39;postgres&#39;, &#39;database-name&#39; = &#39;postgres&#39;, &#39;schema-name&#39; = &#39;public&#39;, &#39;table-name&#39; = &#39;shipments&#39;, &#39;slot.name&#39; = &#39;flink&#39; ); Finally, create enriched_orders table that is used to load data to the Elasticsearch.
-- Flink SQL Flink SQL&gt; CREATE TABLE enriched_orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, product_name STRING, product_description STRING, shipment_id INT, origin STRING, destination STRING, is_arrived BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;connector&#39; = &#39;elasticsearch-7&#39;, &#39;hosts&#39; = &#39;http://localhost:9200&#39;, &#39;index&#39; = &#39;enriched_orders&#39; ); Enriching orders and load to ElasticSearch # Use Flink SQL to join the order table with the products and shipments table to enrich orders and write to the Elasticsearch.
-- Flink SQL Flink SQL&gt; INSERT INTO enriched_orders SELECT o.*, p.name, p.description, s.shipment_id, s.origin, s.destination, s.is_arrived FROM orders AS o LEFT JOIN products AS p ON o.product_id = p.id LEFT JOIN shipments AS s ON o.order_id = s.order_id; Now, the enriched orders should be shown in Kibana. Visit http://localhost:5601/app/kibana#/management/kibana/index_pattern to create an index pattern enriched_orders.
Visit http://localhost:5601/app/kibana#/discover to find the enriched orders.
Next, do some change in the databases, and then the enriched orders shown in Kibana will be updated after each step in real time.
Insert a new order in MySQL --MySQL INSERT INTO orders VALUES (default, &#39;2020-07-30 15:22:00&#39;, &#39;Jark&#39;, 29.71, 104, false); Insert a shipment in Postgres --PG INSERT INTO shipments VALUES (default,10004,&#39;Shanghai&#39;,&#39;Beijing&#39;,false); Update the order status in MySQL --MySQL UPDATE orders SET order_status = true WHERE order_id = 10004; Update the shipment status in Postgres --PG UPDATE shipments SET is_arrived = true WHERE shipment_id = 1004; Delete the order in MySQL --MySQL DELETE FROM orders WHERE order_id = 10004; The changes of enriched orders in Kibana are as follows: Clean up # After finishing the tutorial, run the following command to stop all containers in the directory of docker-compose.yml:
docker-compose down Run the following command to stop the Flink cluster in the directory of Flink flink-1.18.0:
./bin/stop-cluster.sh Back to top
`}),e.add({id:51,href:"/flink/flink-cdc-docs-master/docs/connectors/flink-sources/tutorials/",title:"Tutorials",section:"Flink Sources",content:" "}),e.add({id:52,href:"/flink/flink-cdc-docs-master/versions/",title:"Versions",section:"Apache Flink CDC",content:` Versions # An appendix of hosted documentation for all versions of Apache Flink CDC.
v3.0 `})})()